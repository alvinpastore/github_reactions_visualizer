{"568416293": {"author_username": "nicolov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33550, "title": "Normalize reward-to-go in C++ actor-critic", "body": "Comparing to the [Python implementation](https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py), it seems like the tensor of normalized reward-to-go is computed but never used. Even if it's just an integration test, this PR switches to the normalized version for better convergence.", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 16:22:37", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568401223": {"author_username": "zz-xx", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33549, "title": "[Documentation] Fix minor typo in torch.serialization", "body": "", "labels": ["open source", "triaged"], "number_of_comments": 0, "created_at": "2020-02-20 15:58:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568398024": {"author_username": "gchanan", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33548, "title": "[RESUBMIT] [pytorch] Migrating index_add cuda to ATen", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33548 [RESUBMIT] [pytorch] Migrating index_add cuda to ATen**\n\nSummary:\nMostly just moved code.\nIndex dim and number of indices checks are added to make checks idential to index_add_cpu_\n\nThis is a resubmit of #30573, which got reverted.\n\nDifferential Revision: [D20002248](https://our.internmc.facebook.com/intern/diff/D20002248)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-20 15:54:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568386381": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33547, "title": "Cannot define one-off keyword only arguments", "body": "Keyword arguments are less likely to cause backwards compatibility problems in the future, but we don't make it easily possible to define them. According to @kurtamohler the \"obvious\" thing doesn't work in https://github.com/pytorch/pytorch/pull/33430#discussion_r381565565\r\n\r\nwith error\r\n\r\n```\r\nWriting torch/csrc/autograd/generated/python_variable_methods.cpp\r\nTraceback (most recent call last):\r\n  File \"tools/setup_helpers/generate_code.py\", line 93, in <module>\r\n    main()\r\n  File \"tools/setup_helpers/generate_code.py\", line 88, in main\r\n    options.selected_op_list_path,\r\n  File \"tools/setup_helpers/generate_code.py\", line 43, in generate_code\r\n    gen_autograd_python(declarations_path or DECLARATIONS_PATH, autograd_gen_dir, 'tools/autograd')\r\n  File \"/home/kurtamohler/development/pytorch-bmm/tools/autograd/gen_autograd.py\", line 234, in gen_autograd_python\r\n    out, aten_decls + deprecated, template_path)\r\n  File \"/home/kurtamohler/development/pytorch-bmm/tools/autograd/gen_python_functions.py\", line 173, in gen_py_torch_functions\r\n    env = create_python_bindings(py_torch_functions, is_python_method=False, is_module=False)\r\n  File \"/home/kurtamohler/development/pytorch-bmm/tools/autograd/gen_python_functions.py\", line 192, in create_python_bindings\r\n    py_methods.append(method_impl(name, overload_decls, is_python_method, is_module))\r\n  File \"/home/kurtamohler/development/pytorch-bmm/tools/autograd/gen_python_functions.py\", line 884, in method_impl\r\n    grouped = group_overloads(declarations, is_python_method)\r\n  File \"/home/kurtamohler/development/pytorch-bmm/tools/autograd/gen_python_functions.py\", line 1046, in group_overloads\r\n    x, list(dictionary.keys())))\r\nRuntimeError: 'base' not in dictionary for bmm(Tensor input, Tensor mat2, bool deterministic). keys are ['out', 'signature']\r\n\r\n```", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 15:36:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568362587": {"author_username": "albanD", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33546, "title": "`torch.seed()` and `torch.initial_seed()` returns not-long numbers", "body": "Our function that give the current or initial seed do not play well with setting these seeds.\r\n\r\n```python\r\nimport torch\r\n\r\ntorch.manual_seed(torch.initial_seed()) # Same with torch.seed()\r\n```\r\nFails with:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-055df2441829> in <module>\r\n----> 1 import torch; torch.manual_seed(torch.initial_seed())\r\n\r\nsource/torch/random.py in manual_seed(seed)\r\n     32         torch.cuda.manual_seed_all(seed)\r\n     33\r\n---> 34     return default_generator.manual_seed(seed)\r\n     35\r\n     36\r\n\r\nRuntimeError: Overflow when unpacking long\r\n```\r\n\r\n", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 15:04:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568317366": {"author_username": "IlichevSergey", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33545, "title": "HistogramObserver: forward returns detached tensor", "body": "forward is implemented as follows:\r\n\r\n    def forward(self, x_orig):\r\n        # type: (Tensor) -> Tensor\r\n        x = x_orig.detach()\r\n        ...\r\n        return x\r\n\r\n## Expected behavior\r\n\r\nI suppose that there is should be \r\n\r\n    def forward(self, x_orig):\r\n        # type: (Tensor) -> Tensor\r\n        x = x_orig.detach()\r\n        ...\r\n        return x_orig\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0\r\nCMake version: version 3.15.5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.96\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-memlab==0.0.2\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas 1.0 mkl\r\n[conda] libmklml 2019.0.3 0\r\n[conda] mkl 2019.5 281 anaconda\r\n[conda] mkl-service 2.3.0 py36hb782905_0 anaconda\r\n[conda] mkl_fft 1.0.12 py36h14836fe_0\r\n[conda] mkl_random 1.0.2 py36h343c172_0\r\n[conda] pytorch 1.4.0 py3.6_cuda101_cudnn7_0 pytorch\r\n[conda] pytorch-memlab 0.0.2 pypi_0 pypi\r\n[conda] torchvision 0.5.0 py36_cu101 pytorch\r\n", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 13:54:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568237914": {"author_username": "marka17", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33544, "title": "Very slow torch.roll in comparison with numpy.roll", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\ntorch.roll is about 1000 times slower than numpy.roll.\r\n\r\n## To Reproduce\r\n\r\nThe simple code below reproduces the problem:\r\n\r\n`import torch`\r\n`import numpy as np`\r\n`import random`\r\n\r\n`torch_tensor = torch.rand(16 * 100000)`\r\n`numpy_array = np.random.rand(16 * 100000)`\r\n\r\n`%%timeit -n 4 -r 4`\r\n`rolled = torch.roll(torch_tensor, random.randint(-torch_tensor.shape[0], torch_tensor.shape[0]))`\r\nResult : 3.29 s \u00b1 344 ms\r\n\r\n`%%timeit -n 4 -r 4`\r\n`rolled = np.roll(numpy_array, random.randint(-numpy_array.shape[0], numpy_array.shape[0]))`\r\nResult: 3.29 ms \u00b1 670\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.43\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.4.0\r\n[pip] torchaudio==0.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 11:35:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568178075": {"author_username": "DuckSoft", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33543, "title": "adding IterableDataset to utils.data.__init__", "body": "this shall fix issue #27820 again\r\n\r\n", "labels": ["open source"], "number_of_comments": 2, "created_at": "2020-02-20 09:53:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568112394": {"author_username": "lly-zero-one", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33540, "title": "Add the conv3d fusion support in pytorch quantization flow", "body": "", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 07:46:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568084010": {"author_username": "yinghai", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33539, "title": "Call RandomNumberSeed() on-demand", "body": "Summary: We rarely use the `random_seed_` in context but we always initialize it with `RandomNumberSeed()` which isn't trivial. This diff makes it that we only call `RandomNumberSeed()` once when we want to use `random_seed_`.\n\nTest Plan: unittests.\n\nDifferential Revision: D19993190\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-20 06:38:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568062337": {"author_username": "andrewdelong", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33538, "title": "Fix bug in baddbmm corner case (#33467)", "body": "Ensure `torch.baddbmm(c, a, b)` returns `beta*c` when `a @ b` has empty inner dimension.\r\n\r\nFixes #33467. \r\n", "labels": ["open source"], "number_of_comments": 0, "created_at": "2020-02-20 05:33:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568060950": {"author_username": "kennyhorror", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33537, "title": "[C2] Small improvement for elementwise_mul operator.", "body": "Summary:\nCases of embeddings smaller than 128, we can get a bit more compute by\nallocating less threads per block.\n\nTest Plan: Unit-test, benchmark.\n\nDifferential Revision: D19969594\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-20 05:29:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568054991": {"author_username": "bhosmer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33536, "title": "reuse named tensor error message in generated code", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33536 reuse named tensor error message in generated code**\n\r\nSimple fix, merge the identical string literals that were being inlined into every wrapper for ops that don't support named tensors. E.g.\r\n```\r\nTensor all(const Tensor & self, int64_t dim, bool keepdim) {\r\n    if (self.has_names()) {\r\n        AT_ERROR(\r\n            \"all is not yet supported with named tensors. Please drop names via \"\r\n            \"`tensor = tensor.rename(None)`, call the op with an unnamed tensor, \"\r\n            \"and set names on the result of the operation.\");\r\n    }\r\n    const OptionalDeviceGuard device_guard(device_of(self));\r\n    return at::native::all(self, dim, keepdim);\r\n}\r\n```\r\nbecomes\r\n```\r\nTensor all(const Tensor & self, int64_t dim, bool keepdim) {\r\n    if (self.has_names()) {\r\n        AT_ERROR(\"all\", named_tensors_unsupported_error);\r\n    }\r\n    const OptionalDeviceGuard device_guard(device_of(self));\r\n    return at::native::all(self, dim, keepdim);\r\n}\r\n```\r\n\r\nAlso updated the generated file comments to include the source template names, e.g. \r\n```\r\n// @generated by aten/src/ATen/gen.py from TypeDefault.cpp\r\n```\n\nDifferential Revision: [D19993407](https://our.internmc.facebook.com/intern/diff/D19993407)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-20 05:09:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568046738": {"author_username": "aCuria", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33535, "title": "Crash in torchvision tutorial sample code", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nCrash when running \"tv-training-code.py\"\r\nhttps://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\r\n\r\n![image](https://user-images.githubusercontent.com/35476457/74906690-b4dd2300-53ec-11ea-8d15-984a97bdf8a6.png)\r\n\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run tv-training-code.py with python 3.7, pycocotools 2.00, vs2019\r\n2. Crash after first epoch\r\n\r\nEpoch: [0]  [ 0/60]  eta: 0:01:59  lr: 0.000090  loss: 3.6203 (3.6203)  loss_classifier: 0.6837 (0.6837)  loss_box_reg: 0.1773 (0.1773)  loss_mask: 2.7259 (2.7259)  loss_objectness: 0.0237 (0.0237)  loss_rpn_box_reg: 0.0098 (0.0098)  time: 1.9867  data: 0.7440  max mem: 2139\r\nEpoch: [0]  [10/60]  eta: 0:00:27  lr: 0.000936  loss: 1.8186 (2.3457)  loss_classifier: 0.4372 (0.4416)  loss_box_reg: 0.1773 (0.1920)  loss_mask: 1.2114 (1.6839)  loss_objectness: 0.0110 (0.0167)  loss_rpn_box_reg: 0.0101 (0.0115)  time: 0.5418  data: 0.0683  max mem: 3076\r\nEpoch: [0]  [20/60]  eta: 0:00:18  lr: 0.001783  loss: 0.7622 (1.5273)  loss_classifier: 0.1937 (0.3105)  loss_box_reg: 0.1568 (0.1637)  loss_mask: 0.3449 (1.0187)  loss_objectness: 0.0140 (0.0205)  loss_rpn_box_reg: 0.0111 (0.0139)  time: 0.3805  data: 0.0006  max mem: 3076\r\nEpoch: [0]  [30/60]  eta: 0:00:12  lr: 0.002629  loss: 0.4866 (1.1928)  loss_classifier: 0.1035 (0.2428)  loss_box_reg: 0.1189 (0.1560)  loss_mask: 0.2358 (0.7609)  loss_objectness: 0.0163 (0.0183)  loss_rpn_box_reg: 0.0122 (0.0149)  time: 0.3697  data: 0.0007  max mem: 3076\r\nEpoch: [0]  [40/60]  eta: 0:00:08  lr: 0.003476  loss: 0.4439 (1.0093)  loss_classifier: 0.0733 (0.2025)  loss_box_reg: 0.1218 (0.1555)  loss_mask: 0.1967 (0.6204)  loss_objectness: 0.0063 (0.0153)  loss_rpn_box_reg: 0.0147 (0.0156)  time: 0.3830  data: 0.0007  max mem: 3076\r\nEpoch: [0]  [50/60]  eta: 0:00:04  lr: 0.004323  loss: 0.4103 (0.8831)  loss_classifier: 0.0628 (0.1729)  loss_box_reg: 0.1218 (0.1463)  loss_mask: 0.1835 (0.5359)  loss_objectness: 0.0041 (0.0129)  loss_rpn_box_reg: 0.0147 (0.0152)  time: 0.3934  data: 0.0006  max mem: 3134\r\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.3786 (0.8003)  loss_classifier: 0.0506 (0.1553)  loss_box_reg: 0.0915 (0.1357)  loss_mask: 0.1835 (0.4829)  loss_objectness: 0.0015 (0.0114)  loss_rpn_box_reg: 0.0141 (0.0149)  time: 0.3875  data: 0.0006  max mem: 3134\r\nEpoch: [0] Total time: 0:00:24 (0.4126 s / it)\r\ncreating index...\r\nindex created!\r\n## Expected behavior\r\n\r\nCrash should not happen in sample code\r\n\r\n## Environment\r\n\r\npython .\\collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: Could not collect\r\nCMake version: version 3.16.0-rc1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 441.99\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n", "labels": [], "number_of_comments": 1, "created_at": "2020-02-20 04:42:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568018918": {"author_username": "mingfeima", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33534, "title": "Fix torch.cat()  performance regression on single core CPU", "body": "This PR addresses the performance regression on `torch.cat()` on CPU with single thread.\r\nPrevious optimization #30806 introduced regression for several cases on pytorch operator benchmark.\r\nSee #33334 for detail.", "labels": ["open source"], "number_of_comments": 2, "created_at": "2020-02-20 03:02:42", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568013199": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33533, "title": "[not4land] Temporary workaround to get quantized ops", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33533 [not4land] Temporary workaround to get quantized ops**\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWorkaround to get passes after inline to run\nto unblock from freeze API\n\nTest Plan:\n.\nReviewers:\n.\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-20 02:41:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568013160": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33532, "title": "[quant][graphmode] Add quantization pattern for quantized::add_relu", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* **#33532 [quant][graphmode] Add quantization pattern for quantized::add_relu**\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-20 02:41:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568013101": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33531, "title": "[quant][graphmode] Replicate dequantize nodes", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* **#33531 [quant][graphmode] Replicate dequantize nodes**\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWe already insert dequantize for each use of the value, but there might still be cases where we only\nsee the value is used multiple times after inline. This pass adds the support to replicate dequantize\nafter inline to ensure output of dequantize is only used by one node, which is necessary to preserve all\nquantization patterns like `dequant - conv - quant`\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-20 02:41:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "568011066": {"author_username": "shersoni610", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33530, "title": "Pytorch + Cuda + GCC Version ", "body": "Hello,\r\n\r\nI am using Ubuntu 18.04 which has gcc-9.0 installed. I also use Titan black card for which\r\nI need to install pytorch from the sources.\r\n\r\nHowever, when I try to compile the source codes, I see that cuda 10.2 does not support the\r\ngcc > 7.0.  Why there is a such a restriction? Can I use gcc > 8 to compile pytorch?\r\n\r\n\r\nThanks", "labels": [], "number_of_comments": 0, "created_at": "2020-02-20 02:34:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567992022": {"author_username": "kennyhorror", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33529, "title": "[C2] Native GPU implementation for bucketize", "body": "Summary:\nCurrent version goes through GPU -> CPU -> GPU copy and is pretty slow: ~19 ms\nfor 1M elements with 20 possible buckets based on benchmark.\n\nThis new version is ~0.2 on the same\n\nTest Plan: benchmark + unit-test\n\nDifferential Revision: D19969518\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-20 01:26:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567987948": {"author_username": "zdevito", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33528, "title": "Use cheaper check in isTensorList", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33528 Use cheaper check in isTensorList**\n\nDifferential Revision: [D19989166](https://our.internmc.facebook.com/intern/diff/D19989166)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-20 01:13:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567986985": {"author_username": "jjsjann123", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33527, "title": "[CUDA_FUSER] Fork CUDA fuser", "body": "Separating CUDA fuser from CPU fuser.\r\n\r\n1. New node in IR - prim::CudaFusionGroup:\r\n   This enables the cuda fuser to co-exist along side the old fuser. Allows us\r\n   to incrementally build and expand cuda fuser.\r\n\r\n2. copied FuseGraph optimization passes to CudaFuserGraph:\r\n   We will re-factor & reuse Chunk/Concat in the old fuser logic, which is\r\n   handled in the optimization pass at this moment. Unfortunately many code in\r\n   the pass is tightly binded with the legacy fuser, which makes code sharing\r\n   difficult.\r\n   The CudaFusionGraph runs before FuseGraph, as the CudaFuseGraph will support\r\n   only a subset of operations comparing to legacy fuser (CUDA only).\r\n   CudaFuseGraph is default off and could be turned on via:\r\n     ```torch._C._jit_set_cuda_fusion_group_executor(True)```\r\n\r\n3. We don't have codegen in this PR yet (WIP). Currently we just fall back to\r\n   the old fuser.\r\n\r\n", "labels": ["jit", "open source"], "number_of_comments": 5, "created_at": "2020-02-20 01:10:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567982992": {"author_username": "wanchaol", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33526, "title": "[jit] make RRef type annotation available in Python", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33263 [jit] allow RRef local creation with IValue objects\n* **#33526 [jit] make RRef type annotation available in Python**\n* #33369 [jit] infer RRef type as container type\n\nThis PR make RRef type annotation available in Python by introducing a\nfake generic type which holds the element type, it allows user use rref\nas a type annotation in python functions that jit calls into (i.e.\njit.ignore)\n\nDifferential Revision: [D19988848](https://our.internmc.facebook.com/intern/diff/D19988848)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-20 00:57:28", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567949712": {"author_username": "lly-zero-one", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33525, "title": "[WIP]add the sls tensor train op", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-20 00:13:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567935615": {"author_username": "xuhdev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33524, "title": "Replace AT_CHECK with TORCH_CHECK in torch/csrc/jit/pybind_utils.h", "body": "This is generating a considerable amount of warning, due to the fact\r\nthat the header file is included in multiple places.\r\n\r\n", "labels": ["jit", "open source"], "number_of_comments": 0, "created_at": "2020-02-19 23:52:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567900711": {"author_username": "shersoni610", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33521, "title": "MPI Error", "body": "Hello,\r\n\r\nI get the following error while compiling pytorc- pybind11 include dirs: /home/Software/MLearn/pytorch/cmake/../third_party/pybind11/include\r\nCMake Error in /home/Software/MLearn/pytorch/build/CMakeFiles/CMakeTmp/CMakeLists.txt:\r\n  Imported target \"MPI::MPI_C\" includes non-existent path\r\n\r\n    \"/home/Software/ParaComp/V3.3/include\"\r\n\r\n  in its INTERFACE_INCLUDE_DIRECTORIES.  Possible reasons include:\r\n\r\n  * The path was deleted, renamed, or moved to another location.\r\n\r\n  * An install or uninstall procedure did not complete successfully.\r\n\r\n  * The installation package was faulty and references files it does not\r\n  provide.\r\n\r\n\r\n\r\nCMake Error at /home/Software/System/CMake/V3.14.4/share/cmake-3.14/Modules/FindMPI.cmake:1188 (try_compile):\r\n  Failed to generate test project build system.\r\nCall Stack (most recent call first):\r\n  /home/Software/System/CMake/V3.14.4/share/cmake-3.14/Modules/FindMPI.cmake:1229 (_MPI_try_staged_settings)\r\n  /home/Software/System/CMake/V3.14.4/share/cmake-3.14/Modules/FindMPI.cmake:1489 (_MPI_check_lang_works)\r\n  cmake/Dependencies.cmake:814 (find_package)\r\n  CMakeLists.txt:404 (include)\r\n", "labels": [], "number_of_comments": 0, "created_at": "2020-02-19 22:53:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567893458": {"author_username": "lematt1991", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33520, "title": "Reverse Cumulative Sum", "body": "## \ud83d\ude80 Feature\r\nAdd `reverse` option to `torch.cumsum`, such as in [tensorflow](https://www.tensorflow.org/api_docs/python/tf/math/cumsum)\r\n\r\n## Motivation\r\nThis would compute right to left cumulative sum more efficiently.  Currently, as far as I know, the only way to do it is \r\n\r\n```Python\r\nx = torch.arange(9).view(3, 3)\r\nr2lcumsum = th.flip(th.cumsum(th.flip(x, [1]), 1), [1])\r\n```\r\n\r\nResult should be:\r\n\r\n```Python\r\ntensor([[ 3,  3,  2],\r\n        [12,  9,  5],\r\n        [21, 15,  8]])\r\n```\r\n\r\n## Pitch\r\nAdd `reverse` arg to native `cumsum` function\r\n\r\n## Alternatives\r\n\r\n## Additional context\r\n", "labels": [], "number_of_comments": 0, "created_at": "2020-02-19 22:40:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567887910": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33518, "title": "Collapse _like overloads into a single overload.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33518 Collapse _like overloads into a single overload.**\n* #33516 Add MemoryFormat to TensorOptions, but not codegen.\n* #33510 Correctly preserve \"not set anywhere\" TensorOptions when merging.\n* #33505 Switch empty_like to use merge_in to process TensorOptions.\n\nThe fact that there were two overloads appears to be a historical\nartifact that dates back to when goldsborough originally added these\nbindings in the first place.  If TensorOptions is made optional,\nthen you only need one overload, not two, as they are exactly redundant\nwith each other.  When MemoryFormat was added, it was made a little\nharder to do this, as the C++ syntax at::empty_like(t, memory_format) would\nnot work if you collapsed the overload; but now it works because TensorOptions\nsupports MemoryFormat.\n\nThe upshot is, I can get rid of all the overloads and just have one overload.\nAmazingly, this change is backwards compatible, as the test attests.  While\nI was at it, I also deleted the overload name from the functions entirely.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-19 22:28:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567859932": {"author_username": "ljk53", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33517, "title": "[pytorch][size] remove unused SparseCPUType from mobile build", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33517 [pytorch][size] remove unused SparseCPUType from mobile build**\n\nI don't think any mobile model uses SparseCPU backend yet so we can skip\ngenerating dispatch code for this backend type.\n\nThis will help reduce mobile code size with dynamic dispatch turned on,\nroughly ~100K for uncompressed iOS: D19616007 +413K v.s. D19616016 +319K.\n\nIt probably doesn't affect much static dispatch build size as the unused\nstatic dispatch methods will be stripped by linker in the end.\n\nDifferential Revision: [D19978633](https://our.internmc.facebook.com/intern/diff/D19978633/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19978633/)!", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 21:30:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567846377": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33516, "title": "Add MemoryFormat to TensorOptions, but not codegen.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33518 Collapse _like overloads into a single overload.\n* **#33516 Add MemoryFormat to TensorOptions, but not codegen.**\n* #33510 Correctly preserve \"not set anywhere\" TensorOptions when merging.\n* #33505 Switch empty_like to use merge_in to process TensorOptions.\n\nThis diff adds MemoryFormat field to TensorOptions, and teaches\nall kernels that take TensorOptions to respect it, but doesn't\nteach the codegen about it.  As such, it is now possible to specify\nmemory_format using TensorOptions syntax, e.g.,\nat::empty_like(tensor, at::memory_format(MemoryFormat::Contiguous))\nin the C++ API, but there isn't any other user visible effect.\n\nThe intended end state of this diff stack is to eliminate the\nexplicit MemoryFormat? arguments from native functions, but\nas this change has BC implications I'd prefer to do it separately.\nSo this starts things off with a non-BC breaking addition to the\nAPI.  For all internal functions that are not bound by codegen,\nI switch them to exclusively using TensorOptions (eliminating\nMemoryFormat); there's only a few, mostly quantized and to().\n\nTo keep things screwed down in the short term, it is a HARD ERROR\nto specify both the explicit MemoryFormat argument as well as\nTensorOptions.  This caught a few errors in my diff where I needed\nto modify memory format settings and then call code later, esp\nin empty_like.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 21:04:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567829383": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33514, "title": "Make setter non-optional, e.g., TensorOptions::device(optional<Device>) -> device(Device), and add a device_opt setter", "body": "Instead of\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nwe should have\r\n\r\n```\r\nclass TensorOptions {\r\n  TensorOptions device(Device) { ... } // setter\r\n  TensorOptions device_opt(optional<Device>) { ... } // setter\r\n}\r\n```\r\n\r\nThis makes the class more user friendly as now `x.device({kCUDA, 1})` works; it also makes it symmetric with `at::device({kCUDA, 1})`.\n\ncc @yf225", "labels": ["module: cpp", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 20:34:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567801119": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33513, "title": "[distributed] skip use_ignore_output tests in c10d if not built with gloo", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33513 [distributed] skip use_ignore_output tests in c10d if not built with gloo**\n\nThese tests require gloo so like the other tests, they should be\nskipped if not building with gloo. Otherwise they crash on Mac if not built\nwith gloo currently.\n\nverified that it does not crash anymore with this PR.\n\nDifferential Revision: [D19976908](https://our.internmc.facebook.com/intern/diff/D19976908/)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 19:42:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567794846": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33510, "title": "Correctly preserve \"not set anywhere\" TensorOptions when merging.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33518 Collapse _like overloads into a single overload.\n* #33516 Add MemoryFormat to TensorOptions, but not codegen.\n* **#33510 Correctly preserve \"not set anywhere\" TensorOptions when merging.**\n* #33505 Switch empty_like to use merge_in to process TensorOptions.\n\nPreviously, we would fill in TensorOption with defaults whenever an\nitem was missing from both the left and right side of the merge.  This\nis morally incorrect: if we don't have an item on the left or right,\nwe should keep the entry empty (so the downstream user can apply\nthe appropriate defaulting rule).\n\nI don't think this caused any bugs, but I noticed this error when\nworking on a later patch in my diff stack.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D20001775](https://our.internmc.facebook.com/intern/diff/D20001775)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 19:32:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567789353": {"author_username": "lara-hdr", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33509, "title": "ONNX Export Support for NLLLoss", "body": "Adding ONNX export support for torch.nn.NLLLoss().", "labels": ["module: onnx", "open source", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 19:23:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567772106": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33507, "title": "Gradcheck for complex", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33507 Gradcheck for complex**\n* #33361 torch.tensor can infer complex dtype now\n* #33268 Added tensor.is_complex(), is_complex and dtype.is_complex py binding, tensor printing, and dixed the scalar type returned for complex float\n\nDifferential Revision: [D19975615](https://our.internmc.facebook.com/intern/diff/D19975615)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 18:53:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567769691": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33506, "title": "Some c10d tests are broken on MacOS", "body": "## \ud83d\udc1b Bug\r\n\r\n`test_ignored_output` and `test_ignored_output_with_unused_parameters` crash on MacOS, when pytorch is built locally:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py\", line 178, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 2786, in test_ignored_output\r\n    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)\r\nAttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'\r\nexiting process with exit code: 10\r\nERROR:root:Caught exception:\r\nTraceback (most recent call last):\r\n  File \"/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py\", line 178, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 2786, in test_ignored_output\r\n    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)\r\nAttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'\r\nexiting process with exit code: 10\r\nFERROR:root:Caught exception:\r\nTraceback (most recent call last):\r\n  File \"/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py\", line 178, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 2828, in test_ignored_output_with_unused_parameters\r\n    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)\r\nAttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'\r\nexiting process with exit code: 10\r\nERROR:root:Caught exception:\r\nTraceback (most recent call last):\r\n  File \"/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py\", line 178, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 2828, in test_ignored_output_with_unused_parameters\r\n    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)\r\nAttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'\r\nexiting process with exit code: 10\r\n```\r\n## To Reproduce\r\nBuild pytorch locally on MacOS (have to build without gloo) and run `python test/test_c10d.py -v`. These 2 tests fail, but the other tests that require gloo are correctly skipped. I think this is missing the requires_gloo decorator.\r\n\r\nI am not sure why these aren't failing in the MacOS CI currently. Perhaps we have some other flag that is correctly set in CI so these tests get skipped\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar", "labels": ["module: distributed", "module: tests", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 18:50:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567761104": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33505, "title": "Switch empty_like to use merge_in to process TensorOptions.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33518 Collapse _like overloads into a single overload.\n* #33516 Add MemoryFormat to TensorOptions, but not codegen.\n* #33510 Correctly preserve \"not set anywhere\" TensorOptions when merging.\n* **#33505 Switch empty_like to use merge_in to process TensorOptions.**\n\nThis shouldn't change semantics, but it has the benefit of making\ntorch::empty_like(x, dtype(kFloat)) actually work (previously, this\nwould just ignore all of the properties from x).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D20001776](https://our.internmc.facebook.com/intern/diff/D20001776)", "labels": [], "number_of_comments": 0, "created_at": "2020-02-19 18:36:45", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567760789": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33504, "title": "[JIT] fix resolving of functions in torch/functional. fix compilation of torch.stft", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#33504 [JIT] fix resolving of functions in torch/functional**\r\n\r\nFix resolution fo functions that are bound onto torch in torch/functional.py. This does not fix compilation of all of those functions, those will be done in follow ups. Does torch.stft as a start.\r\n\r\nFix for https://github.com/pytorch/pytorch/issues/21478", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-19 18:36:19", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567753980": {"author_username": "EscapeZero", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33502, "title": "remove conflicting caffe2 generated header", "body": "Summary:\nGoal is to remove empty `caffe2/core/macros.h`.\n\nThis is mainly to get around an issue on windows/buck where the compilers prefers selecting the empty header over the generated header since they have the same include path.\n\nWe could also possibly reuse `C10_USING_CUSTOM_GENERATED_MACROS` instead of making a new macro?\n\nTest Plan: CI green\n\nDifferential Revision: D19896503\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-19 18:25:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567744578": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33501, "title": "When a Node fails to resolve to an Operator, print out the types of arguments, and all \"close matches\" in known operators", "body": "Related to #33500\r\n\r\nSometimes, malformed nodes can show up in your IR. The most common situation this occurs is when you've written out IR by hand, as is done in some alias analysis and dce tests. When this happens, you get an assert like:\r\n\r\n```\r\nRuntimeError: hasSpecialCase INTERNAL ASSERT FAILED at ../torch/csrc/jit/passes/alias_analysis.cpp:311,  please report a bug to PyTorch. We don't have an op for aten::ones but it isn't a special case. (analyzeImpl at ../torch/csrc/jit/passes/alias_analysis.cpp:311)\r\n```\r\n\r\nAsserts like this can be made dramatically more useful by reporting:\r\n* The types of the arguments\r\n* The list of \"close miss\" declarations which might have matched, but didn't match for some reason\r\n\r\nAssuming there are no bugs in the subsumption check, this would make it instantly clear what the problem is.\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-19 18:12:02", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567740766": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33500, "title": "Use fake nodes in tests that hard code IR", "body": "A number of JIT C++ tests hard-code IR to test aliasing and dce, e.g.,\r\n\r\n```\r\n      script::parseIR(\r\n          R\"IR(\r\n  graph():\r\n    %10 : bool = prim::Constant[value=0]()\r\n    %8 : Device? = prim::Constant()\r\n    %4 : int? = prim::Constant()\r\n    %0 : int = prim::Constant[value=2]()\r\n    %1 : int = prim::Constant[value=3]()\r\n    %2 : int[] = prim::ListConstruct(%0, %1)\r\n    %x : Tensor = aten::rand(%2, %4, %4, %8, %10)\r\n    %12 : int[] = prim::ListConstruct(%0, %1)\r\n    %y : Tensor = aten::rand(%12, %4, %4, %8, %10)\r\n    %22 : int[] = prim::ListConstruct(%0, %1)\r\n    %z : Tensor = aten::rand(%22, %4, %4, %8, %10)\r\n    %32 : int[] = prim::ListConstruct(%0, %1)\r\n    %fresh : Tensor = aten::rand(%32, %4, %4, %8, %10)\r\n    %foo : Tensor[] = prim::ListConstruct(%x, %y)\r\n    %43 : Tensor[] = aten::append(%foo, %z)\r\n    return ()\r\n  )IR\",\r\n```\r\n\r\nin `test/cpp/jit/test_alias_analysis.cpp`\r\n\r\nIR represented at this stage has had all default arguments resolved into explicit arguments in the IR. Unfortunately, this is a major source of test brittleness, because whenever the default arguments of a function change (even if the change is backwards compatible), every IR graph has to be updated (by hand!!) to reflect the default change. That's *really bad*, because it means any developer who wants to work on adding new keyword arguments to our factory functions needs to know about these JIT tests. Also, the error message when you get it wrong is crap:\r\n\r\n```\r\nRuntimeError: hasSpecialCase INTERNAL ASSERT FAILED at ../torch/csrc/jit/passes/alias_analysis.cpp:311,  please report a bug to PyTorch. We don't have an op for aten::ones but it isn't a special case. (analyzeImpl at ../torch/csrc/jit/passes/alias_analysis.cpp:311)\r\n```\r\n\r\nIn the case of the tests here, we don't really need to use `aten::rand`, any factory function would do. So let's create a fake factory function and use that in the tests, instead of directly using real functions which may need to evolve.\r\n\r\nThere are multiple IR tests in this form in the alias analysis tests as well as the DCE tests.\n\ncc @suo", "labels": ["jit", "topic: bootcamp"], "number_of_comments": 1, "created_at": "2020-02-19 18:05:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567737168": {"author_username": "houseroad", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33499, "title": "[Test Only] fix cat performance regression on single thread", "body": "@hl475 ", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 18:00:05", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567681516": {"author_username": "julianmack", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33495, "title": "ONNX export of torch.jit.script module fails", "body": "## \ud83d\udc1b Bug\r\n\r\nI'm trying to export an ONNX model with the following elements:\r\n1) Control flow \r\n2) An LSTM\r\n\r\nMy understanding is that I need to `torch.jit.script` the model before passing it to `torch.onnx.export` in order to accurately represent the control flow as described here: https://pytorch.org/docs/stable/onnx.html#tracing-vs-scripting.\r\n\r\nHowever, the ONNX export operation throws a RuntimeError (same as #24824). \r\n\r\nI am aware of the issue #24235 which has overlap- but the apparent conclusion in this thread is to avoid scripting with LSTMs which prevents the use of 1) and 2) at the same time. \r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nMinimal example here:\r\n```python\r\nimport torch\r\nfrom typing import Tuple\r\nfrom torch import Tensor\r\nfrom copy import deepcopy\r\n\r\nHIDDEN_SIZE = 4\r\nNUM_LAYERS = 1\r\n\r\nclass SimpleControlFlow(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.lstm = torch.nn.LSTM(HIDDEN_SIZE, HIDDEN_SIZE, NUM_LAYERS)\r\n\r\n    def forward(self, x: Tensor, hn: Tensor, cn: Tensor, \r\n                sos: Tensor,\r\n    ) -> Tuple[Tensor, Tensor, Tensor]:\r\n        out, (hn, cn) = self.lstm(x, (hn, cn))\r\n        if sos:\r\n            out += 1.\r\n        return out, hn, cn\r\n\r\ndef gen_args(batch, seq_len, sos, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS):\r\n    \"\"\"Generates input args for SimpleControlFlow.\"\"\"\r\n    y = torch.randn(seq_len, batch, hidden_size)\r\n    zeros = torch.zeros(num_layers, batch, hidden_size) # hn == cn == zeros\r\n    return y, zeros, zeros, torch.tensor([sos])\r\n\r\nmodel = SimpleControlFlow()\r\n```\r\n\r\n### Aside: demonstrate scripting is required\r\n```python\r\nargs1 = gen_args(batch=1, seq_len=3, sos=False)\r\nargs2 = gen_args(batch=1, seq_len=1, sos=True)\r\nmodel_t = torch.jit.trace(deepcopy(model), args1, check_trace=True)\r\nout1_nn = model(*args1)\r\nout1_trace = model_t(*args1)\r\nout2_nn = model(*args2)\r\nout2_trace = model_t(*args2)\r\n\r\nassert torch.allclose(out1_nn[0], out1_trace[0]), \"Will pass as args1 used for tracing.\\n\"\r\nassert not torch.allclose(out2_nn[0], out2_trace[0]), \"Will pass as `sos` was traced as constant = False.\\n\"\r\n```\r\nAs expected, this throws a `TracerWarning` and the assertions pass.\r\n### Aside end\r\n```python\r\nargs = gen_args(batch=1, seq_len=3, sos=False)\r\n\r\nmodel = torch.jit.script(model)\r\n\r\nexample_outputs = model(*args) # i.e. this runs \r\n\r\n\r\ninput_names = ['y', 'hn_in', 'cn_in', 'sos']\r\noutput_names = ['out', 'hn_out', 'cn_out']\r\ndynamic_axes = {\r\n    \"y\": {0: \"seq_len\", 1: \"batch\"},\r\n    \"hn_in\": {1: \"batch\"},\r\n    \"cn_in\": {1: \"batch\"},\r\n    \"out\": {0: \"seq_len\", 1: \"batch\"},\r\n    \"hn_out\": {1: \"batch\"},\r\n    \"cn_out\": {1: \"batch\"},\r\n}\r\n\r\ntorch.onnx.export(\r\n    model,\r\n    args,\r\n    'model.onnx',\r\n    export_params=True,\r\n    verbose=True,\r\n    example_outputs=example_outputs,\r\n    dynamic_axes=None,\r\n    input_names=input_names,\r\n    output_names=output_names,\r\n    opset_version=11,\r\n    )\r\n```\r\nRaises \r\n```shell\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-3-a255a7548070> in <module>\r\n     28     input_names=input_names,\r\n     29     output_names=output_names,\r\n---> 30     opset_version=11,\r\n     31     )\r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\r\n    146                         operator_export_type, opset_version, _retain_param_name,\r\n    147                         do_constant_folding, example_outputs,\r\n--> 148                         strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\r\n    149 \r\n    150 \r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs)\r\n     64             _retain_param_name=_retain_param_name, do_constant_folding=do_constant_folding,\r\n     65             example_outputs=example_outputs, strip_doc_string=strip_doc_string,\r\n---> 66             dynamic_axes=dynamic_axes, keep_initializers_as_inputs=keep_initializers_as_inputs)\r\n     67 \r\n     68 \r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size)\r\n    414                                                         example_outputs, propagate,\r\n    415                                                         _retain_param_name, do_constant_folding,\r\n--> 416                                                         fixed_batch_size=fixed_batch_size)\r\n    417 \r\n    418         # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size)\r\n    263         assert example_outputs is not None, \"example_outputs must be provided when exporting a ScriptModule\"\r\n    264         try:\r\n--> 265             method_graph, params = torch._C._jit_pass_lower_graph(model.forward.graph, model._c)\r\n    266             in_vars, in_desc = torch.jit._flatten(tuple(args) + tuple(params))\r\n    267             graph = _propagate_and_assign_input_shapes(\r\n\r\nRuntimeError: isTensor() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90, please report a bug to PyTorch. Expected Tensor but got Bool (toTensor at /opt/conda/conda-bld/pytorch_1579040055865/work/aten/src/ATen/core/ivalue_inl.h:90)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7f81bec8a627 in /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: torch::jit::LowerGraph(torch::jit::Graph&, c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > const&) + 0x674 (0x7f81c4670794 in /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #2: <unknown function> + 0x6b6479 (0x7f81eff2b479 in /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x28ba06 (0x7f81efb00a06 in /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #4: _PyMethodDef_RawFastCallKeywords + 0x264 (0x55f54c4e26e4 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #5: _PyCFunction_FastCallKeywords + 0x21 (0x55f54c4e2801 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #6: _PyEval_EvalFrameDefault + 0x4e8c (0x55f54c53e2bc in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #7: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #8: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #9: _PyEval_EvalFrameDefault + 0x14ce (0x55f54c53a8fe in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #10: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #11: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #12: _PyEval_EvalFrameDefault + 0x14ce (0x55f54c53a8fe in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #13: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #14: _PyFunction_FastCallKeywords + 0x325 (0x55f54c4e19c5 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #15: _PyEval_EvalFrameDefault + 0x4aa9 (0x55f54c53ded9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #16: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #17: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #18: _PyEval_EvalFrameDefault + 0x14ce (0x55f54c53a8fe in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #19: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #20: PyEval_EvalCodeEx + 0x44 (0x55f54c4803c4 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #21: PyEval_EvalCode + 0x1c (0x55f54c4803ec in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #22: <unknown function> + 0x1e004d (0x55f54c54904d in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #23: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55f54c4e2569 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #24: _PyCFunction_FastCallKeywords + 0x21 (0x55f54c4e2801 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #25: _PyEval_EvalFrameDefault + 0x4755 (0x55f54c53db85 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #26: _PyGen_Send + 0x2a2 (0x55f54c4db672 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #27: _PyEval_EvalFrameDefault + 0x1a6d (0x55f54c53ae9d in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #28: _PyGen_Send + 0x2a2 (0x55f54c4db672 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #29: _PyEval_EvalFrameDefault + 0x1a6d (0x55f54c53ae9d in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #30: _PyGen_Send + 0x2a2 (0x55f54c4db672 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #31: _PyMethodDef_RawFastCallKeywords + 0x8c (0x55f54c4e250c in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #32: _PyMethodDescr_FastCallKeywords + 0x4f (0x55f54c4e286f in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #33: _PyEval_EvalFrameDefault + 0x4c4c (0x55f54c53e07c in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #34: _PyFunction_FastCallKeywords + 0xfb (0x55f54c4e179b in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #35: _PyEval_EvalFrameDefault + 0x416 (0x55f54c539846 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #36: _PyFunction_FastCallKeywords + 0xfb (0x55f54c4e179b in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #37: _PyEval_EvalFrameDefault + 0x6a0 (0x55f54c539ad0 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #38: _PyEval_EvalCodeWithName + 0x2f9 (0x55f54c47f4f9 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #39: _PyFunction_FastCallDict + 0x400 (0x55f54c480800 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #40: _PyObject_Call_Prepend + 0x63 (0x55f54c497c43 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #41: PyObject_Call + 0x6e (0x55f54c48c95e in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #42: _PyEval_EvalFrameDefault + 0x1e20 (0x55f54c53b250 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #43: _PyEval_EvalCodeWithName + 0x5da (0x55f54c47f7da in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #44: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #45: _PyEval_EvalFrameDefault + 0x14ce (0x55f54c53a8fe in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #46: <unknown function> + 0x171cc6 (0x55f54c4dacc6 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #47: <unknown function> + 0x171ecb (0x55f54c4daecb in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #48: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55f54c4e2569 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #49: _PyCFunction_FastCallKeywords + 0x21 (0x55f54c4e2801 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #50: _PyEval_EvalFrameDefault + 0x4755 (0x55f54c53db85 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #51: _PyEval_EvalCodeWithName + 0x5da (0x55f54c47f7da in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #52: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #53: _PyEval_EvalFrameDefault + 0x6a0 (0x55f54c539ad0 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #54: <unknown function> + 0x171cc6 (0x55f54c4dacc6 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #55: <unknown function> + 0x171ecb (0x55f54c4daecb in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #56: _PyMethodDef_RawFastCallKeywords + 0xe9 (0x55f54c4e2569 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #57: _PyCFunction_FastCallKeywords + 0x21 (0x55f54c4e2801 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #58: _PyEval_EvalFrameDefault + 0x4755 (0x55f54c53db85 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #59: _PyEval_EvalCodeWithName + 0x5da (0x55f54c47f7da in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #60: _PyFunction_FastCallKeywords + 0x387 (0x55f54c4e1a27 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #61: _PyEval_EvalFrameDefault + 0x416 (0x55f54c539846 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #62: <unknown function> + 0x171cc6 (0x55f54c4dacc6 in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\nframe #63: <unknown function> + 0x171ecb (0x55f54c4daecb in /home/julian/miniconda3/envs/ml_ci/bin/python)\r\n```\r\n\r\nNote that this is raised despite the fact that all forward arguments, including the bool flag `sos` are `Tensors`.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behaviour\r\nExpect ONNX model to export. \r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n```shell\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 430.64\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.13.3\r\n[pip3] torch-stft==0.1.4\r\n[conda] _pytorch_select           0.1                       cpu_0    anaconda\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243    anaconda\r\n[conda] mkl-service               2.3.0            py37he904b0f_0    anaconda\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0    anaconda\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0    anaconda\r\n[conda] pytorch                   1.4.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    py37_0\r\n```\r\n## Additional context\r\n\r\nThis problem results from the `lstm` as opposed to the if statement - this can be seen by commenting out the if statement and addition in the `SimpleControlFlow` forward method. The control flow is present in the example as it necessary in my use-case.\r\n\n\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["jit", "module: onnx"], "number_of_comments": 1, "created_at": "2020-02-19 16:31:59", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567631863": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33494, "title": "Complex number printing inconsistent with float", "body": "Zeros and  tensor printing for float and complex\r\n\r\n1. In float tensor print, the trailing zeros ( with precision 4) are printed only when there\u2019s one or more digit after decimal or otherwise just a decimal is printed\r\n2. In complex tensor print rn, the trailing zeros are printing in all cases\r\n\r\n>>> torch.zeros((2,2), dtype=torch.float64)                       \r\ntensor([[0., 0.],\r\n[0., 0.]], dtype=torch.float64)\r\n\r\n>>> torch.tensor([1.2, 1.340], dtype=torch.float64)\r\ntensor([1.2000, 1.3400], dtype=torch.float64)\r\n\r\n>>> torch.tensor([1, 1], dtype=torch.float64)\r\ntensor([1., 1.])\r\n\r\n>>> torch.zeros((2,2), dtype=torch.complex64)\r\ntensor([[(0.0000 + 0.0000), (0.0000 + 0.0000j)],\r\n[(0.0000 + 0.0000j), (0.0000 + 0.0000j)]], dtype=torch.complex64)\r\n\r\n>>> torch.tensor([1.2 + 1.340], dtype=torch.complex64)\r\ntensor([1.2000 + 1.3400j], dtype=torch.complex64)\r\n\r\n>>> torch.tensor([1, 1], dtype=torch.complex64)\r\ntensor([1.0000, 1.0000], , dtype=torch.complex64)\n\ncc @ezyang @anjali411 @dylanbespalko", "labels": ["module: complex", "module: printing", "triaged"], "number_of_comments": 2, "created_at": "2020-02-19 15:02:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567629434": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33493, "title": "[Complex] Incorrect Complex Tensor inference", "body": ">>> a=torch.tensor([3/5+4/5j])\r\n>>> a\r\ntensor([(0.6000 - 0.8000j)], dtype=torch.complex128)\r\n\r\nExpected: tensor([(0.6000 + 0.8000j)], dtype=torch.complex128)\r\n\n\ncc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko", "labels": ["high priority", "module: complex", "triage review"], "number_of_comments": 6, "created_at": "2020-02-19 14:59:17", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 1, "heart": 0, "hooray": 0}}, "567519018": {"author_username": "julianmack", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33491, "title": "jit.trace checker fails for LSTM ", "body": "## \ud83d\udc1b Bug\r\n\r\nIt is not possible to trace an LSTM without `check_trace=False` - i.e the checker is unusable. \r\n\r\nMay be related to #23993?\r\n\r\n## To Reproduce\r\nA minimal example is as follows:\r\n```python\r\ninput_size = 2\r\nhidden_size = 3\r\nnum_layers = 2\r\nbatch = 1\r\nseq_len = 2\r\n\r\nlstm = torch.nn.LSTM(input_size, hidden_size, num_layers)\r\n\r\n# args\r\nzeros = torch.zeros(num_layers * 1, batch, hidden_size)\r\na, b =  torch.randn(seq_len, batch, input_size), (zeros, zeros)\r\ntorch.jit.trace(lstm, (a, b))\r\n```\r\n\r\nthrows\r\n```shell\r\n---------------------------------------------------------------------------\r\nTracingCheckError                         Traceback (most recent call last)\r\n<ipython-input-13-74cf3bd6b360> in <module>\r\n     10 zeros = torch.zeros(num_layers * 1, batch, hidden_size)\r\n     11 a, b =  torch.randn(seq_len, batch, input_size), (zeros, zeros)\r\n---> 12 torch.jit.trace(lstm, (a, b))\r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/jit/__init__.py in trace(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\r\n    880         return trace_module(func, {'forward': example_inputs}, None,\r\n    881                             check_trace, wrap_check_inputs(check_inputs),\r\n--> 882                             check_tolerance, _force_outplace, _module_class)\r\n    883 \r\n    884     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and\r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/jit/__init__.py in trace_module(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\r\n   1042                 else:\r\n   1043                     _check_trace([inputs], func, check_trace_method,\r\n-> 1044                                  check_tolerance, _force_outplace, True, _module_class)\r\n   1045     finally:\r\n   1046         torch.jit._trace_module_map = old_module_map\r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/autograd/grad_mode.py in decorate_no_grad(*args, **kwargs)\r\n     47         def decorate_no_grad(*args, **kwargs):\r\n     48             with self:\r\n---> 49                 return func(*args, **kwargs)\r\n     50         return decorate_no_grad\r\n     51 \r\n\r\n~/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/jit/__init__.py in _check_trace(check_inputs, func, traced_func, check_tolerance, force_outplace, is_trace_module, _module_class)\r\n    683         diag_info = graph_diagnostic_info()\r\n    684         if any(info is not None for info in diag_info):\r\n--> 685             raise TracingCheckError(*diag_info)\r\n    686 \r\n    687 \r\n\r\nTracingCheckError: Tracing failed sanity checks!\r\nERROR: Graphs differed across invocations!\r\n\tGraph diff:\r\n\t\t  graph(%self : __torch__.torch.nn.modules.module.Module,\r\n\t\t        %input : Tensor,\r\n\t\t        %2 : (Tensor, Tensor)):\r\n\t\t    %3 : Tensor = prim::GetAttr[name=\"bias_hh_l1\"](%self)\r\n\t\t    %4 : Tensor = prim::GetAttr[name=\"bias_ih_l1\"](%self)\r\n\t\t    %5 : Tensor = prim::GetAttr[name=\"weight_hh_l1\"](%self)\r\n\t\t    %6 : Tensor = prim::GetAttr[name=\"weight_ih_l1\"](%self)\r\n\t\t    %7 : Tensor = prim::GetAttr[name=\"bias_hh_l0\"](%self)\r\n\t\t    %8 : Tensor = prim::GetAttr[name=\"bias_ih_l0\"](%self)\r\n\t\t    %9 : Tensor = prim::GetAttr[name=\"weight_hh_l0\"](%self)\r\n\t\t    %10 : Tensor = prim::GetAttr[name=\"weight_ih_l0\"](%self)\r\n\t\t-   %hx : Tensor, %12 : Tensor = prim::TupleUnpack(%2)\r\n\t\t?                  ^^\r\n\t\t+   %hx.1 : Tensor, %hx : Tensor = prim::TupleUnpack(%2)\r\n\t\t?      ++            ^^\r\n\t\t-   %13 : Tensor[] = prim::ListConstruct(%hx, %hx)\r\n\t\t+   %13 : Tensor[] = prim::ListConstruct(%hx.1, %hx)\r\n\t\t?                                           ++\r\n\t\t    %14 : Tensor[] = prim::ListConstruct(%10, %9, %8, %7, %6, %5, %4, %3)\r\n\t\t    %15 : bool = prim::Constant[value=1]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %16 : int = prim::Constant[value=2]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %17 : float = prim::Constant[value=0]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %18 : bool = prim::Constant[value=1]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %19 : bool = prim::Constant[value=0]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %20 : bool = prim::Constant[value=0]() # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %21 : Tensor, %22 : Tensor, %23 : Tensor = aten::lstm(%input, %13, %14, %15, %16, %17, %18, %19, %20) # /home/julian/miniconda3/envs/ml_ci/lib/python3.7/site-packages/torch/nn/modules/rnn.py:559:0\r\n\t\t    %24 : (Tensor, Tensor) = prim::TupleConstruct(%22, %23)\r\n\t\t    %25 : (Tensor, (Tensor, Tensor)) = prim::TupleConstruct(%21, %24)\r\n\t\t    return (%25)\r\n\tFirst diverging operator:\r\n\tNode diff:\r\n\t\t- %hx : Tensor, %12 : Tensor = prim::TupleUnpack(%2)\r\n\t\t?                ^^\r\n\t\t+ %hx.1 : Tensor, %hx : Tensor = prim::TupleUnpack(%2)\r\n\t\t?    ++            ^^\r\n```\r\n\r\nBy the looks of it the compiler renames `hx`. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behaviour\r\n\r\nThis to pass fine\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 430.64\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.13.3\r\n[pip3] torch-stft==0.1.4\r\n[conda] _pytorch_select           0.1                       cpu_0    anaconda\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243    anaconda\r\n[conda] mkl-service               2.3.0            py37he904b0f_0    anaconda\r\n[conda] mkl_fft                   1.0.12           py37ha843d7b_0    anaconda\r\n[conda] mkl_random                1.0.2            py37hd81dba3_0    anaconda\r\n[conda] pytorch                   1.4.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchaudio                0.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.2.1                    py37_0\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-19 11:56:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567499442": {"author_username": "dasguptar", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33490, "title": "\"BatchSampler\" has no attribute \"batch_size\"", "body": "## \ud83d\udc1b Bug\r\n\r\nPython typing is not working properly on BatchSampler, e.g.\r\n- \"BatchSampler\" has no attribute \"batch_size\"\r\n- \"BatchSampler\" has no attribute \"sampler\"\r\n\r\nwhen trying to access a BatchSampler variable `batch_sampler`'s attributes like `batch_sampler.batch_size` or `batch_sampler.sampler`\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create any sampler on some Dataset. I've tried with SequentialSampler, RandomSampler and DistributedSampler.\r\n2. Create a BatchSampler out of the above sampler.\r\n\r\n## Expected behavior\r\n\r\nPython typing should not be throwing any error.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.2.0+cpu\r\n - OS (e.g., Linux): Windows 10\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6\r\n\n\ncc @SsnL", "labels": ["module: dataloader", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 11:20:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567479027": {"author_username": "ankitdhall", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33489, "title": "torchvision MaskRCNN export to ONNX throws error; onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Node (ConstantOfShape_1966) Op (ConstantOfShape) [ShapeInferenceError] Invalid shape value: 0 ", "body": "## \ud83d\udc1b Bug\r\n\r\nMaskRCNN ONNX model fails to load when reading from disk or from buffer. However, FasterRCNN loads from buffer without errors but throws error when loading from disk. The ONNX model is generated by converting a PyTorch model. The test script has been inspired from the tests in the PyTorch repo. The model is from `torchvision` model zoo, MaskRCNN and FasterRCNN converted with opset=11.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torchvision\r\nimport onnxruntime\r\nimport onnx\r\n\r\nimport io\r\nimport copy\r\n\r\nimport argparse\r\n\r\n\r\ndef ort_test_with_input(ort_sess, input, output, rtol, atol):\r\n    input, _ = torch.jit._flatten(input)\r\n    output, _ = torch.jit._flatten(output)\r\n\r\n    def to_numpy(tensor):\r\n        if tensor.requires_grad:\r\n            return tensor.detach().cpu().numpy()\r\n        else:\r\n            return tensor.cpu().numpy()\r\n\r\n    inputs = list(map(to_numpy, input))\r\n    outputs = list(map(to_numpy, output))\r\n\r\n    ort_inputs = dict((ort_sess.get_inputs()[i].name, input) for i, input in enumerate(inputs))\r\n    ort_outs = ort_sess.run(None, ort_inputs)\r\n\r\n    # compare onnxruntime and PyTorch results\r\n    assert len(outputs) == len(ort_outs), \"number of outputs differ\"\r\n\r\n    # compare onnxruntime and PyTorch results\r\n    [np.testing.assert_allclose(out, ort_out, rtol=rtol, atol=atol) for out, ort_out in zip(outputs, ort_outs)]\r\n\r\n\r\ndef run_model_test(model,\r\n                   batch_size=2,\r\n                   state_dict=None,\r\n                   input=None,\r\n                   use_gpu=True,\r\n                   rtol=0.001,\r\n                   atol=1e-7,\r\n                   example_outputs=None,\r\n                   do_constant_folding=True,\r\n                   dynamic_axes=None,\r\n                   test_with_inputs=None,\r\n                   input_names=None,\r\n                   output_names=None,\r\n                   fixed_batch_size=False,\r\n                   save_and_read_from_disk=False):\r\n    model.eval()\r\n\r\n    if input is None:\r\n        input = torch.randn(batch_size, 3, 224, 224, requires_grad=True)\r\n\r\n    with torch.no_grad():\r\n        if isinstance(input, torch.Tensor):\r\n            input = (input, )\r\n        # In-place operators will update input tensor data as well.\r\n        # Thus inputs are replicated before every forward call.\r\n        input_copy = copy.deepcopy(input)\r\n        output = model(*input_copy)\r\n        if isinstance(output, torch.Tensor):\r\n            output = (output, )\r\n\r\n        # export the model to ONNX\r\n        if save_and_read_from_disk:\r\n            f = \"mrcnn_test.onnx\"\r\n        else:\r\n            f = io.BytesIO()\r\n        input_copy = copy.deepcopy(input)\r\n        torch.onnx.export(model,\r\n                          input_copy,\r\n                          f,\r\n                          opset_version=11,\r\n                          example_outputs=output,\r\n                          do_constant_folding=True,\r\n                          dynamic_axes=dynamic_axes,\r\n                          input_names=input_names,\r\n                          output_names=output_names)\r\n\r\n        # compute onnxruntime output prediction\r\n        if save_and_read_from_disk:\r\n            print('checking model: {} file'.format(f))\r\n            loaded_model = onnx.load(f)\r\n            onnx.checker.check_model(loaded_model)\r\n            ort_sess = onnxruntime.InferenceSession(loaded_model)\r\n        else:\r\n            ort_sess = onnxruntime.InferenceSession(f.getvalue())\r\n        input_copy = copy.deepcopy(input)\r\n        ort_test_with_input(ort_sess, input_copy, output, rtol, atol)\r\n\r\n        # if additional test inputs are provided run the onnx\r\n        # model with these inputs and check the outputs\r\n        if test_with_inputs is not None:\r\n            for test_input in test_with_inputs:\r\n                if isinstance(test_input, torch.Tensor):\r\n                    test_input = (test_input, )\r\n                test_input_copy = copy.deepcopy(test_input)\r\n                output = model(*test_input_copy)\r\n                if isinstance(output, torch.Tensor):\r\n                    output = (output, )\r\n                ort_test_with_input(ort_sess, test_input, output, rtol, atol)\r\n\r\n\r\ndef run(save_and_read_from_disk=False, mrcnn=False):\r\n    if mrcnn:\r\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True, min_size=200, max_size=300)\r\n    else:\r\n        model = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained=True,\r\n                                                                                 min_size=200,\r\n                                                                                 max_size=300)\r\n    model.eval()\r\n    x = torch.randn(2, 3, 200, 300, requires_grad=True)\r\n    run_model_test(model,\r\n                   batch_size=2,\r\n                   input=(x, ),\r\n                   use_gpu=True,\r\n                   rtol=1e-3,\r\n                   atol=1e-5,\r\n                   do_constant_folding=True,\r\n                   dynamic_axes=None,\r\n                   test_with_inputs=None,\r\n                   input_names=None,\r\n                   output_names=None,\r\n                   fixed_batch_size=None,\r\n                   save_and_read_from_disk=save_and_read_from_disk)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='Run tests with ONNX')\r\n    parser.add_argument('--save_and_read_from_disk',\r\n                        action='store_true',\r\n                        help='writes onnx to disk and then loads it to run test')\r\n    parser.add_argument('--mrcnn', action='store_true', help='use maskrcnn or otherwise use fastercnn')\r\n    args = parser.parse_args()\r\n    run(save_and_read_from_disk=args.save_and_read_from_disk, mrcnn=args.mrcnn)\r\n\r\n```\r\n**MaskRCNN**\r\nWhen running the above as `python from_torchvision_tests.py --mrcnn` gives `onnxruntime.capi.onnxruntime_pybind11_state.Fail: [ONNXRuntimeError] : 1 : FAIL : Node (ConstantOfShape_1966) Op (ConstantOfShape) [ShapeInferenceError] Invalid shape value: 0` while `python from_torchvision_tests.py --save_and_read_from_disk --mrcnn` throws `TypeError: Unable to load from type '<class 'onnx.onnx_ONNX_REL_1_6_ml_pb2.ModelProto'>`\r\n\r\n**FasterRCNN**\r\nWhen running the above as `python from_torchvision_tests.py` runs without any errors while `python from_torchvision_tests.py --save_and_read_from_disk` throws the aforementioned.\r\n\r\n## Expected behavior\r\n\r\nLoading from disk should not change behavior and work without errors instead it throws error `TypeError: Unable to load from type '<class 'onnx.onnx_ONNX_REL_1_6_ml_pb2.ModelProto'>`. It seems to not work for either MaskRCNN or FasterRCNN. FasterRCNN does not throw errors when loaded from buffer but MaskRCNN throws a new error in this case.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.5.0.dev20200217\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 435.21\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.5.0.dev20200217\r\n[pip3] torchvision==0.6.0.dev20200217\r\n[conda] Could not collect\r\n\r\n- ONNX Runtime installed from (source or binary):pip\r\n- ONNX Runtime version: onnxruntime (1.1.1)  onnx (1.6.0) \r\n```\r\n\r\n## Additional context\r\n\r\n**FasterRCNN Screenshots**\r\nWithout errors:\r\n![image](https://user-images.githubusercontent.com/8938083/74826242-8b989600-530b-11ea-85e7-16969aefa80d.png)\r\n\r\nWith error:\r\n![image](https://user-images.githubusercontent.com/8938083/74826318-aec34580-530b-11ea-9170-aa21f832badb.png)\r\n\r\n**MaskRCNN screenshots**\r\nLoading from buffer error:\r\n![image](https://user-images.githubusercontent.com/8938083/74843122-c4486780-532b-11ea-8322-9e1bd834f0bf.png)\r\n\r\nLoading from disk error:\r\n![image](https://user-images.githubusercontent.com/8938083/74843200-e0e49f80-532b-11ea-8a89-8eab0b5a652f.png)\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof @fmassa", "labels": ["module: onnx", "module: vision", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 10:45:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567410502": {"author_username": "jingxu10", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33488, "title": "Incomplete conversion from PyTorch model to ONNX model", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.get code from https://github.com/Coldog2333/pytoflow.git\r\n2.train a PyTorch model\r\n3.convert the PyTorch model to ONNX\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\nlog with JIT trace (input of the generated ONNX model becomes shape of the original input tensor):\r\npytoflow/Network.py:211: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  for i in range(frames.size(1)):\r\npytoflow/Network.py:212: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  frames[:, i, :, :, :] = normalize(frames[:, i, :, :, :])\r\npytoflow/Network.py:77: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if tensorFirst[0].size(2) > 32 or tensorFirst[0].size(3) > 32:\r\npytoflow/Network.py:82: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  int(math.floor(tensorFirst[0].size(2) / 2.0)),\r\npytoflow/Network.py:83: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  int(math.floor(tensorFirst[0].size(3) / 2.0)))\r\npytoflow/Network.py:89: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if tensorUpsampled.size(2) != tensorFirst[intLevel].size(2):\r\npytoflow/Network.py:91: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if tensorUpsampled.size(3) != tensorFirst[intLevel].size(3):\r\npytoflow/Network.py:27: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if str(tensorFlow.size()) not in Backward_tensorGrid:\r\npytoflow/Network.py:28: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  tensorHorizontal = torch.linspace(-1.0, 1.0, tensorFlow.size(3)).view(1, 1, 1, tensorFlow.size(3)).expand(tensorFlow.size(0), -1, tensorFlow.size(2), -1)\r\npytoflow/Network.py:29: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  tensorVertical = torch.linspace(-1.0, 1.0, tensorFlow.size(2)).view(1, 1, tensorFlow.size(2), 1).expand(tensorFlow.size(0), -1, -1, tensorFlow.size(3))\r\npytoflow/Network.py:33: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  Backward_tensorGrid[str(tensorFlow.size())] = torch.cat([tensorHorizontal, tensorVertical], 1)\r\npytoflow/Network.py:38: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  return torch.nn.functional.grid_sample(input=tensorInput, grid=(Backward_tensorGrid[str(tensorFlow.size())] + tensorFlow).permute(0, 2, 3, 1), mode='bilinear', padding_mode='border')\r\npytorch_py37/lib/python3.7/site-packages/torch/nn/functional.py:2705: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\r\n  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\r\npytoflow/Network.py:223: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  opticalflows[:, 1, :, :, :] = self.SpyNet(frames[:, 0, :, :, :], frames[:, 1, :, :, :]) / 2\r\npytoflow/Network.py:224: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  opticalflows[:, 0, :, :, :] = self.SpyNet(frames[:, 1, :, :, :], frames[:, 0, :, :, :]) / 2\r\npytoflow/Network.py:235: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  warpframes[:, i, :, :, :] = self.warp(frames[:, i, :, :, :], opticalflows[:, i, :, :, :])\r\npytoflow/Network.py:182: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  for i in range(1, frames.size(1)):\r\ngraph(%0 : Float(1, 2, 3, 256, 448),\r\n      %SpyNet.moduleBasic.0.moduleBasic.0.weight : Float(32, 8, 7, 7),\r\n      %SpyNet.moduleBasic.0.moduleBasic.0.bias : Float(32),\r\n      %SpyNet.moduleBasic.0.moduleBasic.2.weight : Float(64, 32, 7, 7),\r\n      %SpyNet.moduleBasic.0.moduleBasic.2.bias : Float(64),\r\n      %SpyNet.moduleBasic.0.moduleBasic.4.weight : Float(32, 64, 7, 7),\r\n      %SpyNet.moduleBasic.0.moduleBasic.4.bias : Float(32),\r\n      %SpyNet.moduleBasic.0.moduleBasic.6.weight : Float(16, 32, 7, 7),\r\n      %SpyNet.moduleBasic.0.moduleBasic.6.bias : Float(16),\r\n      %SpyNet.moduleBasic.0.moduleBasic.8.weight : Float(2, 16, 7, 7),\r\n      %SpyNet.moduleBasic.0.moduleBasic.8.bias : Float(2),\r\n      %SpyNet.moduleBasic.1.moduleBasic.0.weight : Float(32, 8, 7, 7),\r\n      %SpyNet.moduleBasic.1.moduleBasic.0.bias : Float(32),\r\n      %SpyNet.moduleBasic.1.moduleBasic.2.weight : Float(64, 32, 7, 7),\r\n      %SpyNet.moduleBasic.1.moduleBasic.2.bias : Float(64),\r\n      %SpyNet.moduleBasic.1.moduleBasic.4.weight : Float(32, 64, 7, 7),\r\n      %SpyNet.moduleBasic.1.moduleBasic.4.bias : Float(32),\r\n      %SpyNet.moduleBasic.1.moduleBasic.6.weight : Float(16, 32, 7, 7),\r\n      %SpyNet.moduleBasic.1.moduleBasic.6.bias : Float(16),\r\n      %SpyNet.moduleBasic.1.moduleBasic.8.weight : Float(2, 16, 7, 7),\r\n      %SpyNet.moduleBasic.1.moduleBasic.8.bias : Float(2),\r\n      %SpyNet.moduleBasic.2.moduleBasic.0.weight : Float(32, 8, 7, 7),\r\n      %SpyNet.moduleBasic.2.moduleBasic.0.bias : Float(32),\r\n      %SpyNet.moduleBasic.2.moduleBasic.2.weight : Float(64, 32, 7, 7),\r\n      %SpyNet.moduleBasic.2.moduleBasic.2.bias : Float(64),\r\n      %SpyNet.moduleBasic.2.moduleBasic.4.weight : Float(32, 64, 7, 7),\r\n      %SpyNet.moduleBasic.2.moduleBasic.4.bias : Float(32),\r\n      %SpyNet.moduleBasic.2.moduleBasic.6.weight : Float(16, 32, 7, 7),\r\n      %SpyNet.moduleBasic.2.moduleBasic.6.bias : Float(16),\r\n      %SpyNet.moduleBasic.2.moduleBasic.8.weight : Float(2, 16, 7, 7),\r\n      %SpyNet.moduleBasic.2.moduleBasic.8.bias : Float(2),\r\n      %SpyNet.moduleBasic.3.moduleBasic.0.weight : Float(32, 8, 7, 7),\r\n      %SpyNet.moduleBasic.3.moduleBasic.0.bias : Float(32),\r\n      %SpyNet.moduleBasic.3.moduleBasic.2.weight : Float(64, 32, 7, 7),\r\n      %SpyNet.moduleBasic.3.moduleBasic.2.bias : Float(64),\r\n      %SpyNet.moduleBasic.3.moduleBasic.4.weight : Float(32, 64, 7, 7),\r\n      %SpyNet.moduleBasic.3.moduleBasic.4.bias : Float(32),\r\n      %SpyNet.moduleBasic.3.moduleBasic.6.weight : Float(16, 32, 7, 7),\r\n      %SpyNet.moduleBasic.3.moduleBasic.6.bias : Float(16),\r\n      %SpyNet.moduleBasic.3.moduleBasic.8.weight : Float(2, 16, 7, 7),\r\n      %SpyNet.moduleBasic.3.moduleBasic.8.bias : Float(2),\r\n      %ResNet.conv_3x2_64_9x9.weight : Float(64, 6, 9, 9),\r\n      %ResNet.conv_3x2_64_9x9.bias : Float(64),\r\n      %ResNet.conv_3x7_64_9x9.weight : Float(64, 21, 9, 9),\r\n      %ResNet.conv_3x7_64_9x9.bias : Float(64),\r\n      %ResNet.conv_64_64_9x9.weight : Float(64, 64, 9, 9),\r\n      %ResNet.conv_64_64_9x9.bias : Float(64),\r\n      %ResNet.conv_64_64_1x1.weight : Float(64, 64, 1, 1),\r\n      %ResNet.conv_64_64_1x1.bias : Float(64),\r\n      %ResNet.conv_64_3_1x1.weight : Float(3, 64, 1, 1),\r\n      %ResNet.conv_64_3_1x1.bias : Float(3)):\r\n  %51 : Tensor = onnx::Shape(%0)\r\n  %52 : Tensor = onnx::Constant[value={0}]()\r\n  %53 : Long() = onnx::Gather[axis=0](%51, %52) # /home/intel/Documents/Frameworks/PyTorch/exps/pytoflow/Network.py:219:0\r\n  %54 : Tensor = onnx::Shape(%0)\r\n  %55 : Tensor = onnx::Constant[value={1}]()\r\n  %56 : Long() = onnx::Gather[axis=0](%54, %55) # /home/intel/Documents/Frameworks/PyTorch/exps/pytoflow/Network.py:219:0\r\n  %57 : Tensor = onnx::Shape(%0)\r\n  %58 : Tensor = onnx::Constant[value={3}]()\r\n  %59 : Long() = onnx::Gather[axis=0](%57, %58) # /home/intel/Documents/Frameworks/PyTorch/exps/pytoflow/Network.py:219:0\r\n  %60 : Tensor = onnx::Shape(%0)\r\n  %61 : Tensor = onnx::Constant[value={4}]()\r\n  %62 : Long() = onnx::Gather[axis=0](%60, %61) # /home/intel/Documents/Frameworks/PyTorch/exps/pytoflow/Network.py:219:0\r\n\r\nlog with JIT script (failed to do the conversion with JIT script mode):\r\nTraceback (most recent call last):\r\n  File \"evaluate.py\", line 127, in <module>\r\n    vimeo_evaluate(dataset_dir, './evaluate', pathlistfile, task=task, cuda_flag=cuda_flag)\r\n  File \"evaluate.py\", line 119, in vimeo_evaluate\r\n    onnx.export(torch.jit.script(net), input_frames, 'model.onnx', verbose=True)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/__init__.py\", line 1255, in script\r\n    return torch.jit._recursive.recursive_script(obj)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 296, in create_script_module\r\n    return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 336, in create_script_module_impl\r\n    script_module = torch.jit.RecursiveScriptModule._construct(cpp_module, init_fn)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/__init__.py\", line 1593, in _construct\r\n    init_fn(script_module)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 328, in init_fn\r\n    scripted = recursive_script(orig_value)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 296, in create_script_module\r\n    return create_script_module_impl(nn_module, concrete_type, cpp_module, stubs)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 340, in create_script_module_impl\r\n    create_methods_from_stubs(concrete_type, stubs)\r\n  File \"pytorch_py37/lib/python3.7/site-packages/torch/jit/_recursive.py\", line 259, in create_methods_from_stubs\r\n    concrete_type._create_methods(defs, rcbs, defaults)\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following variants are available:\r\n  \r\n  aten::avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=[0, 0], bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> (Tensor):\r\n  Argument self not provided.\r\n  \r\n  aten::avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=[0, 0], bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> (Tensor(a!)):\r\n  Argument self not provided.\r\n\r\nThe original call is:\r\n  File \"pytoflow/Network.py\", line 78\r\n        for intLevel in range(3):\r\n            if tensorFirst[0].size(2) > 32 or tensorFirst[0].size(3) > 32:\r\n                tensorFirst.insert(0, torch.nn.functional.avg_pool2d(input=tensorFirst[0], kernel_size=2, stride=2))\r\n                                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\r\n                tensorSecond.insert(0, torch.nn.functional.avg_pool2d(input=tensorSecond[0], kernel_size=2, stride=2))\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\nfully convert PyTorch model to ONNX one. Currently, the input of the generated ONNX model is shape of the original input tensor with JIT trace mode. JIT script mode doesn't work.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n - PyTorch Version (e.g., 1.0): 1.4.0+cpu\r\n - OS (e.g., Linux): Ubuntu 18.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.7\r\n - CUDA/cuDNN version: N/A\r\n - GPU models and configuration: N/A\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 08:52:48", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567404707": {"author_username": "vcjob", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33487, "title": "QuantStub doesn't convert tensor to int8 in quantized model", "body": "Hello everyone!\r\n\r\nI try to quantize a Tacotron2 model with dynamic quantization. \r\nTorch version = 1.4.0\r\nWhat I do:\r\n```\r\nmodel_tts = Tacotron()\r\ncp = torch.load(config.model_path, map_location=torch.device('cpu'))\r\nmodel_tts.load_state_dict(cp['model'])\r\nmodel_tts.eval().cpu()\r\ntorch.quantization.quantize_dynamic(model_tts, inplace = True)\r\n```\r\nBesides, I tried to change input in _forward()_ with QuantStub.\r\nThe _forward_ method takes text as an argument. The text is represented by a tensor with shape [1, x] where x is the number of symbols. All symbols are integers (see picture below).\r\n![image](https://user-images.githubusercontent.com/51916323/74816386-1da09e80-530c-11ea-8669-6f9019aac6d2.png)\r\nRight after that input is being passed to nn.Embedding layer, which returns float embedding (see pic. below).\r\n![image](https://user-images.githubusercontent.com/51916323/74817151-80df0080-530d-11ea-8b19-54ef2d26c0e0.png)\r\n**My question is:**\r\nWhy there are floats in the quantized model?\r\nWhy when I try to quantize embeddings manually with QuantStub(embedding), it returns exactly the same float embedding? \r\n```\r\nfrom torch.quantization import QuantStub\r\n.....\r\nself.quant = QuantStub()\r\n.....\r\nembedded_inputs = self.quant(self.embedding(text).transpose(1, 2))\r\nprint(type(embedded_inputs), embedded_inputs)\r\n```\r\nWhat should I do to quantize my model?\r\n\r\nThank you!\r\n\r\n-------\r\nAfter I quantize the model and print it, what I get is:\r\n```\r\nSize (MB): 115.603768\r\nSize (MB): 105.853096\r\nTacotron2(\r\n  (embedding): Embedding(67, 512)\r\n  (encoder): Encoder(\r\n    (convolutions): Sequential(\r\n      (0): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): ReLU()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (1): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): ReLU()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (2): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): ReLU()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n    )\r\n    (lstm): DynamicQuantizedLSTM(\r\n      512, 256, batch_first=True, bidirectional=True\r\n      (_all_weight_values): ModuleList(\r\n        (0): PackedParameter()\r\n        (1): PackedParameter()\r\n        (2): PackedParameter()\r\n        (3): PackedParameter()\r\n      )\r\n    )\r\n  )\r\n  (decoder): Decoder(\r\n    (prenet): Prenet(\r\n      (layers): ModuleList(\r\n        (0): Linear(\r\n          (linear_layer): DynamicQuantizedLinear(\r\n            in_features=80, out_features=256\r\n            (_packed_params): LinearPackedParams()\r\n          )\r\n        )\r\n        (1): Linear(\r\n          (linear_layer): DynamicQuantizedLinear(\r\n            in_features=256, out_features=256\r\n            (_packed_params): LinearPackedParams()\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (attention_rnn): LSTMCell(768, 1024)\r\n    (attention): OriginalAttention(\r\n      (query_layer): Linear(\r\n        (linear_layer): DynamicQuantizedLinear(\r\n          in_features=1024, out_features=128\r\n          (_packed_params): LinearPackedParams()\r\n        )\r\n      )\r\n      (inputs_layer): Linear(\r\n        (linear_layer): DynamicQuantizedLinear(\r\n          in_features=512, out_features=128\r\n          (_packed_params): LinearPackedParams()\r\n        )\r\n      )\r\n      (v): Linear(\r\n        (linear_layer): DynamicQuantizedLinear(\r\n          in_features=128, out_features=1\r\n          (_packed_params): LinearPackedParams()\r\n        )\r\n      )\r\n      (location_layer): LocationLayer(\r\n        (location_conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)\r\n        (location_dense): Linear(\r\n          (linear_layer): DynamicQuantizedLinear(\r\n            in_features=32, out_features=128\r\n            (_packed_params): LinearPackedParams()\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (decoder_rnn): LSTMCell(1536, 1024, bias=1)\r\n    (linear_projection): Linear(\r\n      (linear_layer): DynamicQuantizedLinear(\r\n        in_features=1536, out_features=560\r\n        (_packed_params): LinearPackedParams()\r\n      )\r\n    )\r\n    (stopnet): Sequential(\r\n      (0): Dropout(p=0.1, inplace=False)\r\n      (1): Linear(\r\n        (linear_layer): DynamicQuantizedLinear(\r\n          in_features=1584, out_features=1\r\n          (_packed_params): LinearPackedParams()\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (postnet): Postnet(\r\n    (convolutions): ModuleList(\r\n      (0): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): Tanh()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (1): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): Tanh()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (2): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): Tanh()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (3): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): Tanh()\r\n          (3): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n      (4): ConvBNBlock(\r\n        (net): Sequential(\r\n          (0): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))\r\n          (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\r\n          (2): Dropout(p=0.5, inplace=False)\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (quant): QuantStub()\r\n  (dequant): DeQuantStub()\r\n)\r\n```\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a", "labels": ["quantization", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 08:41:45", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567381373": {"author_username": "cafeal", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33486, "title": "Fix HistogramObserver to work with GPUs", "body": "I found HistogramObserver cannot work with CUDA", "labels": ["open source", "quantization", "triaged"], "number_of_comments": 1, "created_at": "2020-02-19 07:53:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567377170": {"author_username": "SamPruden", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33485, "title": "torch.rfft returns NaNs for some half precision CUDA inputs", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using half precision and CUDA, torch.rfft sometimes returns NaNs.\r\n\r\n## To Reproduce\r\n\r\n```Python3\r\nsize = 64\r\nwn = torch.rand(size, size, size, dtype = torch.half, device = \"cuda\")\r\nassert torch.isfinite(wn).all()\r\nwn_freq = torch.rfft(wn, 3, onesided = True)\r\nassert torch.isfinite(wn_freq).all() # <- FAILS\r\n```\r\n\r\nSetting `size` to 32 or smaller seem to work fine, 64 or bigger seems to fail. I haven't tested this rigorously. I'm 80% sure that I was running the same code on the same setup with the same version without hitting this error a few days ago, but I can't be sure. The issue survives a restart of the machine.\r\n\r\n## Expected behavior\r\n\r\nNot NaNs.\r\n\r\n## Environment\r\nGoogle Colab.\r\n\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla P100-PCIE-16GB\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.5\r\n[pip3] torch==1.4.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n```\n\ncc @ngimel", "labels": ["module: cuda", "module: half", "topic: NaNs and Infs", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 07:44:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567326296": {"author_username": "hczhu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33482, "title": "Distributed Data Parallel for computation graphs that make RPCs in forward()", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nThe current [DDP implementation](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) require a computation graph is on a single node (maybe multiple devices). DDP won't work if a computation graph makes RPCs in its `forward()` method. This feature is to make DDP work for such computation graphs by leveraging the existing [Distributed Autograd/Optimizer](https://pytorch.org/docs/stable/notes/distributed_autograd.html).\r\n\r\nNote that the parameter servers can't use DDP in this use case.\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nTo support the following model\r\n\r\n```\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, d_in, d_out):\r\n        super(SimpleNet, self).__init__()\r\n        self.net = nn.Linear(d_in, d_out)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(self, input):\r\n        return self.relu(self.net(input))\r\n\r\nclass DdpModelWithRpc(nn.Module):\r\n    def __init__(self, remote_parameter_server):\r\n        super(DdpModelWithRpc, self).__init__()\r\n        self.net1 = DDP(SimpleNet(5, 8))\r\n        self.rps = remote_parameter_server\r\n        self.net2 = DDP(SimpleNet(5, 3))\r\n\r\n    def forward(self, x):\r\n        x = self.net1(x)\r\n        x = RPC(self.rps, another_remote_net, x)\r\n        return self.net2(x)\r\n\r\n```\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nTDB\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\nTDB\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\r\n\r\nCC: @pritamdamania87  @mrshenli @zhaojuanmao \r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley", "labels": ["module: distributed", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 05:28:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567323537": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33481, "title": "[quant][graphmode] Handling ops doesn't require observation in insertObservers", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* **#33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers**\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWe have to propagate observed values through ops like max_pool2d, flatten and\navoid inserting duplicated observers.\nFor example:\n```\nx1 = self.conv(x)\nx2 = maxpool(x1)\nx3 = self.conv(x2)\n```\nIf x1 is observed, we should pass this information through maxpool and\nwe should consider x2 as observed as well.\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-19 05:19:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567286771": {"author_username": "pritamdamania87", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33480, "title": "Make distributed autograd and distributed optimizer APIs functional.", "body": "Code while using distributed autograd and distributed optimizer currently looks like this:\r\n\r\n```\r\nwith dist_autograd.context() as context_id:\r\n  // forward pass.\r\n  dist_autograd.backward([loss.sum()])\r\n  dist_optim.step()\r\n```\r\n\r\n`dist_autograd.backward` and `dist_optim.step` currently assume the thread local `context_id` as the context id that we would like to work with. This is a bit magical to users currently and relies on an implementation detail where `context_id` is a thread_local. We should make these APIs more functional where we pass the context_id explicitly. This way the APIs are more functional and it also provides more flexibility for users such that they can call these APIs in a separate thread if they wish to.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley", "labels": ["module: distributed", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 03:11:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567278578": {"author_username": "pritamdamania87", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33479, "title": "Enable torch.jit._fork() within distributed autograd context", "body": "Currently, if we have code as follows:\r\n\r\n```\r\n@torch.jit.script\r\ndef foo(t1, t2):\r\n  return rpc_async(torch.add, t1, t2);\r\n\r\nt1 = torch.rand((3, 3), requires_grad=True)\r\nt2 = torch.rand((3, 3), requires_grad=True)\r\nwith dist_autograd.context() as context:\r\n  loss = torch.jit._fork(foo, t1, t2).wait()\r\n  dist_autograd.backward([loss])\r\n```\r\n\r\nThis won't work as expected for the user since the autograd context would not be carried over to the JIT thread and the rpc_async call within `foo` would not be recorded as part of the autograd graph. We should carry over the autograd context to the JIT thread similar to what was done in https://github.com/pytorch/pytorch/pull/16101/files.\r\n\r\n\n\ncc @suo @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["jit", "module: distributed", "module: rpc"], "number_of_comments": 0, "created_at": "2020-02-19 02:42:28", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567269259": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33477, "title": "Add 32-bit CI (e.g., Raspberry PI CI)", "body": "c.f. #33456\n\ncc @ezyang", "labels": ["module: ci", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 02:09:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567268426": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33476, "title": "Make ArrayRef::size() return int64_t rather than size_t", "body": "Would have prevented #33001 (PR at #33456)", "labels": ["module: internals", "triaged"], "number_of_comments": 0, "created_at": "2020-02-19 02:05:51", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567264296": {"author_username": "jlin27", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33475, "title": "[don't merge] Build 1.4.0 stable docs with Google Analytics tag", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 01:50:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567258721": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33474, "title": "[jit] Remove list specializations from pickler", "body": "Stacked PRs\n * **#33474 - [jit] Remove list specializations from pickler**\n * #33255 - [jit] Add type tags to lists/dicts in pickle", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-19 01:29:59", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567257298": {"author_username": "twostay", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33473, "title": "JIT does not support binary operators", "body": "## \ud83d\udc1b Bug\r\n\r\nI have a code that looks like this\r\n```\r\nimport torch as tc\r\nfrom torch import jit\r\n@jit.script\r\ndef func(inp):\r\n    return inp<<1\r\na = tc.tensor([3,4,5])\r\nfunc(a)\r\n```\r\nwhen I run it, I got the error:\r\n```\r\ntorch.jit.frontend.NotSupportedError: unsupported binary operator: LShift:\r\n  File \"example.py\", line 5\r\n@jit.script\r\ndef func(inp):\r\n    return inp<<1\r\n              ~~ <--- HERE\r\n```\r\nHowever, if I change `inp<<1` to `inp.__lshift__(1)`, it works. This also happens in C++.\r\n\r\n## Expected behavior\r\n\r\nI expect the binary operators to work without changing them to the call to methods.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0+cpu\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Home\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.4.0+cpu\r\n[pip3] torchvision==0.5.0+cpu\r\n[conda] Could not collect\r\n## Additional context\r\n\r\nIt is not a very severe problem and I would like to fix it if I know where the source code is located.\r\n\n\ncc @suo", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-02-19 01:24:42", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567253601": {"author_username": "SplitInfinity", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33472, "title": "[JIT] Implement Tensor.tolist()", "body": "**Summary**\r\nThis commit adds an implementation of `Tensor.tolist()` to the JIT interpreter.\r\n\r\n**Testing**\r\nThis commit adds several unit tests that test that this function works correctly for\r\n0D, 1D, 2D and 3D tensors of type `float`, `int` and `bool`.\r\n\r\n```\r\n(base) meghanl-mbp:pytorch meghanl$ python test/test_jit.py TestList.test_to_list -v\r\nFail to import hypothesis in common_utils, tests are not derandomized\r\ntest_to_list (jit.test_list_dict.TestList)\r\nUnit tests for Tensor.tolist() function. ... ok\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.329s\r\n\r\nOK\r\n```\r\n\r\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-19 01:12:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567238946": {"author_username": "seemethere", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33471, "title": ".circleci: Add CUDA 10.2 to our CI pipeline", "body": "Currently a draft, still testing\r\n\r\nDepends on https://github.com/pytorch/builder/pull/404", "labels": [], "number_of_comments": 1, "created_at": "2020-02-19 00:21:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567211507": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33468, "title": "[JIT] remove inline everything jitter skip", "body": "The `not inline_everything` check was causing the jitter check to be skipped whenever we emitted a function. thanks @SplitInfinity for pointing this out.", "labels": [], "number_of_comments": 1, "created_at": "2020-02-18 23:01:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567203867": {"author_username": "andrewdelong", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33467, "title": "baddbmm(c, a, b) forgets to add c when a @ b inner dimension is empty", "body": "## \ud83d\udc1b Bug\r\n\r\nCorner case where `torch.baddmm(c, a, b)` doesn't match `torch.bmm(a, b) + c`: if the inner dimension of *a* and *b* is empty, the result is *zeros_like(c)* rather than  *c*.\r\n\r\nEasy to work around, but gives subtly wrong results, so maybe worth fixing.\r\n\r\n## To Reproduce\r\n\r\n```python\r\na = torch.zeros((2, 1, 0))\r\nb = torch.zeros((2, 0, 1))\r\nc = torch.ones((2, 1, 1))\r\ntorch.bmm(a, b) + c     # <-- ones((2, 1, 1))  (OK)\r\ntorch.baddbmm(c, a, b)  # <-- zeros((2, 1, 1)) (WRONG)\r\n```\r\n## Possible Fix\r\n\r\nThe bug seems to be in `bmm_out_or_baddbmm_` from `aten/src/ATen/native/LinearAlgebra.cpp`:\r\n```c++\r\n  // handle pathological cases that blas may not like\r\n  if (self_or_result.numel() == 0) {\r\n    return self_or_result;\r\n  } else if (contraction_size == 0) {\r\n    return self_or_result.zero_();       // <-- BUG for baddbmm\r\n  }\r\n```\r\nReplacing that line with this seems to fix it:\r\n```c++\r\n    if (is_bmm_out) {\r\n      return self_or_result.zero_();\r\n    } else {\r\n      return self_or_result.mul_(beta);\r\n    }\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch 1.5.0a0+6dd6b0b\r\n - MacOS\r\n - conda install\r\n - `USE_CUDA=0 python setup.py develop`\r\n - Python 3.8.1", "labels": ["module: operators", "triaged"], "number_of_comments": 1, "created_at": "2020-02-18 22:42:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567201210": {"author_username": "supriyar", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33466, "title": "Quantized Convolution error on Mobile with some scale values.", "body": "## \ud83d\udc1b Bug\r\n\r\nQNNPACK throws an error with certain scale values for input and weight tensors. The error happens when the convolution scale is greater than 1.0. convolution scale is computed as `input_scale * kernel_scale / output_scale;`\r\n\r\nThis is a problem which arises when model is trained with QAT and run on QNNPACK mobile backend.\r\n\r\n## To Reproduce\r\n\r\nScript to repro the error\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nqconv = torch.ops.quantized.conv2d\r\nqconv_prepack = torch.ops.quantized.conv2d_prepack\r\n\r\nstrides = (1, 1)\r\npads = (0, 0)\r\ndilations = (1, 1)\r\ngroups = 1\r\n\r\n\r\nfor name in [\"fbgemm\", \"qnnpack\"]:\r\n    torch.backends.quantized.engine = name\r\n    print(\"Running on backend \", name)\r\n    x = torch.randn(1, 4, 4, 4)\r\n    qx = torch.quantize_per_tensor(x, scale=0.052, zero_point=0, dtype=torch.quint8)\r\n    weight = torch.randn(2, 4, 2, 2)\r\n    qweight = torch.quantize_per_tensor(weight, scale=2.39, zero_point=0, dtype=torch.qint8)\r\n    w_prepack = qconv_prepack(qweight, None, strides, pads, dilations, groups)\r\n    print(qconv(qx, w_prepack, strides, pads, dilations, groups, 0.112, 0))\r\n```\r\n\r\n## Expected behavior\r\n\r\nOutput from FBGEMM \r\n```\r\ntensor([[[[0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000],\r\n          [0.0000, 0.0000, 0.0000]],\r\n\r\n         [[1.2320, 0.2240, 0.0000],\r\n          [0.0000, 0.0000, 2.6880],\r\n          [0.4480, 0.0000, 0.0000]]]], size=(1, 2, 3, 3), dtype=torch.quint8,\r\n       quantization_scheme=torch.per_tensor_affine, scale=0.112, zero_point=0)\r\n```\r\n\r\nOutput from QNNPACK\r\n`Error in QNNPACK: failed to create convolution with 0.052 input scale, 2.39 kernel scale, and 0.112 output scale: convolution scale 1.109643 is greater or equal to 1.0`\r\n\r\n\r\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a, @kimishpatel \r\n", "labels": ["mobile", "quantization", "triaged"], "number_of_comments": 3, "created_at": "2020-02-18 22:36:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567191394": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33464, "title": "[TensorExpr] Add a boilerplate pass for future TensorExpr fusion pass.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33464 [TensorExpr] Add a boilerplate pass for future TensorExpr fusion pass.**\n\nI added a python-exposed knob to register this pass in custom passes pipeline. If the knob is not used, the pass is not registered and thus not run at all.\n\nDifferential Revision: [D19958217](https://our.internmc.facebook.com/intern/diff/D19958217)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-18 22:14:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567177189": {"author_username": "xvdp", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33463, "title": "torch._C.Node.scopeName() missing in pytorch 1.4", "body": "## \ud83d\udc1b Bug\r\n\r\nscopeName() appears as a method in pytorch 1.4 torch._C.Node but it seems to lo longer be written to.\r\nIt is minor but doesn't seem to be listed as an intended change, so I surmise is a bug.\r\n\r\nIn the meantime, If there's a workaround to get the scopename pls let me know.\r\nThank you\r\n## To Reproduce\r\n\r\n1.run \r\n```python\r\n# setup model: run before either of both scripts\r\nimport torch\r\nfrom torchvision import models\r\nfrom torch.onnx import utils\r\nimage = torch.randn([2,3,224,224])\r\nnet = models.resnet18()\r\n```\r\n2. in pytorch 1.4 access any given valid node ( in resnet this should be a conv node)\r\n```python\r\ntrace_module = torch.jit.trace(net, image)\r\nnode = list(trace_module.graph.nodes())[9] # grab a conv node\r\nprint(node.scopeName())\r\n# ''  # > returns empty string, method exists but it is no longer useful\r\n```\r\n3. for validation access same node in pytorch 1.4 using private method\r\n```python \r\ngraph, out = torch.jit._get_trace_graph(net, image)\r\ngraph = utils._optimize_graph(graph,\r\noperator_export_type=torch._C._onnx.OperatorExportTypes.ONNX)\r\nnode=list(graph.nodes())[11]\r\nprint(node.scopeName())\r\n#'' # > returns nothing\r\n```\r\n## Expected behavior\r\n\r\n4. compare to pytorch 1.3\r\ntorch._C.Node.scopeName() returns a readable address that refers to the python model syntax\r\n```python \r\n# pytorch 1.3,  \r\ntrace, out = torch.jit.get_trace_graph(net, image)\r\ngraph = utils._optimize_graph(trace.graph(), operator_export_type=torch._C._onnx.OperatorExportTypes.ONNX)\r\nnode=list(graph.nodes())[11]\r\nprint(node.scopeName())\r\n#'ResNet/Sequential[layer1]/BasicBlock[1]/Conv2d[conv1]' # > returns scopename\r\n```\r\n\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 440.26\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] mkl                       2019.5                      281    conda-forge\r\n[conda] pytorch                   1.4.0           py3.8_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.5.0                py38_cu100    pytorch\r\n\n\ncc @suo", "labels": ["jit", "triaged"], "number_of_comments": 0, "created_at": "2020-02-18 21:44:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567148522": {"author_username": "VitalyFedyunin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33460, "title": "[WIP] fake channels last support in CUDA `cat_`", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33461 Make slow_conv_transpose2d_backward tensors contiguous\n* **#33460 [WIP] fake channels last support in CUDA `cat_`**\n* #33459 [WIP] Fake channels last in `avg_pool2d`\n* #33458 [WIP] fake channels last support in thnn_conv_depthwise2d call\n\n", "labels": [], "number_of_comments": 1, "created_at": "2020-02-18 20:47:46", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567148467": {"author_username": "VitalyFedyunin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33459, "title": "[WIP] Fake channels last in `avg_pool2d`", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33461 Make slow_conv_transpose2d_backward tensors contiguous\n* #33460 [WIP] fake channels last support in CUDA `cat_`\n* **#33459 [WIP] Fake channels last in `avg_pool2d`**\n* #33458 [WIP] fake channels last support in thnn_conv_depthwise2d call\n\n", "labels": [], "number_of_comments": 1, "created_at": "2020-02-18 20:47:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567148404": {"author_username": "VitalyFedyunin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33458, "title": "[WIP] fake channels last support in thnn_conv_depthwise2d call", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33461 Make slow_conv_transpose2d_backward tensors contiguous\n* #33460 [WIP] fake channels last support in CUDA `cat_`\n* #33459 [WIP] Fake channels last in `avg_pool2d`\n* **#33458 [WIP] fake channels last support in thnn_conv_depthwise2d call**\n\n", "labels": [], "number_of_comments": 1, "created_at": "2020-02-18 20:47:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567118777": {"author_username": "hczhu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33455, "title": "[WIP] Add a new Python unit test for combining DDP and Distributed Autograd/Optimizer", "body": "Summary:\r\nThis test construct a distributed model, which calls a RPC as one of the forward steps. The computation steps are:\r\n`input ==> local DDP net1 ==> remote net (on another node) ==> local DDP net2`\r\n\r\nDDP will update the weights of `net1` and `net2`. While distributed optimizer will update `remote net`.\r\n\r\nTest Plan:\r\n\r\n`python test/distributed/test_ddp_with_rpc.py`\r\n", "labels": [], "number_of_comments": 3, "created_at": "2020-02-18 19:47:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567088336": {"author_username": "jotterbach", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33452, "title": "2nd derivative for im2col and col2im not implemented", "body": "## \ud83d\ude80 Feature\r\n\r\nImplement second derivate for im2col and col2im. The following operations do not allow for backpropping the second derivative\r\n\r\n```\r\n_im2col = nn.Unfold(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\r\n_col2im = nn.Fold(top_shape, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\r\n```\r\n\r\nand error with \r\n\r\n```\r\nRuntimeError: derivative for im2col_backward is not implemented\r\n```\r\n\r\n## Motivation\r\n\r\nThe operation is needed, e.g. when using gradient penalties on architectures that involve gradients of these operations. The workaround using native slicing and reindexing of the tensors is extremely slow, resulting in a 10-20x slow-down. \r\n\r\n## Pitch\r\n\r\nImplement the AD function that generates the 2nd derivative of above operations\r\n\r\n## Alternatives\r\n\r\nSee motivation. Workarounds lead to significant slow-down.  Also see #22274\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n", "labels": ["enhancement", "topic: derivatives", "topic: double backwards", "triaged"], "number_of_comments": 0, "created_at": "2020-02-18 18:48:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567086737": {"author_username": "zasdfgbnm", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33451, "title": "Merge minor cases of GPU loops", "body": "Most of the use cases of GPU loops are for contiguous tensors and no dynamic casting. For other cases, they are not used much, therefore does not worth specialize by so many if statements. \r\n\r\nEnd to end resnet50 benchmark shows no difference in performance: https://github.com/zasdfgbnm/things/blob/master/2020Q1/resnet50-end2end-pr33451.ipynb", "labels": ["module: cuda", "open source", "triaged"], "number_of_comments": 2, "created_at": "2020-02-18 18:45:21", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567062510": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33450, "title": "[C++ API Parity] rmsprop optimizer update", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-18 17:58:51", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567059040": {"author_username": "kurtamohler", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33449, "title": "block_diag dense ", "body": "Add block_diag function for dense tensors, based on scipy.linalg.block_diag\r\n\r\nIssue #31932 ", "labels": ["module: operators", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-18 17:52:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "567006383": {"author_username": "bwasti", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33445, "title": "[hotfix] Remove gratuitously checked in file", "body": "A small test file was accidentally checked in a while back.  Thanks @jamesr66a for catching!", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-18 16:22:25", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 1, "heart": 0, "hooray": 0}}, "566880279": {"author_username": "dxxz", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33443, "title": "When exporting MaskRCNN ONNX model from torchvision, the scale_factor is traced as the constant other than tensor", "body": "## \ud83d\udc1b Bug\r\nWhen exporting MaskRCNN ONNX model from torchvision, the scale_factor is traced as the constant other than tensor. So it makes the detection result of MaskRCNN ONNX model wrong with different size input image. \r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Use PyTorch Nightly built version: pip install https://download.pytorch.org/whl/nightly/cu101/torch-1.5.0.dev20200216-cp36-cp36m-linux_x86_64.whl\r\n1. In my local branch, I merge the workaround in this pull: [pytorch/vision#1840](https://github.com/pytorch/vision/pull/1840), and try with dynamic input size.\r\nI upload the test script used in the local branch: https://gist.github.com/dxxz/3c40b5059bd5efe939ce96752d628158\r\n1. python test_onnx_export.py --sanity-check\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nWhen running the MaskRCNN ONNX model with images of same input size, the detection results are right. But running on the images of different input sizem the detection results are wrong.\r\n\r\nThe logic of about code is using one image to export the Mask-RCNN ONNX model from pre-trained PyTorch model. Then run inference by Mask-RCNN ONNX model and PyTorch model individually with another sanity check image of different input sizes. Finally, compare the detection results by ONNX model and PyTorch model.\r\n\r\nThe compared results as follows:\r\n\r\n    ONNXRuntime inputs:\r\n    image\r\n    [3, 'height', 'width']\r\n    tensor(float)\r\n    ONNXRuntime outputs:\r\n    boxes\r\n    ['obj', 4]\r\n    tensor(float)\r\n    labels\r\n    ['obj']\r\n    tensor(int64)\r\n    scores\r\n    ['obj']\r\n    tensor(float)\r\n    masks\r\n    ['obj', 1, 'height', 'width']\r\n    tensor(float)\r\n\r\n    Sanity check the detection shape of pretrained model and ONNX model on the image for export process.\r\n    0\r\n    arr:     (100, 4)\r\n    ort_arr: (100, 4)\r\n    1\r\n    arr:     (100,)\r\n    ort_arr: (100,)\r\n    2\r\n    arr:     (100,)\r\n    ort_arr: (100,)\r\n    3\r\n    arr:     (100, 1, 426, 640)\r\n    ort_arr: (100, 1, 426, 640)\r\n\r\n    Sanity check the detection shape of pretrained model and ONNX model on the sanity check images.\r\n    0\r\n    sanity_arr:     (1, 4)\r\n    ort_sanity_arr: (4, 4)\r\n    1\r\n    sanity_arr:     (1,)\r\n    ort_sanity_arr: (4,)\r\n    2\r\n    sanity_arr:     (1,)\r\n    ort_sanity_arr: (4,)\r\n    3\r\n    sanity_arr:     (1, 1, 640, 586)\r\n    ort_sanity_arr: (4, 1, 640, 586)\r\n\r\n    Finish the sanity check process.\r\n\r\nFrom this result, we can see the detection results by ONNX model and PyTorch model in the sanity check image are different.\r\n\r\nThe MaskRCNN ONNX export log as follows:\r\n\r\n       %319 : Tensor = onnx::Shape(%318)\r\n      %320 : Tensor = onnx::Constant[value={2}]()\r\n      %321 : Long() = onnx::Gather[axis=0](%319, %320) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %322 : Float() = onnx::Cast[to=1](%321) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %323 : Float() = onnx::Constant[value={2.13333}]()\r\n      %324 : Float() = onnx::Mul(%322, %323)\r\n      %325 : Float() = onnx::Cast[to=1](%324) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %326 : Float() = onnx::Floor(%325) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %327 : Tensor = onnx::Shape(%318)\r\n      %328 : Tensor = onnx::Constant[value={3}]()\r\n      %329 : Long() = onnx::Gather[axis=0](%327, %328) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %330 : Float() = onnx::Cast[to=1](%329) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %331 : Float() = onnx::Constant[value={2.13333}]()\r\n      %332 : Float() = onnx::Mul(%330, %331)\r\n      %333 : Float() = onnx::Cast[to=1](%332) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n      %334 : Float() = onnx::Floor(%333) # /scratch2/Chong_dxxz_Projects/Gitlab/MaskRCNN_ONNX/TorchVision/env/lib/python3.6/site-packages/torch/nn/functional.py:2538:0\r\n\r\nThe related function in torch/nn/functional.py:\r\n\r\n    def _output_size(dim):\r\n        _check_size_scale_factor(dim)\r\n        if size is not None:\r\n            return size\r\n        scale_factors = _ntuple(dim)(scale_factor)\r\n        # math.floor might return float in py2.7\r\n\r\n        # make scale_factor a tensor in tracing so constant doesn't get baked in\r\n        if torch._C._get_tracing_state():\r\n            return [(torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i],\r\n                     dtype=torch.float32)).float())) for i in range(dim)]\r\n        else:\r\n            return [int(math.floor(float(input.size(i + 2)) * scale_factors[i])) for i in range(dim)]\r\n\r\nfunctional.py:2538 is: \r\n\r\n            return [(torch.floor((input.size(i + 2).float() * torch.tensor(scale_factors[i],\r\n                     dtype=torch.float32)).float())) for i in range(dim)]\r\n\r\nFrom the log, we can see the scale_factors[i] be exported as the Constant, not the expected tensor.\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0.dev20200216\r\n - OS (e.g., Linux): Ubuntu 16.04.6 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip install https://download.pytorch.org/whl/nightly/cu101/torch-1.5.0.dev20200216-cp36-cp36m-linux_x86_64.whl\r\n - Python version: 3.6\r\n - CUDA runtime version: 10.1.163\r\n - cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.0\r\n - GPU models and configuration: GPU 0: Tesla V100-SXM2-16GB\r\n - Any other relevant information:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.5.0.dev20200216\r\n[pip3] torchvision==0.5.0a0+a4303c8\r\n## Additional context\r\n\r\n\r\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["jit", "module: onnx", "triaged"], "number_of_comments": 5, "created_at": "2020-02-18 13:06:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566879968": {"author_username": "ZyUestc", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33442, "title": "Runtime Error in batch normalization when using multi processing", "body": "## \ud83d\udc1b Bug\r\n\r\nAssuming there is  a model  f using batch normalization and a  loss function having  a recurrent item like `l1_loss(f(f(x)), y),` it will raise:\r\n`RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: ` in multi processing case.\r\n\r\n## To Reproduce\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\n# +\r\nimport torch\r\n\r\nimport torch.utils.data as data\r\n\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nimport torch.distributed as dist\r\nimport torch.multiprocessing as mp\r\n\r\nimport torch.nn.functional as F\r\nfrom torch.nn.parameter import Parameter\r\n\r\n\r\n# -\r\n\r\nclass G1(nn.Module):\r\n    def __init__(self):\r\n        super(G1, self).__init__()\r\n        \r\n        self.conv1 = nn.Conv1d(2, 2, 1)\r\n        self.bn1 = nn.BatchNorm1d(2)\r\n    \r\n    def forward(self, x):\r\n        return self.bn1(self.conv1(x))\r\n\r\n\r\nclass G2(nn.Module):\r\n    def __init__(self):\r\n        super(G2, self).__init__()\r\n        \r\n        self.conv1 = nn.Conv1d(2, 2, 1)\r\n    \r\n    def forward(self, x):\r\n        return self.conv1(x)\r\n\r\n\r\n# +\r\ndef worker(gpuId, X):\r\n\r\n    dist.init_process_group(backend='nccl', init_method='tcp://127.0.0.1:6666', world_size=4, rank=gpuId)\r\n\r\n    \r\n    g = G1().cuda(gpuId) # Runtime Error!\r\n    # g = G2().cuda(gpuId)\r\n    g = nn.parallel.DistributedDataParallel(g, device_ids=[gpuId])\r\n    \r\n    d = g\r\n    \r\n    optimG = optim.SGD(g.parameters(), lr=1)\r\n    optimD = optim.SGD(d.parameters(), lr=1)\r\n    \r\n    sampler = data.distributed.DistributedSampler(X)\r\n    dataloader = data.DataLoader(X, sampler=sampler)\r\n    \r\n    for x, y in dataloader:\r\n        x = x.cuda(gpuId)\r\n        y = y.cuda(gpuId)\r\n        \r\n        pred = g(x)\r\n        seperateLoss = F.mse_loss(pred, y) \r\n        predPureLoss = F.l1_loss(d(pred), pred)\r\n        lossG = predPureLoss + seperateLoss\r\n        lossG.backward()\r\n        optimizerG.step()\r\n        \r\n        \r\ndef test():\r\n    X = data.TensorDataset(torch.rand(4, 2, 2), torch.rand(4, 2, 2))\r\n    mp.spawn(worker, nprocs=4, args=(X,)) \r\n    \r\n    \r\nif __name__ == '__main__':\r\n    test()\r\n\r\n```\r\nYou can get the script and run it with:\r\n```\r\nwget https://github.com/ZyUestc/Notes/blob/master/multi_process/gan.py\r\npython gan.py\r\n```\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar", "labels": ["module: distributed", "triaged"], "number_of_comments": 0, "created_at": "2020-02-18 13:05:31", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566838956": {"author_username": "HekpoMaH", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33439, "title": "Sparse tensor persistency - error when loading", "body": "## \ud83d\udc1b Bug \ud83d\udc1b\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen loading a saved sparse tensor an error occurs: `RuntimeError: size is inconsistent with indices: for dim 0, size is 3 but found index 140115406052656`\r\nClearly, that is impossible (can't even fit such a large tensor on my memory)\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install PyTorch and torch-sparse\r\n1. Run the below code (which produces the above error for me)\r\n\r\n```\r\nimport torch\r\nx=torch.zeros(3,3)\r\nx[1][1]=1\r\nx=x.to_sparse()\r\ntorch.save(x, './scratch/test4e')\r\ny = torch.load('./scratch/test4e')\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nLoads `y` normally ...\r\n\r\n## Environment\r\n```\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1070\r\nNvidia driver version: 440.59\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] torch                     1.4.0                     <pip>\r\n[conda] torch-cluster             1.4.5                     <pip>\r\n[conda] torch-geometric           1.4.1                     <pip>\r\n[conda] torch-scatter             2.0.3                     <pip>\r\n[conda] torch-sparse              0.5.1                     <pip>\r\n```\r\n\n\ncc @ezyang @gchanan @zou3519 @vincentqb", "labels": ["high priority", "module: serialization", "module: sparse", "triage review", "triaged"], "number_of_comments": 1, "created_at": "2020-02-18 11:47:42", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566747789": {"author_username": "Godricly", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33437, "title": "GPU mem leak when export with Dict output [JIT]", "body": "## \ud83d\udc1b Bug\r\nThe Dict output export has mem leak if called repeatly. The list one is stable.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. run the script.\r\n1. OOM is expected after 100 iters for dict.\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\n\r\nclass Demo1(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 20, 3)\r\n        self.conv2 = nn.Conv2d(3, 20, 3)\r\n\r\n    def forward(self, x):\r\n        return [self.conv1(x), self.conv2(x)]\r\n\r\n\r\nclass Demo2(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 20, 3)\r\n        self.conv2 = nn.Conv2d(3, 20, 3)\r\n\r\n    def forward(self, x):\r\n        return {'out1': self.conv1(x), 'out2': self.conv2(x)}\r\n\r\n\r\ndef test_demo(model):\r\n    inputs = torch.zeros(2, 3, 512, 512)\r\n    if torch.cuda.is_available():\r\n        model = model.cuda()\r\n        inputs = inputs.cuda()\r\n\r\n    for i in range(100):\r\n        model.train()\r\n        torch.onnx.export(model, inputs,\r\n                          'debug.onnx',\r\n                          input_names=['data'],\r\n                          opset_version=11,\r\n                          verbose=True,\r\n                          dynamic_axes={'data': {0: 'batch', 2: 'width', 3: 'height'}})\r\n        torch.cuda.empty_cache()\r\n        print(torch.cuda.memory_summary(device=None, abbreviated=False))\r\n\r\nif __name__ == '__main__':\r\n    model1 = Demo1()\r\n    model2 = Demo2()\r\n    print('demo1')\r\n    test_demo(model1)\r\n    print('demo2')\r\n    test_demo(model2)\r\n```\r\n## Expected behavior\r\nincreasing memory\r\n```\r\n|===========================================================================|\r\n|                  PyTorch CUDA memory summary, device ID 0                 |\r\n|---------------------------------------------------------------------------|\r\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\r\n|===========================================================================|\r\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\r\n|---------------------------------------------------------------------------|\r\n| Allocated memory      |    6806 MB |    6813 MB |   16473 MB |    9667 MB |\r\n|       from large pool |    6806 MB |    6813 MB |   16472 MB |    9666 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Active memory         |    6806 MB |    6813 MB |   16473 MB |    9667 MB |\r\n|       from large pool |    6806 MB |    6813 MB |   16472 MB |    9666 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved memory   |    6822 MB |    6822 MB |   14822 MB |    8000 MB |\r\n|       from large pool |    6820 MB |    6820 MB |   14820 MB |    8000 MB |\r\n|       from small pool |       2 MB |       2 MB |       2 MB |       0 MB |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable memory |   16372 KB |   16378 KB |    1691 MB |    1675 MB |\r\n|       from large pool |   14336 KB |   14336 KB |    1688 MB |    1674 MB |\r\n|       from small pool |    2036 KB |    2045 KB |       3 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Allocations           |     179    |     185    |    1675    |    1496    |\r\n|       from large pool |     171    |     173    |     927    |     756    |\r\n|       from small pool |       8    |      12    |     748    |     740    |\r\n|---------------------------------------------------------------------------|\r\n| Active allocs         |     179    |     185    |    1675    |    1496    |\r\n|       from large pool |     171    |     173    |     927    |     756    |\r\n|       from small pool |       8    |      12    |     748    |     740    |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved segments |     172    |     172    |     372    |     200    |\r\n|       from large pool |     171    |     171    |     371    |     200    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable allocs |       2    |       2    |       3    |       1    |\r\n|       from large pool |       1    |       1    |       2    |       1    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|===========================================================================|\r\n\r\n|===========================================================================|\r\n|                  PyTorch CUDA memory summary, device ID 0                 |\r\n|---------------------------------------------------------------------------|\r\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\r\n|===========================================================================|\r\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\r\n|---------------------------------------------------------------------------|\r\n| Allocated memory      |    6886 MB |    6893 MB |   16562 MB |    9676 MB |\r\n|       from large pool |    6886 MB |    6893 MB |   16561 MB |    9675 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Active memory         |    6886 MB |    6893 MB |   16562 MB |    9676 MB |\r\n|       from large pool |    6886 MB |    6893 MB |   16561 MB |    9675 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved memory   |    6902 MB |    6902 MB |   14902 MB |    8000 MB |\r\n|       from large pool |    6900 MB |    6900 MB |   14900 MB |    8000 MB |\r\n|       from small pool |       2 MB |       2 MB |       2 MB |       0 MB |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable memory |   16372 KB |   16378 KB |    1700 MB |    1684 MB |\r\n|       from large pool |   14336 KB |   14336 KB |    1697 MB |    1683 MB |\r\n|       from small pool |    2036 KB |    2045 KB |       3 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Allocations           |     181    |     187    |    1684    |    1503    |\r\n|       from large pool |     173    |     175    |     932    |     759    |\r\n|       from small pool |       8    |      12    |     752    |     744    |\r\n|---------------------------------------------------------------------------|\r\n| Active allocs         |     181    |     187    |    1684    |    1503    |\r\n|       from large pool |     173    |     175    |     932    |     759    |\r\n|       from small pool |       8    |      12    |     752    |     744    |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved segments |     174    |     174    |     374    |     200    |\r\n|       from large pool |     173    |     173    |     373    |     200    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable allocs |       2    |       2    |       3    |       1    |\r\n|       from large pool |       1    |       1    |       2    |       1    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|===========================================================================|\r\n\r\n|===========================================================================|\r\n|                  PyTorch CUDA memory summary, device ID 0                 |\r\n|---------------------------------------------------------------------------|\r\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\r\n|===========================================================================|\r\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\r\n|---------------------------------------------------------------------------|\r\n| Allocated memory      |    6966 MB |    6973 MB |   16651 MB |    9685 MB |\r\n|       from large pool |    6966 MB |    6973 MB |   16650 MB |    9684 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Active memory         |    6966 MB |    6973 MB |   16651 MB |    9685 MB |\r\n|       from large pool |    6966 MB |    6973 MB |   16650 MB |    9684 MB |\r\n|       from small pool |       0 MB |       0 MB |       1 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved memory   |    6982 MB |    6982 MB |   14982 MB |    8000 MB |\r\n|       from large pool |    6980 MB |    6980 MB |   14980 MB |    8000 MB |\r\n|       from small pool |       2 MB |       2 MB |       2 MB |       0 MB |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable memory |   16372 KB |   16378 KB |    1709 MB |    1693 MB |\r\n|       from large pool |   14336 KB |   14336 KB |    1706 MB |    1692 MB |\r\n|       from small pool |    2036 KB |    2045 KB |       3 MB |       1 MB |\r\n|---------------------------------------------------------------------------|\r\n| Allocations           |     183    |     189    |    1693    |    1510    |\r\n|       from large pool |     175    |     177    |     937    |     762    |\r\n|       from small pool |       8    |      12    |     756    |     748    |\r\n|---------------------------------------------------------------------------|\r\n| Active allocs         |     183    |     189    |    1693    |    1510    |\r\n|       from large pool |     175    |     177    |     937    |     762    |\r\n|       from small pool |       8    |      12    |     756    |     748    |\r\n|---------------------------------------------------------------------------|\r\n| GPU reserved segments |     176    |     176    |     376    |     200    |\r\n|       from large pool |     175    |     175    |     375    |     200    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|---------------------------------------------------------------------------|\r\n| Non-releasable allocs |       2    |       2    |       3    |       1    |\r\n|       from large pool |       1    |       1    |       2    |       1    |\r\n|       from small pool |       1    |       1    |       1    |       0    |\r\n|===========================================================================|\r\n\r\n```\r\n## Environment\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["jit", "module: onnx"], "number_of_comments": 2, "created_at": "2020-02-18 09:10:46", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566722404": {"author_username": "songyuc", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33436, "title": "It seems nn.Sequential.add_module() could take duplicate names", "body": "Hi, guys,\r\nI am reproducing DeepLabV3+ model these days,\r\nand I find that nn.Sequential.add_module() could take duplicate names, as\r\n```python\r\n            # some initialization code\r\n            depthwise.add_module(\"act\", act)\r\n            pointwise.add_module(\"act\", act)\r\n```\r\nIs that normal?\r\n\r\nAny idea or suggestion will be appreciated!\r\n\r\n", "labels": ["module: nn", "triaged"], "number_of_comments": 1, "created_at": "2020-02-18 08:20:19", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566678578": {"author_username": "carter54", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33433, "title": "question about GPT 2 quantization", "body": "## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\r\nI follow the example here (https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html) to make GPT 2 model quantization. code is as followed:\r\n```python\r\nfrom transformers import GPT2Tokenizer, GPT2Model\r\nimport torch\r\n\r\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\nmodel = GPT2Model.from_pretrained('gpt2')\r\nmodel.to('cpu')\r\n\r\nquantized_model = torch.quantization.quantize_dynamic(\r\n    model, dtype=torch.float16\r\n)\r\nprint(quantized_model)\r\n\r\ndef print_size_of_model(model):\r\n    torch.save(model.state_dict(), \"temp.p\")\r\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\r\n    os.remove('temp.p')\r\n\r\nprint_size_of_model(model)\r\nprint_size_of_model(quantized_model)\r\n```\r\n\r\nhowever, nothing is changed in the quantized_model, as I getting the same model size (548.119351MB) with the original model.\r\n\r\n```\r\nGPT2Model(\r\n  (wte): Embedding(50257, 768)\r\n  (wpe): Embedding(1024, 768)\r\n  (drop): Dropout(p=0.1, inplace=False)\r\n  (h): ModuleList(\r\n    (0): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (1): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (2): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (3): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (4): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (5): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (6): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (7): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (8): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (9): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (10): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n    (11): Block(\r\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (attn): Attention(\r\n        (c_attn): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (attn_dropout): Dropout(p=0.1, inplace=False)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n      (mlp): MLP(\r\n        (c_fc): Conv1D()\r\n        (c_proj): Conv1D()\r\n        (dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\n  )\r\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\r\n)\r\nSize (MB): 548.119351\r\nSize (MB): 548.119351\r\n```\r\ndid I miss anything?\r\nany help would be appriciated, thank you!\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a", "labels": ["quantization"], "number_of_comments": 1, "created_at": "2020-02-18 06:35:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566661849": {"author_username": "vishwakftw", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33432, "title": "Check for consistent devices in at::where", "body": "Changelog:\r\n- Add a check to ensure that all inputs to `where` lie on the same device\r\n\r\nTest Plan:\r\n- Added test_where_invalid_device\r\n\r\nFixes #33422 \r\n", "labels": ["module: operators", "open source", "triaged"], "number_of_comments": 4, "created_at": "2020-02-18 05:45:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566587789": {"author_username": "cheparukhin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33431, "title": "[Caffe2] Fix shape inference for element-wise operators", "body": "Summary:\nSome elementwise operators don't have shape and type inference specified for the output tensor: `BitwiseOr`, `BitwiseAnd`, `BitwiseXor`, `Not`, `Sign`.\n\nThis change fixes this issue:\n- For `Not` and `Sign` operators, the output has the same type and shape as the input, so `IdenticalTypeAndShapeOfInput` function is used to specify that.\n- For bitwise operators created by `CAFFE2_SCHEMA_FOR_BINARY_BITWISE_OP` macro, the type and shape inference rules should be the same as for other binary element-wise operators, so `TensorInferenceFunction(ElementwiseOpShapeInference)` is used to specify that.\n\nAlso some tests were modified to ensure that the shape and type are inferred (`ensure_outputs_are_inferred` parameter)\n\nTest Plan:\n```\nCAFFE2_ASSERT_SHAPEINFERENCE=1 buck test caffe2/caffe2/python/operator_test:elementwise_ops_test\nCAFFE2_ASSERT_SHAPEINFERENCE=1 buck test caffe2/caffe2/python/operator_test:math_ops_test\n```\n\nNote that the tests have to be executed with `CAFFE2_ASSERT_SHAPEINFERENCE=1` in order to fail upon shape inference failure.\n\nDifferential Revision: D19880164\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-18 01:15:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566543774": {"author_username": "kurtamohler", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33430, "title": "Bmm sparse dense", "body": "Add sparse-dense BMM operation for CUDA and CPU.\r\n\r\nIssue #5672 ", "labels": ["module: sparse", "open source", "triaged"], "number_of_comments": 3, "created_at": "2020-02-17 22:29:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566543721": {"author_username": "xuhdev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33429, "title": "Fix all occurrences of C416.", "body": "C416: Unnecessary (list/set) comprehension - rewrite using list/set().\r\n\r\nSee https://pypi.org/project/flake8-comprehensions/\r\n\r\n", "labels": ["module: lint", "open source"], "number_of_comments": 1, "created_at": "2020-02-17 22:29:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566540632": {"author_username": "xwang233", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33428, "title": "Fix deprecated python \"add\" calls ", "body": "This PR fixed those python \"add\" calls using deprecated signature `add(Scalar, Tensor)`. The alternative signature `add(Tensor, alpha = Scalar)` is used. \r\n\r\ncc @csarofeen @zasdfgbnm @ptrblck @ngimel ", "labels": ["module: optimizer", "open source"], "number_of_comments": 7, "created_at": "2020-02-17 22:18:51", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566531313": {"author_username": "pritamdamania87", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33427, "title": "Avoid clone for sparse tensors during accumulation of grads.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33427 Avoid clone for sparse tensors during accumulation of grads.**\n* #33214 Unify gradient accumulation between distributed autograd and local autograd\nengine.\nengine.\nengine.\nengine.\nengine.\n\nThis PR is an attempt to avoid clone for sparse tensors similar to how\nwe avoid clone for dense tensors currently.\n\nAs per my understanding even if the 'indices' and 'values' of a sparse tensor\nare non-continguous, operations like 'add' are still supported. As a result,\nthe major change in this PR is to use create a shallow copy instead of clone()\nfor sparse tensors.\n\nDifferential Revision: [D19926698](https://our.internmc.facebook.com/intern/diff/D19926698/)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-17 21:48:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566470813": {"author_username": "xuhdev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33424, "title": "Turn ONNX_ML into a proper build option.", "body": "The detection of the env variable ONNX_ML has been properly handled in tools/setup_helpers/cmake.py,\r\nline 242.\r\n\r\n", "labels": ["module: build", "module: onnx", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 18:59:00", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566448398": {"author_username": "mfkasim91", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33423, "title": "Batched torch.eig() and use of magma for CUDA torch.eig() operation", "body": "This is a new pull request because I messed up my previous branch.\r\nThis pull request is to add the capability of `torch.eig` in handling batched tensor.", "labels": ["module: operators", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 17:59:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566318310": {"author_username": "ggoossen", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33419, "title": "[pytorch] blas gemm fix for k=0", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33419 [pytorch] blas gemm fix for k=0**\n\nThese conditions are for the specific implementation, the fallback implementation works without these checks. So use that if any of these checks isn't true.\n\nDifferential Revision: [D19941103](https://our.internmc.facebook.com/intern/diff/D19941103/)", "labels": [], "number_of_comments": 3, "created_at": "2020-02-17 14:00:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566318264": {"author_username": "ahmadsalim", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33418, "title": "Feature/vonmises upstream", "body": "Third try of #33177 \ud83d\ude04 ", "labels": ["module: distributions", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 14:00:36", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566081795": {"author_username": "shuki-hpcnt", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33413, "title": "IterableDataset with num_workers > 0 and drop_last=True drops more instances than expected", "body": "## \ud83d\udc1b Bug\r\nWhen using `IterableDataset` with `num_workers > 0` and `drop_last=True`, DataLoader drops more instances than expected.\r\nIt seems like the drop of incomplete batches are occurred at each worker.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nfrom torch.utils.data import IterableDataset\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.data import get_worker_info\r\n\r\nclass MyIterableDataset(IterableDataset):\r\n    def __iter__(self):\r\n        worker_info = get_worker_info()\r\n        worker_id = worker_info.id\r\n        yield from range(100*worker_id, 100*(worker_id+1))\r\n\r\ndataset = MyIterableDataset()\r\ndata_loader = DataLoader(dataset, batch_size=128, num_workers=10, drop_last=True)\r\nnext(iter(data_loader))\r\n\r\n>>>\r\n(Expect this to return some batch, but this just raises StopIteration)\r\n```\r\n\r\n## Expected behavior\r\nDrops only the incomplete batch of complete iteration, not per each worker.\r\n\r\n## Environment\r\nPyTorch version: 1.3.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 440.44\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.3.1\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py36he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py36ha843d7b_0\r\n[conda] mkl_random                1.1.0            py36hd6b4f25_0\r\n[conda] pytorch                   1.3.1           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.4.2                py36_cu101    pytorch\r\n\n\ncc @SsnL", "labels": ["feature", "module: dataloader", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 06:21:31", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566076858": {"author_username": "Godricly", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33412, "title": "argsort not found when ONNX exporting [JIT]", "body": "# \ud83d\udc1b Bug\r\nThe `Tensor.argsort()` is not supported in onnx export. While the `Tensor.sort()` is supported by topK.\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\n\r\n1. run the scripts\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nimport numpy as np\r\n\r\nclass Demo(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, x):\r\n        v, inds = x.sort(descending=True)\r\n        # inds = x.argsort(descending=True)\r\n        return inds\r\n\r\nif __name__ == \"__main__\":\r\n    input_tensor = torch.range(20, 80)\r\n    demo = Demo()\r\n    out = demo(input_tensor)\r\n    torch.onnx.export(demo, input_tensor, \"debug.onnx\", verbose=True,\r\n                        input_names=['data'],\r\n                        opset_version=11,\r\n                        do_constant_folding=True,\r\n                        dynamic_axes={'data':{0:'batch'}})\r\n```\r\n## Expected behavior\r\n\r\n  File \"/home/god/python36/lib/python3.6/site-packages/torch/onnx/symbolic_registry.py\", line 91, in get_registered_op\r\n    return _registry[(domain, version)][opname]\r\nKeyError: 'argsort'\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\nCMake version: version 3.15.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\nexport result of `Tensor.sort()`\r\n```bash\r\n  input_tensor = torch.range(20, 80)\r\ngraph(%data : Float(61)):\r\n  %1 : Float(61), %2 : Long(61) = onnx::TopK[axis=-1, k=61](%data) # onnx.py:10:0\r\n  return (%2)\r\n```\r\n\n\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["jit", "module: onnx"], "number_of_comments": 0, "created_at": "2020-02-17 06:06:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566067953": {"author_username": "MrGrayCode", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33411, "title": "fix typo", "body": "", "labels": ["caffe2", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 05:36:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566039231": {"author_username": "buoyancy99", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33408, "title": "Support cdf for mixture_same_family distribution", "body": "The new added mixture_same_family should support cdf if the family has cdf implemented.\r\n\r\nThis is very useful for flow models where cdf of mixture of gassian/logistic is used to model flow\r\n", "labels": ["module: distributions", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-17 03:48:21", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "566008943": {"author_username": "as754770178", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33407, "title": "Dataloader make reading slower", "body": "# dataset: image_dataset.py\r\n```\r\n# -*-coding:utf-8-*-\r\n\r\nimport os\r\nimport cv2\r\nimport time\r\n\r\n\r\nclass ImageDataset(object):\r\n  def __init__(self, data_path=\"/home/zgz/DataSet/coco/2017/train2017\"):\r\n    self.data_path = data_path\r\n\r\n    self.image_list = []\r\n    for root, dirs, files in os.walk(data_path):\r\n      for file in files:\r\n        if file.endswith(\"jpg\"):\r\n          image_file = os.path.join(root, file)\r\n          self.image_list.append(image_file)\r\n    self.image_totoal_num = len(self.image_list)\r\n\r\n    self.image_num = 0\r\n\r\n  def __len__(self):\r\n    return self.image_totoal_num\r\n\r\n  def __getitem__(self, index):\r\n    # print(\"coco dataset : {}\".format(self.image_num))\r\n    # start_time = time.time()\r\n    self.image_num = self.image_num + 1\r\n    image = cv2.imread(self.image_list[index])\r\n    aug_image = cv2.resize(image, (468, 468))\r\n    # print(\"{} cost time : {}\".format(self.image_num, time.time() - start_time))\r\n    return aug_image\r\n\r\nimage_loader = ImageDataset()\r\n```\r\n\r\n# data loader test script\uff1a\r\n```\r\n# dataloader\r\nimport os, time\r\nimport torch\r\nimport random\r\n\r\nprint(torch.__version__)\r\nfrom torch.multiprocessing import Process, Manager, Lock\r\n\r\nprint(\"mutli test\")\r\nfrom image_dataset import image_loader\r\n\r\n\r\ndef image_reader(sign, share_ls, lock):\r\n  print(sign, 'pid:', os.getpid())\r\n  max_len = len(image_loader)\r\n\r\n  from torch.utils.data import DataLoader\r\n  pytorch_loader = DataLoader(dataset=image_loader,\r\n                              batch_size=3,\r\n                              shuffle=False,\r\n                              sampler=None,\r\n                              batch_sampler=None,\r\n                              num_workers=0,\r\n                              collate_fn=None,\r\n                              pin_memory=False,\r\n                              drop_last=False,\r\n                              timeout=0,\r\n                              worker_init_fn=None,\r\n                              multiprocessing_context=None)\r\n\r\n  for _ in range(10000):\r\n    index = random.randint(0, max_len)\r\n    aug_image = image_loader[index]\r\n\r\n  # for _ in pytorch_loader:\r\n    with lock:\r\n      share_ls[0] = share_ls[0] + 1\r\n      # print(\"{} : {}\".format(os.getpid(), share_ls[0]))\r\n      if share_ls[0] % 1000 == 0:\r\n        cur_time = time.time()\r\n        print(\"reader {} , total time: {}, cur time: {}\".format(share_ls[0],\r\n                                                                cur_time - share_ls[1],\r\n                                                                cur_time - share_ls[2]))\r\n        share_ls[2] = cur_time\r\n\r\n\r\n# Main\r\ndef run():\r\n  print('Main:', os.getpid())\r\n\r\n  m = Manager()\r\n  share_ls = m.list()\r\n  total_reader_num = 0\r\n  total_start_time = time.time()\r\n  cur_start_time = time.time()\r\n  share_ls.append(total_reader_num)\r\n  share_ls.append(total_start_time)\r\n  share_ls.append(cur_start_time)\r\n\r\n  plist = []\r\n  lock = Lock()\r\n  for j in range(5):\r\n    p = Process(target=image_reader, args=('process', share_ls, lock))\r\n    p.start()\r\n    plist.append(p)\r\n\r\n  for p in plist:\r\n    p.join()\r\n  print(\"test =================================\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  run()\r\n```\r\nWhen I comment \r\n```\r\nfor _ in range(10000):\r\n    index = random.randint(0, max_len)\r\n    aug_image = image_loader[index]\r\n```\r\nto use `for _ in pytorch_loader`, the program become slow.\n\ncc @SsnL", "labels": ["module: dataloader", "triaged"], "number_of_comments": 4, "created_at": "2020-02-17 01:28:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565961334": {"author_username": "peterbell10", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33405, "title": "Add 64-bit indexing support to THC index reductions", "body": "Fixes #32863, (together with #33310 for the `TensorIterator` reductions)\r\n\r\nThis adds 64-bit indexed kernels for `THC_reduceDimIndex` and uses `THCTensor_canUse32BitIndexMath` to switch between the two at runtime. \r\n\r\nI have a test for this locally but haven't included it here because `max` is much slower than `argmax`. To the point where the test takes several minutes to call max on just one `2**32` element tensor. That seems excessive, even for a slow test but I can push it if preferred.", "labels": ["open source"], "number_of_comments": 4, "created_at": "2020-02-16 20:07:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565931112": {"author_username": "peterjc123", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33404, "title": "Revert \"Disable flaky test TestCppExtensionAOT.test_cuda_extension in\u2026", "body": "\u2026 Windows CI (#33282)\"\r\n\r\nThis reverts commit 5b922918d023126ad1f468c68577c9b599ad202d.\r\n\r\nFixes #33270.", "labels": ["merge-this-please", "open source"], "number_of_comments": 6, "created_at": "2020-02-16 16:27:37", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565926282": {"author_username": "jspark1105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33402, "title": "D19919020", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-16 15:52:25", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565900920": {"author_username": "jeremycochoy", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33400, "title": "Add CELU operator's export", "body": "## Issue\r\nThe CELU operator can't be exported (see https://github.com/pytorch/pytorch/issues/33399).\r\n\r\n## Content\r\nThis PR add a simple implementation of the export based on the actual Pytorch implementation of CELU operator.\r\n\r\nHow the export graph looks using `CELU(alpha=2)` as exported model.\r\n\r\n![image](https://user-images.githubusercontent.com/4193259/74605013-83064c80-50cc-11ea-8c1d-458c75a96f31.png)\r\n\r\nYou can test the export with:\r\n```\r\nimport torch.onnx\r\nimport torch.nn\r\n\r\ntorch.onnx.export(torch.nn.CELU(alpha=2), torch.rand(1), 'celu_export.onnx')\r\n```\r\n\r\n---\r\n\r\nPS: Do you have tests for onnx export of operators like LeakyRelu/Relu/Celu?\r\n\r\nNb: In the ONNX 12 Opset, there will be a CELU operator. But for now, we can still provide an export that will be able to run on ONNX 9 backends (pretty much any existing backend) with few lines.", "labels": ["module: onnx", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-16 12:56:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565900778": {"author_username": "jeremycochoy", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33399, "title": "ONNX export failed on ATen operator celu because torch.onnx.symbolic_opset9.celu does not exist", "body": "## \ud83d\udc1b Bug\r\n\r\nModel containing a `torch.nn.CELU` node can't be exported using `torch.onnx.export`.\r\n\r\n## To Reproduce\r\n\r\nRun the following command in a 1python3.71 interpreter:\r\n```\r\nimport torch.onnx; import torch.nn; torch.onnx.export(torch.nn.CELU(alpha=2), torch.rand(1), 'tst.onnx')\r\n```\r\n\r\nThe error message is:\r\n```\r\nPython 3.7.4 (default, Sep  7 2019, 18:27:02) \r\n[Clang 10.0.1 (clang-1001.0.46.4)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import torch.onnx; import torch.nn; torch.onnx.export(torch.nn.CELU(alpha=2), torch.rand(1), 'tst.onnx')\r\n/Users/jeremycochoy/symphonia/pytorch/torch/onnx/utils.py:715: UserWarning: ONNX export failed on ATen operator celu because torch.onnx.symbolic_opset9.celu does not exist\r\n  .format(op_name, opset_version, op_name))\r\n```\r\n\r\n## Expected behavior\r\n\r\nWe would like to get a model containing the `CELU` node (or equivalent implementation) exported as a `onnx` model.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.3\r\nGCC version: Could not collect\r\nCMake version: version 3.15.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.4\r\n[pip3] pytorch-lightning==0.5.3.2\r\n[pip3] torch==1.4.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.6.0a0+0156d58\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nThe CELU node is not yet part of the standard ONNX. Luckily, It has been implemented in Pytorch using the Elu operator which IS part of the ONNX standard :)\r\n\n\ncc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["jit", "module: onnx"], "number_of_comments": 0, "created_at": "2020-02-16 12:54:53", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565874537": {"author_username": "haowen-xu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33396, "title": "PyTorch 1.4.0 does not support using `Module` or `ModuleList` in attribute annotations in ScriptModule", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nThe following piece of code:\r\n\r\n```python\r\nimport torch\r\nfrom torch import Tensor, zeros\r\nfrom torch.nn import Module, Linear, ModuleList\r\nfrom torch.jit import script\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n\r\n    __constants__ = ('wrapped',)\r\n\r\n    wrapped: Module\r\n\r\n    def __init__(self, wrapped: Module):\r\n        super().__init__()\r\n        self.wrapped = wrapped\r\n\r\n    def forward(self, input: Tensor):\r\n        return self.wrapped(input)\r\n\r\n\r\nm = script(MyModule(Linear(3, 3)))\r\n```\r\n\r\nwould produce the following traceback:\r\n\r\n```\r\n/opt/local/miniconda3/envs/py3.6/bin/python /Users/ipwx/projects/tensorkit/debug.py\r\nTraceback (most recent call last):\r\n  File \"/Users/ipwx/projects/tensorkit/debug.py\", line 19, in <module>\r\n    m = script(MyModule(Linear(3, 3)))\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 1255, in script\r\n    return torch.jit._recursive.recursive_script(obj)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 293, in create_script_module\r\n    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 236, in get_or_create_concrete_type\r\n    concrete_type_builder = infer_concrete_type_builder(nn_module)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 115, in infer_concrete_type_builder\r\n    attr_type = infer_type(name, item)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 79, in infer_type\r\n    attr_type = torch.jit.annotations.ann_to_type(class_annotations[name])\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/annotations.py\", line 253, in ann_to_type\r\n    raise ValueError(\"Unknown type annotation: '{}'\".format(ann))\r\nValueError: Unknown type annotation: '<class 'torch.nn.modules.module.Module'>'\r\n```\r\n\r\nAlso, the following piece of code:\r\n\r\n```python\r\nfrom typing import *\r\n\r\nimport torch\r\nfrom torch import Tensor, zeros\r\nfrom torch.nn import Module, Linear, ModuleList\r\nfrom torch.jit import script\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n\r\n    __constants__ = ('wrapped',)\r\n\r\n    wrapped: ModuleList\r\n\r\n    def __init__(self, wrapped: List[Module]):\r\n        super().__init__()\r\n        self.wrapped = ModuleList(wrapped)\r\n\r\n    def forward(self, input: Tensor):\r\n        output = input\r\n        for m in self.wrapped:\r\n            output = m(output)\r\n        return output\r\n\r\n\r\nm = script(MyModule([Linear(3, 3), Linear(3, 3)]))\r\n```\r\n\r\nwould produce the following traceback:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/ipwx/projects/tensorkit/debug.py\", line 24, in <module>\r\n    m = script(MyModule([Linear(3, 3), Linear(3, 3)]))\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 1255, in script\r\n    return torch.jit._recursive.recursive_script(obj)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 534, in recursive_script\r\n    return create_script_module(nn_module, infer_methods_to_compile(nn_module))\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 293, in create_script_module\r\n    concrete_type = concrete_type_store.get_or_create_concrete_type(nn_module)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 236, in get_or_create_concrete_type\r\n    concrete_type_builder = infer_concrete_type_builder(nn_module)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 115, in infer_concrete_type_builder\r\n    attr_type = infer_type(name, item)\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/_recursive.py\", line 79, in infer_type\r\n    attr_type = torch.jit.annotations.ann_to_type(class_annotations[name])\r\n  File \"/opt/local/miniconda3/envs/py3.6/lib/python3.6/site-packages/torch/jit/annotations.py\", line 253, in ann_to_type\r\n    raise ValueError(\"Unknown type annotation: '{}'\".format(ann))\r\nValueError: Unknown type annotation: '<class 'torch.nn.modules.container.ModuleList'>'\r\n```\r\n\r\nAlso, I noticed that, if I removed the type annotation for `wrapped`, no error was thrown, but the code of `wrapped.forward` was not inlined, which was the behavior of PyTorch 1.3.1.  Formally, the following piece of code:\r\n\r\n```python\r\nimport torch\r\nfrom torch import Tensor, zeros\r\nfrom torch.nn import Module, Linear, ModuleList\r\nfrom torch.jit import script\r\n\r\n\r\nclass MyModule(torch.nn.Module):\r\n\r\n    __constants__ = ('wrapped',)\r\n\r\n    # wrapped: Module\r\n\r\n    def __init__(self, wrapped: Module):\r\n        super().__init__()\r\n        self.wrapped = wrapped\r\n\r\n    def forward(self, input: Tensor):\r\n        return self.wrapped(input)\r\n\r\n\r\nm = script(MyModule(Linear(3, 3)))\r\nprint(m.code)\r\n```\r\n\r\nwould generate the following output:\r\n\r\n```\r\ndef forward(self,\r\n    input: Tensor) -> Tensor:\r\n  return (self.wrapped).forward(input, )\r\n```\r\n\r\nbut the same code generates the following output with PyTorch 1.3.1:\r\n\r\n```\r\nimport __torch__\r\nimport __torch__.torch.nn.modules.linear\r\ndef forward(self,\r\n    input: Tensor) -> Tensor:\r\n  _0 = self.wrapped\r\n  _1 = _0.weight\r\n  _2 = _0.bias\r\n  if torch.eq(torch.dim(input), 2):\r\n    _3 = torch.__isnot__(_2, None)\r\n  else:\r\n    _3 = False\r\n  if _3:\r\n    bias = ops.prim.unchecked_unwrap_optional(_2)\r\n    ret = torch.addmm(bias, input, torch.t(_1), beta=1, alpha=1)\r\n  else:\r\n    output = torch.matmul(input, torch.t(_1))\r\n    if torch.__isnot__(_2, None):\r\n      bias0 = ops.prim.unchecked_unwrap_optional(_2)\r\n      output0 = torch.add_(output, bias0, alpha=1)\r\n    else:\r\n      output0 = output\r\n    ret = output0\r\n  return ret\r\n```\r\n\r\n## Expected behavior\r\n\r\nI suppose PyTorch should support `Module` and `ModuleList` in its annotations, just as 1.3.1.  Also, the code of wrapped is expected to be inlined, as a performance optimization.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Mac OSX 10.15.3\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): none\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: none\r\n - GPU models and configuration: none\r\n - Any other relevant information: none\r\n\n\ncc @suo", "labels": ["jit", "triaged"], "number_of_comments": 1, "created_at": "2020-02-16 09:12:42", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565854880": {"author_username": "v0dro", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33394, "title": "Non-deterministic scatter reduction algorithms for scatter operations for CUDA (sum, subtract, divide, multiply). ", "body": "## \ud83d\ude80 Feature\r\n\r\nCUDA equivalent of https://github.com/pytorch/pytorch/issues/33389\r\n\r\nNew issue for the sake of clarity and brevity.", "labels": ["triaged"], "number_of_comments": 0, "created_at": "2020-02-16 05:30:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565845064": {"author_username": "ayush-1506", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33392, "title": "Add error message when using as_strided with negative strides", "body": "Fixes #33290 \r\n\r\nWould love a review. @SsnL ", "labels": ["open source", "topic: error checking", "triaged"], "number_of_comments": 1, "created_at": "2020-02-16 03:33:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565840794": {"author_username": "songyuc", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33390, "title": "It is not good to separate the steps of modules making and forward computation", "body": "## \ud83d\ude80 Feature\r\nHi, dear developers, I think,\r\n<!-- A clear and concise description of the feature proposal -->\r\nIt is not good to separate the steps of modules making and forward computation,\r\nwhich should be combined like a circuit diagram\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nBecause separating  the steps of modules making and forward computation will make code highly redundant, as the module initiation and forward computation are a procedure essentially the same.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n", "labels": ["feature", "triaged"], "number_of_comments": 1, "created_at": "2020-02-16 02:40:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565838711": {"author_username": "v0dro", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33389, "title": "Non-deterministic scatter reduction algorithms for scatter operations for CPU (sum, subtract, divide, multiply).", "body": "## \ud83d\ude80 Feature\r\n\r\nImplementing non-deterministic parallel scatter algorithms for various reduction operations. This issue is a follow-up of https://github.com/pytorch/pytorch/issues/33065 and a prelude to https://github.com/pytorch/pytorch/issues/22378.\r\n\r\n## Motivation\r\n\r\nRead comment https://github.com/pytorch/pytorch/issues/22378#issuecomment-586081656 for more details.\r\n\r\n## Pitch\r\n\r\nA scatter reduction API for addition, subtraction, multiplication and division that will have the following form:\r\n``` python\r\ninput = torch.zeros(4, 4, device=device)\r\nsrc = torch.ones(2, 2, device=device)\r\nindex = torch.tensor([[1], [2]], device=device, dtype=torch.long)\r\ninput.scatter_(0, index, src, reduce=\"sum\")\r\n```\r\n", "labels": ["enhancement", "module: operators", "triage review", "triaged"], "number_of_comments": 5, "created_at": "2020-02-16 02:15:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565833822": {"author_username": "luckytoilet", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33388, "title": "Sobol engine generates out-of-bounds samples after drawing too many samples", "body": "## \ud83d\udc1b Bug\r\n\r\nThe SobolEngine should always produce points between [0, 1]. However, after drawing about a billion samples, it produces points far outside this range.\r\n\r\n## To Reproduce\r\n\r\n```\r\nimport torch\r\n\r\ni = 1\r\nsobol = torch.quasirandom.SobolEngine(dimension=2)\r\nwhile True:\r\n  X = sobol.draw(1000000)\r\n  print('iteration: %d, min=%0.9f, max=%0.9f, mean=%0.9f'% (i, X.min(), X.max(), X.mean()))\r\n  i += 1\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe first few iterations are correct (all values between 0 and 1):\r\n\r\n```\r\niteration: 1, min=0.000000954, max=0.999999046, mean=0.499999553\r\niteration: 2, min=0.000000477, max=0.999999523, mean=0.499999940\r\niteration: 3, min=0.000000238, max=0.999999762, mean=0.500000298\r\n```\r\n\r\nAt iteration 1074, the bug appears:\r\n\r\n```\r\niteration: 1072, min=0.000000286, max=0.999998808, mean=0.500000775\r\niteration: 1073, min=0.000000955, max=0.999999821, mean=0.499999911\r\niteration: 1074, min=0.000000001, max=131019.000000000, mean=16913.619140625\r\niteration: 1075, min=0.000000955, max=131019.000000000, mean=65512.328125000\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.7.5\r\n - CUDA/cuDNN version: not relevant\r\n - GPU models and configuration: not relevant\r\n", "labels": ["module: nn", "module: random", "triaged"], "number_of_comments": 1, "created_at": "2020-02-16 01:19:41", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565817540": {"author_username": "pbelevich", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33386, "title": "[DO NOT MERGE] Testing", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33386 Migrate random_ from the TH to Aten (CPU)**\n\nDifferential Revision: [D19925976](https://our.internmc.facebook.com/intern/diff/D19925976)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-15 22:21:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565808018": {"author_username": "ultimomaximus", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33385, "title": "Illegal instruction: 4 - OSX 10.13.6 install from source", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. git clone --recursive https://github.com/pytorch/pytorch\r\n... \r\n2. export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\r\n3. MACOSX_DEPLOYMENT_TARGET=10.13.6 CC=clang CXX=clang++ python setup.py install\r\n\r\nIllegal instruction: 4\r\n\r\n## Expected behavior\r\n\r\nThis installation was already done some days before and it worked, but without cudnn. \r\nPytorch was removed with pip and cudnn files were added.\r\n\r\nThe same install command, as before and the message \"**_Illegal instruction: 4_**\" was displayed. Hardware is not changed.\r\n\r\ncuddn files removed, install retried - same message.\r\n\r\n## Environment\r\n\r\nfrom collect_env:\r\n\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: 1.1.0\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda/lib/libcudnn.7.dylib\r\n/usr/local/cuda/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n\r\n[pip3] numpy==1.18.1\r\n[pip3] numpydoc==0.9.2\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      233  \r\n[conda] mkl-include               2020.0                      166  \r\n[conda] mkl-service               2.3.0            py36hfbe908c_0  \r\n[conda] mkl_fft                   1.0.15           py36h5e564d8_0  \r\n[conda] mkl_random                1.1.0            py36ha771720_0\r\n\r\n\r\n--\r\nfrom conda info:\r\n \r\nconda version : 4.8.2\r\nconda-build version : 3.18.11\r\npython version : 3.6.10.final.0\r\nvirtual packages : __cuda=10.1\r\n\r\n--\r\n\r\nkextstat | grep -i cuda\r\n  147    0 0xffffff7f8197c000 0x2000     0x2000     com.nvidia.CUDA (1.1.0) 5CF5D7EC-2985-32A1-9150-1BE18678BFC6 <4 1>\r\n\r\ncudnn: 7.6.3.30\r\n\r\n--\r\n\r\nHardware: \r\n\r\nMacPro 5.1 \r\nNVIDIA GeForce GTX 960\r\n", "labels": ["module: build", "module: osx", "triaged"], "number_of_comments": 0, "created_at": "2020-02-15 20:53:32", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565802444": {"author_username": "josh-gleason", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33384, "title": "Max-pooling uses implicit negative infinity padding, not zero-padding as indicated in documentation", "body": "## \ud83d\udc1b Bug\r\n\r\nAccording to the documentation on [`nn.MaxPool1d`](https://pytorch.org/docs/stable/nn.html#torch.nn.MaxPool1d):\r\n\r\n> If `padding` is non-zero, then **the input is implicitly zero-padded on both sides** for `padding` number of points. `dilation` controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what `dilation` does.\r\n\r\nHowever, the input is never padded with zeros, either explicitly or implicitly. Doing a bit of source diving I found that the maximum operation [initializes with negative infinity](https://github.com/pytorch/pytorch/blob/d35a4c202e917b5d5cb67ac749fcf7931212c25b/aten/src/ATen/native/DilatedMaxPool2d.cpp#L59), not zero, and only considers entries from the original, unpadded input tensor. Therefore it would be correct to say that the max-pooling operation uses implicit negative infinity padding but not zero-padding.\r\n\r\nThis appears to be either a bug in the API or documentation (of course PEBCAK is always a possibility).\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Install PyTorch\r\n2. Run the following code\r\n```\r\n>>> import torch\r\n>>> from torch.nn.functional import max_pool1d\r\n>>> x = torch.tensor([[[-1., -2.]]])\r\n>>> max_pool1d(x, kernel_size=2, padding=1)\r\ntensor([[[-1., -2.]]])\r\n>>> max_pool1d(x.cuda(), kernel_size=2, padding=1)\r\ntensor([[[-1., -2.]]], device='cuda:0')\r\n```\r\n\r\n## Expected behavior\r\n\r\nIf zero-padding was being used we would expect the output to be `tensor([[[0., 0.]]])` since zero is larger than all the input tensor elements.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Red Hat Enterprise Linux Workstation release 7.7 (Maipo)\r\nGCC version: (GCC) 6.3.0\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 430.40\r\ncuDNN version: 7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n\r\nThis issue is based on a [question originally asked on StackOverflow](https://stackoverflow.com/questions/60240434) by user trsvchn.", "labels": ["triage review", "triaged"], "number_of_comments": 1, "created_at": "2020-02-15 20:08:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565801583": {"author_username": "shz0116", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33383, "title": "[TEST ONLY] test master branch", "body": "Summary: As title said. 02/15/2020 12PM\n\nTest Plan: buck test\n\nDifferential Revision: D19925687\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-15 20:02:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565789682": {"author_username": "andrewdelong", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33381, "title": "torch.lu(A) traces but doesn't script", "body": "## \ud83d\udc1b Bug\r\n\r\n`torch.lu` can be traced, but script compilation raises `Unknown builtin op: aten::lu` error.\r\n\r\n## To Reproduce\r\n\r\n```python\r\ndef f(A, b):\r\n    return torch.lu_solve(b, *torch.lu(A))\r\n\r\nA = torch.eye(3)\r\nb = torch.ones((3, 1))\r\n\r\ntorch.jit.trace(f, (A, b))  # <-- OK\r\ntorch.jit.script(f)         # <-- Unknown builtin op: aten::lu\r\n```\r\nUsing `torch.solve` or `torch.cholsky`+`torch.cholesky_solve` works fine.\r\n\r\n## Workaround\r\n\r\nTrace `torch.lu` and then call it from the script:\r\n```python\r\nlu_trace = torch.jit.trace(torch.lu, torch.eye(2))\r\n\r\n@torch.jit.script\r\ndef f(A, b):\r\n    return torch.lu_solve(b, *lu_trace(A))  # <-- OK\r\n```\r\n\r\n## Environment\r\n\r\n - PyTorch 1.4.0\r\n - MacOS\r\n - conda install\r\n - Python 3.7.6\r\n - CPU only\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-15 18:23:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565785068": {"author_username": "zasdfgbnm", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33380, "title": "Kill cudaDeviceAllocator in THCState", "body": "", "labels": ["module: cuda", "open source", "triaged"], "number_of_comments": 2, "created_at": "2020-02-15 17:49:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565780007": {"author_username": "jamesr66a", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33379, "title": "[WIP][quantization] Use torchbind for Linear PackedParams", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33379 [WIP][quantization] Use torchbind for Linear PackedParams**\n\nDifferential Revision: [D19701952](https://our.internmc.facebook.com/intern/diff/D19701952/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19701952/)!", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-15 17:09:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565725980": {"author_username": "zasdfgbnm", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33374, "title": "Kill torch.range", "body": "It is deprecated and should be removed on 0.5\r\n```C++\r\n  if (r.idx == 0) {\t\r\n    PyErr_WarnEx(PyExc_UserWarning, \"torch.range is deprecated in favor of torch.arange \"\t\r\n        \"and will be removed in 0.5. Note that arange generates values in [start; end), \"\t\r\n        \"not [start; end].\", 1);\r\n```", "labels": ["module: operators", "open source", "triaged"], "number_of_comments": 2, "created_at": "2020-02-15 09:58:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565716655": {"author_username": "chengmengli06", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33373, "title": "torch.clamp_ not inplace during backward", "body": "I use torch.clamp_ to avoid forward overflow for fp16 calculation. \r\nIt does not consume extra memory during forward pass. \r\nHowever, it has the **same gpu memory cost** as torch.clamp during backward pass. \r\nIn comparison, torch.relu_ does not take extra gpu memory during both forward and backward pass.\r\n\r\ntorch version: 1.3.0\r\ntorch.version.cuda: 10.1.243\r\n\r\n```\r\n_conv3d = nn.Conv3d\r\nclass Conv3dWrapper(_conv3d):\r\n  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,\r\n               groups=1, bias=True, padding_mode='zeros'):\r\n    super(Conv3dWrapper, self).__init__(in_channels, out_channels, kernel_size, stride,\r\n                             padding, dilation, groups, bias, padding_mode)\r\n\r\n  def forward(self, input):\r\n    return torch.clamp_(super(Conv3dWrapper, self).forward(input), -65504, 65504)\r\n\r\n_conv_trans3d = nn.ConvTranspose3d\r\nclass ConvTranspose3dWrapper(_conv_trans3d):\r\n  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\r\n            output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros'):\r\n    super(ConvTranspose3dWrapper, self).__init__(in_channels, out_channels, kernel_size,\r\n            stride, padding, output_padding, groups, bias, dilation, padding_mode)\r\n\r\n  def forward(self, input):\r\n    return torch.clamp_(super(ConvTranspose3dWrapper, self).forward(input), -65504, 65504)\r\n\r\n```\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen", "labels": ["module: autograd", "module: operators", "topic: memory usage", "triaged"], "number_of_comments": 6, "created_at": "2020-02-15 08:25:37", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565633725": {"author_username": "wanchaol", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33369, "title": "[jit] infer RRef type as container type", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33263 [jit] allow RRef local creation with IValue objects\n* #33526 [jit] make RRef type annotation available in Python\n* **#33369 [jit] infer RRef type as container type**\n\nThis PR add RRef type infer rule when we try to infer a type from a\npyobject, this allow script module attributes could contain a rref,\n(i.e. List[RRefs] as a module attribute)\n\nDifferential Revision: [D19918320](https://our.internmc.facebook.com/intern/diff/D19918320)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-15 01:05:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565626196": {"author_username": "v0dro", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33367, "title": "Expose `internal::GRAIN_SIZE` through Python API.", "body": "## \ud83d\ude80 Feature\r\n\r\nCurrently the `internal::GRAIN_SIZE` is visible only via the C++ API. Its hard to know how many threads will be spawned by a given method for purposes of debugging or performance analysis purely through the Python API.\r\n\r\nHaving a way to do this using something like `torch.GRAIN_SIZE` should serve as a simple way of doing this.\r\n\r\n## Motivation\r\n\r\nI'm working on https://github.com/pytorch/pytorch/issues/33065 and find that I can't easily get the grain size from the python API and need to put a `cout` in the C++ code and then recompile it.\r\n\r\n## Pitch\r\n\r\n`torch.GRAIN_SIZE` should return the `internal::GRAIN_SIZE` number.\r\n\r\n", "labels": ["enhancement", "topic: multithreading", "triage review", "triaged"], "number_of_comments": 2, "created_at": "2020-02-15 00:23:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565620886": {"author_username": "mcarilli", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33366, "title": "[WIP] Reanimate gradient scaling API with original scale update heuristic", "body": "Windows memory failures still need to be debugged.\r\n\r\nThis PR (initially) contains 2 commits:\r\n* a revert of the revert\r\n* all changes to implement the original Apex scale update heuristic, squashed into a single commit for easier diff review", "labels": ["open source"], "number_of_comments": 1, "created_at": "2020-02-15 00:11:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565613633": {"author_username": "jspark1105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33364, "title": "D19696471", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-14 23:54:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565604835": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33363, "title": "Make it easier to add new messages in RPC layer", "body": "## \ud83d\ude80 Feature\r\nAs more features are implemented in the RPC layer there will likely be a need to add new message types and custom logic for responding to those messages.\r\n\r\nCurrently, there is a lot of scaffolding that needs to be written which is sort of duplicated across message types, for example we need to:\r\n\r\n1) define the message type \r\n2) define whether it is a request or response\r\n3) define its serdes methods\r\n4) define how we should respond to the message\r\n\r\nAll of these hooks need to be added in separate files and it's quite easy to miss certain hooks and spend extra time debugging. It would be good to consolidate, such as by implementing some of the ideas below:\r\n\r\n1) Have all RPC commands define an `execute()` method, and call this in `request_callback_impl.cpp`. This will save us the giant switch statement there (there is a TODO for this in the code currently)\r\n2) Encapsulate the message type and whether it is a request/response into a single point of truth,\r\n3) Provide a `registerMessage()` or similar interface where the user can just define their message type, any custom logic needed to convert from message --> command, and how to respond to the message. The framework will take care of instantiating the concrete objects via lambda functions that the developer can pass in.\r\n\r\n## Motivation\r\n\r\nIt takes quite a bit of time to add new messages/commands currently.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["better-engineering", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-14 23:26:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565590315": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33362, "title": "Standardize expanded TensorOptions representation in native_functions.yaml", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33362 Standardize expanded TensorOptions representation in native_functions.yaml**\n* #33347 Update documentation on why _cudnn_init_dropout_state looks the way it is.\n* #33305 Remove special case codegen for tril_indices/triu_indices.\n\nHere is the line of thinking that went into the patch:\n\n- In native_parse.py. we have three different ways you can write the\n  expanded form of TensorOptions arguments.  This is dumb, there should\n  be exactly one form of TensorOptions that is valid, everything\n  else should be disallowed.  So I fiat that all TensorOptions arguments\n  must have the form\n\n      *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool pin_memory=False\n\n  and I fix all entries in native_functions.yaml to abide by this specific\n  format.\n\n  There was a little bit of hemming and hawing about whether or not\n  it should be bool pin_memory=False or bool? pin_memory=None.  In the\n  end, I believe making pin_memory a nullable parameter is the epitome\n  of \"foolish consistency\", and we should abide by how things are\n  documented in Python (which matches the above form).\n\n- By doing this, I have transformed some previously mandatory overloads\n  of TensorOptions into optional overloads of TensorOptions, in the\n  C++ API.  For all _like() factory functions, this makes the TensorOptions\n  function ambiguous with the MemoryFormat overload, which is also optional.\n  These overloads should never have existed in the first place, so I hack around\n  this by just making the parameter on the MemoryFormat overload mandatory, so that I pick the TensorOptions\n  overload when no arguments are provided.  There's one minor but obvious fix\n  for Python argument parsing when I do this.  TorchScript exported model BC can\n  <redacted> <redacted> <redacted>.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D19975409](https://our.internmc.facebook.com/intern/diff/D19975409)", "labels": [], "number_of_comments": 3, "created_at": "2020-02-14 22:37:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565588422": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33361, "title": "torch.tensor can infer complex dtype now", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33507 Gradcheck for complex\n* **#33361 torch.tensor can infer complex dtype now**\n* #33268 Added tensor.is_complex(), is_complex and dtype.is_complex py binding, tensor printing, and dixed the scalar type returned for complex float\n\nDifferential Revision: [D19943477](https://our.internmc.facebook.com/intern/diff/D19943477)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-14 22:31:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565577873": {"author_username": "hantek", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33360, "title": "Dropout of attention weights in function F.multi_head_attention_forward() breaks sum-to-1 constraint", "body": "## \ud83d\udc1b Bug\r\n\r\nDropout by calling the built-in dropout function includes rescaling the un-dropped elements, which results in the dropped attention weight vectors possibly sum to a larger than 1 value. \r\n\r\n \r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nSee line 3374 to 3376 in file `torch/nn/functional.py`\r\n\r\n```python\r\n        attn_output_weights = softmax(\r\n            attn_output_weights, dim=-1)\r\n        attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)\r\n\r\n        attn_output = torch.bmm(attn_output_weights, v)\r\n```\r\n\r\nStep through in PDB:\r\n```\r\n(Pdb) attn_output_weights.sum(2)\r\ntensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\r\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\r\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\r\n        ...,\r\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\r\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\r\n        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\r\n       device='cuda:0', grad_fn=<SumBackward1>)\r\n(Pdb) n\r\n> /home/hantek/.conda/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py(3378)multi_head_attention_forward()\r\n-> attn_output = torch.bmm(attn_output_weights, v)\r\n(Pdb) attn_output_weights.sum(2)\r\ntensor([[1.0307, 1.0081, 1.0560,  ..., 1.0524, 1.0065, 0.9454],\r\n        [1.0181, 1.0151, 1.0170,  ..., 1.0090, 0.9986, 0.9864],\r\n        [0.9757, 0.9827, 0.9963,  ..., 1.0014, 0.9632, 0.9294],\r\n        ...,\r\n        [1.0049, 0.9946, 0.9487,  ..., 1.0083, 1.0059, 0.9952],\r\n        [0.9971, 1.0024, 1.0114,  ..., 0.9894, 1.0492, 1.0207],\r\n        [1.0050, 1.0084, 1.0159,  ..., 0.9867, 0.9799, 1.0328]],\r\n       device='cuda:0', grad_fn=<SumBackward1>)\r\n```\r\nWhat I expect to happen is that the weights still sum-to-1, or sum to a smaller than 1 value after attention dropout.\r\n\r\n## Environment\r\npython 3.7, pytorch 1.4.0\r\n\r\n## Additional context", "labels": ["module: nn", "triaged"], "number_of_comments": 2, "created_at": "2020-02-14 22:04:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565560178": {"author_username": "kevinbchen", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33359, "title": "[pytorch] Set alias analysis kind to FROM_SCHEMA for qadd, qmul, qclamp, qconcat", "body": "Summary:\nUpdated alias analysis kind to FROM_SCHEMA so input tensors can be marked as nonmutable\nwhen appropriate, allowing for constant folding of these tensors.\n\nNeeded to update the schemas of the _out variants with annotations to mark the output input\ntensor as aliased and mutable.\n\nTest Plan:\n```\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n\n    def forward(self, x):\n        w = torch.tensor([3], dtype=torch.float)\n        w = torch.quantize_per_tensor(w, 1.0, 0, torch.qint8)\n        y = torch.tensor([3], dtype=torch.float)\n        y = torch.quantize_per_tensor(w, 1.0, 0, torch.qint8)\n        return torch.ops.quantized.add_out(x, w, y)\n\nm = torch.jit.script(M())\ntorch._C._jit_pass_constant_propagation(m.graph)\nprint(m.graph)\n```\n```\ngraph(%self : __torch__.___torch_mangle_9.M,\n      %x.1 : Tensor):\n  %11 : int = prim::Constant[value=12]() # <ipython-input-11-1dd94c30cb58>:9:49\n  %9 : float = prim::Constant[value=1.]() # <ipython-input-11-1dd94c30cb58>:9:41\n  %10 : int = prim::Constant[value=0]() # <ipython-input-11-1dd94c30cb58>:9:46\n  %36 : QInt8(1) = prim::Constant[value={3}]()\n  %y.2 : Tensor = aten::quantize_per_tensor(%36, %9, %10, %11) # <ipython-input-11-1dd94c30cb58>:11:12\n  %24 : Tensor = quantized::add_out(%x.1, %36, %y.2) # <ipython-input-11-1dd94c30cb58>:12:15\n  return (%24)\n```\nAs expected, the aten::quantize_per_tensor() for w is now folded. The aten::quantize_per_tensor()\nfor y is not folded, since that tensor is aliased/modified.\n\nDifferential Revision: D19910667\n\n", "labels": ["fb-exported"], "number_of_comments": 4, "created_at": "2020-02-14 21:17:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565532877": {"author_username": "vincentqb", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33356, "title": "convert counter back to list #33229", "body": "Fixes #33229", "labels": [], "number_of_comments": 2, "created_at": "2020-02-14 20:09:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565519356": {"author_username": "nairbv", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33354, "title": "Long torchscript warmup times can be problematic for production serving", "body": "Torchscript models take some time to compile on their first query. Initial warmup can be triggered via shadow traffic, but in a large scale elastic cluster of nodes serving many torchscript models, this potentially makes orchestration or user experience more difficult.\r\n\r\nIn one particular case (densenet 161) we saw warmup times of over 1 minute.\r\n\r\nPotential improvements:\r\n* Off-thread compilation (run initial queries unoptimized while compiling in another thread) or have a flag to run uncompiled?\r\n* Improve compile times for particular outliers like densenet161\r\n\r\ncc @suo , @fbbradheintz \r\n\r\nRelated to:\r\nhttps://github.com/pytorch/pytorch/issues/27610\r\n\r\nExample of slow initial query attached, with output:\r\n```\r\n$ python test_torchscript.py\r\nloading model\r\nmodel loaded\r\nwarmup\r\n['n02123045', 'tabby']\r\nfirst query took 109.52137017250061\r\nstarting\r\n['n02123045', 'tabby']\r\n['n02123045', 'tabby']\r\n['n02123045', 'tabby']\r\n['n02123045', 'tabby']\r\n['n02123045', 'tabby']\r\n--- 1.9634242057800293 seconds ---\r\n```\r\n\r\n\r\n", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-14 19:37:23", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565517326": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33353, "title": "[jit] `Node` callstack is incorrect", "body": "After applying #33352,\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport pprint\r\n\r\nclass MyNet(nn.Module):\r\n    def __init__(self):\r\n        super(MyNet, self).__init__()\r\n        self.conv = nn.Conv2d(1, 6, 5, 1, 2)\r\n        self.maxpool = nn.MaxPool2d(2, 2)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.maxpool(x)\r\n        return x\r\n\r\nmodel = MyNet()\r\ntorch._C._jit_set_inline_everything_mode(True)\r\nscript = torch.jit.script(model)\r\n\r\nfor node in script.graph.nodes():\r\n    if node.kind() == \"aten::conv2d\":\r\n        pprint.pprint(node.callstack())\r\n```\r\n\r\noutputs\r\n\r\n```\r\n['__torch__.torch.nn.modules.pooling.MaxPool2d.forward',                                                                     \r\n '__torch__.torch.nn.modules.conv.Conv2d.forward',                                                                           \r\n '__torch__.torch.nn.modules.conv.Conv2d._conv_forward'] \r\n```\r\n\r\nwhich includes `MaxPool2d.forward` but probably shouldn't\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-14 19:32:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565517094": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33352, "title": "[jit][wip] Expose Node.callstack to Python", "body": "", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-14 19:32:00", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565514214": {"author_username": "ailzhang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33351, "title": "Behavior of view on out= functions", "body": "When I fix test failures on XLA backend, I found this example (not in our test) giving surprising results. \r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: a = torch.rand(4, 4)\r\n\r\nIn [3]: base = torch.zeros_like(a)\r\n\r\nIn [4]: first_row = a[0]\r\n\r\nIn [5]: torch.pow(base, 2, out=first_row)\r\nOut[5]:\r\ntensor([[0., 0., 0., 0.],\r\n        [0., 0., 0., 0.],\r\n        [0., 0., 0., 0.],\r\n        [0., 0., 0., 0.]])\r\n\r\nIn [6]: a\r\nOut[6]:\r\ntensor([[0., 0., 0., 0.],\r\n        [0., 0., 0., 0.],\r\n        [0., 0., 0., 0.],\r\n        [0., 0., 0., 0.]])\r\n```\r\nSo as a user I only know `first_row` shares the first row with `a`. But when we update `first_row`, we actually update the whole tensor of `a`. \r\nI was expecting all rows of `a` except the first to remain the old values instead of 0. \r\n\r\nI kinda understand this is an outcome of `first_row` shares storage with `a` so when we do the update we just did it inplace. \r\nBut I want to argue that the current behavior might be bad:\r\n- Updating tensor `first_row` which is a tensor of size 4, affected all 16 elements in its base tensor. This doesn't look right and can cause unexpected changes. \r\n\r\nAlso want to add that the current behavior, if implemented on XLA backend (graph based view), requires view tensor to have full knowledge of all its base tensors, which element of storage it touched during chains of views, which can complicate the problem a lot. In fact I ran out of ideas how to match this behavior on XLA end, thus opening an issue here. :P \r\ncc: @ezyang @albanD @gchanan for thoughts. \n\ncc @ezyang @SsnL @gchanan @albanD @zou3519 @gqchen", "labels": ["module: operators", "topic: bc-breaking", "triaged"], "number_of_comments": 2, "created_at": "2020-02-14 19:25:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565503327": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33350, "title": "[DO NOT COMMIT] [Tensor Expressions] Squashed PR (for testing only)", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33350 [DO NOT COMMIT] [Tensor Expressions] Squashed PR (for testing only)**\n\n**Last commit:**\n\n-----------------------------------------------\n\nBroadcast based on input shapes (#178)\n\n-----------------------------------------------\n\n    ghstack-source-id: d3453513662ef5ed7afada28e201a0a88ea0dbae\n    ", "labels": ["jit"], "number_of_comments": 3, "created_at": "2020-02-14 19:05:05", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565461198": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33348, "title": "test_backward_python_udf_error is flaky", "body": "## \ud83d\udc1b Bug\r\n\r\nThis test is flaky in `dist_autograd_test`:\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4490663\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4477746\r\n\r\n(I initially thought it might be due to https://github.com/pytorch/pytorch/pull/32957, but there were failures before those changes as well)\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: distributed", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-14 17:29:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565408130": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33347, "title": "Update documentation on why _cudnn_init_dropout_state looks the way it is.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33362 Standardize expanded TensorOptions representation in native_functions.yaml\n* **#33347 Update documentation on why _cudnn_init_dropout_state looks the way it is.**\n* #33305 Remove special case codegen for tril_indices/triu_indices.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D19975410](https://our.internmc.facebook.com/intern/diff/D19975410)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-14 15:48:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565366117": {"author_username": "blizda", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33345, "title": "Issue with longe sequence", "body": "## \ud83d\udc1b Bug\r\n\r\nI try to train Reformer in Pytorch implementation from here\r\n\r\nhttps://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch\r\n\r\nusing this script\r\n\r\nhttps://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py\r\n\r\nonly change I made - add 16 bit model training with model.half()\r\n\r\nI set batch size to 16 and sequence lenght - 24960\r\n\r\nafter I tried to start training I get this error: \r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-11-9219ded2be62> in <module>\r\n     28         loss = loss_fn(output, labels)\r\n     29         optimizer.zero_grad()\r\n---> 30         loss.backward()\r\n     31         optimizer.step()\r\n     32         running_loss += loss.item()\r\n\r\n/opt/anaconda/envs/train_torch/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n    193                 products. Defaults to ``False``.\r\n    194         \"\"\"\r\n--> 195         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n    196 \r\n    197     def register_hook(self, hook):\r\n\r\n/opt/anaconda/envs/train_torch/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     97     Variable._execution_engine.run_backward(\r\n     98         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 99         allow_unreachable=True)  # allow_unreachable flag\r\n    100 \r\n    101 \r\n\r\nRuntimeError: index_put_ with accumulation is not supported on large tensors, number of source elements =12189265920file a support request on github\r\n\r\nif I switch batch size to 2, training is going fine\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: Quadro RTX 8000\r\nGPU 1: Quadro RTX 8000\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] reformer-pytorch==0.12.7\r\n[pip] revtorch==0.2.4\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.4.2\r\n[conda] _pytorch_select           0.1                       cpu_0  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] reformer-pytorch          0.12.7                   pypi_0    pypi\r\n[conda] revtorch                  0.2.4                    pypi_0    pypi\r\n[conda] torch                     1.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.4.2           cpu_py37h9ec355b_0\n\ncc @ngimel", "labels": ["module: cuda", "triaged"], "number_of_comments": 1, "created_at": "2020-02-14 14:38:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565362901": {"author_username": "Lezcano", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33344, "title": "Parametrization Functionality", "body": "Provides the implementation for feature request issue #28937.\r\n\r\nAdds the `Parametrization` functionality and implements `Pruning` on top of it.\r\nIt adds the `auto` mode, on which the parametrization is just computed once per forwards pass. The previous implementation computed the pruning on every forward, which is not optimal when pruning RNNs for example.\r\n\r\nIt implements a caching mechanism for parameters. This is implemented through the mechanism proposed at the end of the discussion #7313. In particular, it assumes that the user will not manually change the updated parameters between the call to `backwards()` and the `optimizer.step()`. If they do so, they would need to manually call the `.invalidate()` function provided in the implementation. This could be made into a function that gets a model and invalidates all the parameters in it. It might be the case that this function has to be called in the `.cuda()` and `.to` and related functions.\r\n\r\nAs described in #7313, this could be used, to implement in a cleaner way the `weight_norm` and `spectral_norm` functions. It also allows, as described in #28937, for the implementation of constrained optimization on manifolds (i.e. orthogonal constraints, positive definite matrices, invertible matrices, weights on the sphere or the hyperbolic space...)\r\n\r\nTODO (when implementation is validated):\r\n- More thorough test\r\n- Documentation\r\n\r\n@albanD \r\n", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-14 14:32:42", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565319985": {"author_username": "bjliuzp", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33343, "title": "How to convert the model to onnx in libtorch? ", "body": "struct Net : torch::nn::Module {\r\n\tNet()\r\n\t\t: conv1(torch::nn::Conv2dOptions(1, 20, /*kernel_size=*/5).stride(1)),\r\n\t\tconv2(torch::nn::Conv2dOptions(20, 40, /*kernel_size=*/5)),\r\n\t\tfc1(640, 120),\r\n\t\tfc2(120, 10) {\r\n\t\tregister_module(\"conv1\", conv1);\r\n\t\tregister_module(\"conv2\", conv2);\r\n\t\tregister_module(\"conv2_drop\", conv2_drop);\r\n\t\tregister_module(\"fc1\", fc1);\r\n\t\tregister_module(\"fc2\", fc2);\r\n\t}\r\n\ttorch::Tensor forward(torch::Tensor x) {\r\n\t\tx = torch::relu(torch::max_pool2d(conv1->forward(x), 2));//(28-5)+1=24,12 x 12 x 10\r\n\t\tx = torch::relu(torch::max_pool2d(conv2_drop->forward(conv2->forward(x)), 2));//(12-5)+1=8,4 x 4 x 20\r\n\t\t//x = torch::relu(torch::avg_pool2d(conv2_drop->forward(conv2->forward(x)), 2));//(12-5)+1=8,4 x 4 x 20\r\n\r\n\t\tx = x.view({ -1, 640 });\r\n\t\tx = torch::relu(fc1->forward(x));\r\n\t\tx = torch::dropout(x, /*p=*/0.5, /*training=*/is_training());\r\n\t\tx = fc2->forward(x);\r\n\t\treturn torch::log_softmax(x, /*dim=*/1);\r\n\t}\r\n\ttorch::nn::Conv2d conv1;\r\n\ttorch::nn::Conv2d conv2;\r\n\ttorch::nn::Dropout2d conv2_drop;\r\n\ttorch::nn::Linear fc1;\r\n\ttorch::nn::Linear fc2;\r\n};\n\ncc @yf225 @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: cpp", "module: onnx", "triaged"], "number_of_comments": 4, "created_at": "2020-02-14 13:14:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565291934": {"author_username": "PLoic", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33342, "title": " malloc(): memory corruption (fast)", "body": "## \ud83d\udc1b Bug\r\nI use the official Docker Images of Pytorch. When I try train my model using the DataParallel module, and during the training it's randomly crash, by segfault. Without DataParallel I get no error.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Start to train a  model, using data DataParallel on 2 GPUs\r\nExample of model (Sequential order) :\r\n - nn.Embedding()\r\n - nn.LSTM(bidirectionnal=True)\r\n - nn.Linear()\r\n \r\nLoss used :\r\n- BCEWithlogitsloss\r\n\r\nOptimizer :\r\n- Adam\r\n\r\n```\r\n*** Error in '/opt/conda/bin/python': malloc(): memory corruption (fast): 0x000055a9cabee518 ***\r\n======= Backtrace: =========\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f6094b627e5]\r\n/lib/x86_64-linux-gnu/libc.so.6(+0x82651)[0x7f6094b6d651]\r\n/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f6094b6f184]\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_Znwm+0x18)[0x7f60848a5e78]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so(+0x1af81)[0x7f603c2b3f81]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libc10_cuda.so(+0x1cd5e)[0x7f603c2b5d5e]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN2at6native10empty_cudaEN3c108ArrayRefIlEERKNS1_13Tenso\r\nrOptionsENS1_8optionalINS1_12MemoryFormatEEE+0x284)[0x7f6042151094]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x455b8d8)[0x7f6040a218d8]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x1eedc47)[0x7f603e3b3c47]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x3ead8a5)[0x7f60403738a5]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x1eedc47)[0x7f603e3b3c47]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x1c5e1c0)[0x7f603e1241c0]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN2at6native5zerosEN3c108ArrayRefIlEERKNS1_13TensorOptio\r\nnsE+0x25)[0x7f603e12dea5]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x1fa1a83)[0x7f603e467a83]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x3d912ce)[0x7f60402572ce]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x1fe5819)[0x7f603e4ab819]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x37c6c3b)[0x7f603fc8cc3b]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN5torch8autograd9generated13SliceBackward5applyEOSt6vec\r\ntorIN2at6TensorESaIS5_EE+0x111)[0x7f603fc8f6e1]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(+0x3fb3106)[0x7f6040479106]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN5torch8autograd6Engine17evaluate_functionERSt10shared_\r\nptrINS0_9GraphTaskEEPNS0_4NodeERNS0_11InputBufferE+0x1373)[0x7f6040475563]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN5torch8autograd6Engine11thread_mainERKSt10shared_ptrIN\r\nS0_9GraphTaskEEb+0x4b2)[0x7f6040476192]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch.so(_ZN5torch8autograd6Engine11thread_initEi+0x39)[0x7f604046\r\nfa89]\r\n/opt/conda/lib/python3.6/site-packages/torch/lib/libtorch_python.so(_ZN5torch8autograd6python12PythonEngine11thread_in\r\nitEi+0x2a)[0x7f60851f7dea]\r\n/opt/conda/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(+0xedef)[0x7f6085e21def]\r\n/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f6094ebc6ba]\r\n/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f6094bf241d]\r\n\r\n```\r\n## Expected behavior\r\n\r\nTrain normaly, and save the model\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0+cu100\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: Tesla V100-PCIE-32GB\r\nGPU 1: Tesla V100-PCIE-32GB\r\n\r\nNvidia driver version: 410.79\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.4\r\n[pip] numpydoc==0.9.2\r\n[pip] pytorch-pretrained-bert==0.6.2\r\n[pip] pytorch-transformers==1.1.0\r\n[pip] torch==1.4.0+cu100\r\n[pip] torchcontrib==0.0.2\r\n[pip] torchfile==0.1.0\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.5.0+cu100\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl_fft                   1.0.10           py36ha843d7b_0  \r\n[conda] mkl_random                1.0.2            py36hd81dba3_0  \r\n[conda] pytorch-pretrained-bert   0.6.2                    pypi_0    pypi\r\n[conda] pytorch-transformers      1.1.0                    pypi_0    pypi\r\n[conda] torch                     1.4.0+cu100              pypi_0    pypi\r\n[conda] torchcontrib              0.0.2                    pypi_0    pypi\r\n[conda] torchfile                 0.1.0                    pypi_0    pypi\r\n[conda] torchtext                 0.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.5.0+cu100              pypi_0    pypi\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "labels": ["module: data parallel", "triaged"], "number_of_comments": 0, "created_at": "2020-02-14 12:15:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565266859": {"author_username": "w1005444804", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33341, "title": "how-to-adjust-learning-rate using libtorch", "body": "## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n", "labels": ["triaged"], "number_of_comments": 0, "created_at": "2020-02-14 11:25:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565169758": {"author_username": "lly-zero-one", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33339, "title": "Add the 3d avg pool for video related model", "body": "", "labels": [], "number_of_comments": 3, "created_at": "2020-02-14 08:11:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565126077": {"author_username": "Krovatkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33338, "title": "[WIP] initial implementation of symbolic shapes", "body": "", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-14 06:08:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565124023": {"author_username": "Krovatkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33337, "title": "run peephole to do profile-based optimizations", "body": "We need to run a peephole before constant propagation in the profiling pipeline, so we fold `prim::shape` for inputs with complete tensor types.\r\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-14 06:01:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565113011": {"author_username": "hl475", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33334, "title": "suspicious performance regression found in #30806", "body": "There is a suspicious  performance regression found in https://github.com/pytorch/pytorch/pull/30806 with `cat` operator. Creating the new issue here since we do not want to revert the PR.\r\n\r\n\r\nTo reproduce, I am using [PT OP Benchmark](https://github.com/pytorch/pytorch/tree/master/benchmarks/operator_benchmark) with `--operators cat --tag_filter all`. environment variable `MKL_NUM_THREADS=1` and `OMP_NUM_THREADS=1`. We haven't set `KMP_AFFINITY`. For `numactl` we are using `taskset --cpu-list NUMBER` where `NUMBER` is a random int within the cpu-list range.\r\n\r\nHere is our benchmark machine info\r\n\r\n```\r\n$ lscpu\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              36\r\nOn-line CPU(s) list: 0-35\r\nThread(s) per core:  2\r\nCore(s) per socket:  18\r\nSocket(s):           1\r\nNUMA node(s):        1\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) D-2191A CPU @ 1.60GHz\r\nStepping:            4\r\nCPU MHz:             1600.021\r\nCPU max MHz:         1601.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            3200.00\r\nVirtualization:      VT-x\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            25344K\r\nNUMA node0 CPU(s):   0-35\r\n```\r\n\r\n\r\nFor outputs from our internal run, check below. Where `before` is the current PyTorch codebase, and `after` is revert the changes in this PR.\r\n \r\n```\r\n================================================================================\r\nBefore the change, Program Output:\r\n================================================================================\r\n# ----------------------------------------\r\n# PyTorch/Caffe2 Operator Micro-benchmarks\r\n# ----------------------------------------\r\n# Tag : all\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M1_N1_K1_dim0_cpu\r\n# Input: M: 1, N: 1, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 10.651\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M256_N512_K1_dim0_cpu\r\n# Input: M: 256, N: 512, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 110.463\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M512_N512_K2_dim1_cpu\r\n# Input: M: 512, N: 512, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 529.833\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim0_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 18.580\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim1_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 1, device: cpu\r\nForward Execution Time (us) : 22.946\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim2_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 2, device: cpu\r\nForward Execution Time (us) : 39.233\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim0_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 0, device: cpu\r\nForward Execution Time (us) : 25.150\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim1_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 30.775\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim2_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 2, device: cpu\r\nForward Execution Time (us) : 516.519\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim0_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 111.330\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim1_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 1, device: cpu\r\nForward Execution Time (us) : 114.270\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim2_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 2, device: cpu\r\nForward Execution Time (us) : 256.177\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim0_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 0, device: cpu\r\nForward Execution Time (us) : 267.849\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim1_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 269.662\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim2_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 2, device: cpu\r\nForward Execution Time (us) : 4038.911\r\n\r\n================================================================================\r\nAfter the change, Program Output:\r\n================================================================================\r\n# ----------------------------------------\r\n# PyTorch/Caffe2 Operator Micro-benchmarks\r\n# ----------------------------------------\r\n# Tag : all\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M1_N1_K1_dim0_cpu\r\n# Input: M: 1, N: 1, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 5.776\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M256_N512_K1_dim0_cpu\r\n# Input: M: 256, N: 512, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 75.062\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M512_N512_K2_dim1_cpu\r\n# Input: M: 512, N: 512, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 418.434\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim0_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 15.653\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim1_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 1, device: cpu\r\nForward Execution Time (us) : 23.845\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K1_dim2_cpu\r\n# Input: M: 128, N: 128, K: 1, dim: 2, device: cpu\r\nForward Execution Time (us) : 1432.811\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim0_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 0, device: cpu\r\nForward Execution Time (us) : 23.533\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim1_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 27.186\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N128_K2_dim2_cpu\r\n# Input: M: 128, N: 128, K: 2, dim: 2, device: cpu\r\nForward Execution Time (us) : 1440.207\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim0_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 0, device: cpu\r\nForward Execution Time (us) : 74.519\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim1_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 1, device: cpu\r\nForward Execution Time (us) : 108.705\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K1_dim2_cpu\r\n# Input: M: 128, N: 1024, K: 1, dim: 2, device: cpu\r\nForward Execution Time (us) : 11414.430\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim0_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 0, device: cpu\r\nForward Execution Time (us) : 196.254\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim1_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 1, device: cpu\r\nForward Execution Time (us) : 192.988\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_M128_N1024_K2_dim2_cpu\r\n# Input: M: 128, N: 1024, K: 2, dim: 2, device: cpu\r\nForward Execution Time (us) : 11472.040\r\n\r\n================================================================================\r\n```\r\n\r\n_Originally posted by @hl475 in https://github.com/pytorch/pytorch/pull/30806#issuecomment-585920259_\n\ncc @ezyang @gchanan @zou3519 @VitalyFedyunin @ngimel", "labels": ["high priority", "topic: performance", "triage review", "triaged"], "number_of_comments": 5, "created_at": "2020-02-14 05:24:21", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565057945": {"author_username": "mingfeima", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33330, "title": "port masked_fill from TH to ATen", "body": "port `masked_fill` from TH to ATen with TensorIterator.\r\n\r\nsingle core performance roughly stays the same, single socket performance has **3~16x** boost.\r\n\r\n`masked_fill` is missing from https://github.com/pytorch/pytorch/issues/24507", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 1, "created_at": "2020-02-14 02:04:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565056350": {"author_username": "xush6528", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33329, "title": "Register rpc.rpc_async(..) as a JIT operator", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33329 Register rpc.rpc_async(..) as a JIT operator**\n\r\n# Use case\r\n\r\n```\r\n@torch.jit.script\r\ndef send_rpc_async(dst_worker_name, user_func_qual_name, tensor):\r\n    # type: (str, str, Tensor) -> None\r\n    rpc._rpc_async_torchscript(\r\n        dst_worker_name, user_func_qual_name, args=(tensor,)\r\n    )\r\n```\r\n\r\n# Problem\r\n\r\n```\r\ntorch.jit.frontend.NotSupportedError: keyword-arg expansion is not supported:\r\n  File \"/data/users/shihaoxu/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/rpc_spawn#binary,link-tree/torch/distributed/rpc/api.py\", line 722\r\n    args = args if args else ()\r\n    kwargs = kwargs if kwargs else {}\r\n    fut = _invoke_rpc_torchscript(to, qualified_name, *args, **kwargs)\r\n                                                               ~~~~~~ <--- HERE\r\n    return fut\r\n```\r\n\r\n# Solution\r\n\r\nRegister `rpc.rpc_async(..)` as a JIT operator to handle variable-length argument list.\r\n\r\n# Plan\r\n\r\nThis PR is the required changes to make `rpc.rpc_async(..)` a JIT prim operator, which can dynamically handle different number of arguments.\r\n\r\n- Register \"prim::rpc_async\" as a `Symbol` in \"interned_string.h\"\r\n- Add a if branch in \"python_sugared_value.cpp\" `toSugarValue(py::object, ..)` entry utility function to set up how JIT frontend convert `torch.distributed.rpc.rpc_async(..)` Python function (Python object) into a `SpecialFormValue` (IR SugaredValue).\r\n- Add a switch case for \"prim::rpc_aynsc\" Symbol in \"ir_emitter.cpp\" and `emitApplySpecialForm(..)` to set up how JIT compiler provides inputs to the \"prim::rpc_aynsc\" Operator.\r\n- Register \"prim::rpc_async\" as a `jit::Operator` and provide implementation in \"register_distributed_ops.cpp\".\r\n\r\nNotice, since the distributed module is an optional part when building PyTorch. The code to be added in this PR should be wrapped within preprocessing maco.\r\n```\r\n#ifdef USE_DISTRIBUTED\r\nnew code here\r\n#endif\r\n```\r\n\r\nDifferential Revision: [D5738300](https://our.internmc.facebook.com/intern/diff/D5738300/)\r\n\r\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D5738300/)!", "labels": ["jit", "module: rpc"], "number_of_comments": 1, "created_at": "2020-02-14 01:58:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565048427": {"author_username": "zhehangd", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33327, "title": "Creating Torch tensors slows OpenCV video reading a lot", "body": "## \ud83d\udc1b Bug\r\n\r\nI am developing a system that samples frames from videos and runs them through a deep network.\r\nWhen I profiled the code I noticed that OpenCV read videos much slower than expected. Compared to a naive program that only grabs frames and does nothing else, this program spends x2 more time  grabbing frames. \r\n\r\nI managed to figure out a simple condition to trigger this slowing: to create a torch tensor of size >= 100000 whenever I sampled a frame. Something like \r\n```python\r\nmagic_tensor = torch.zeros(100000)\r\n```\r\nI can use any shape (e.g. 100000x1, 320x320), and even any data type (e.g. uint8, float64).\r\nThe turning point is exactly 100000, as 99999 won't trigger the slowing. Making a larger tensor or creating more tensors don't deteriorate the speed further.\r\n\r\nCreating such an array slows the all cv2.grab() function calls in a short period of time. If I call time.sleep(1) to sleep for 1 second after I create an array, subsequent video reading is not affected at all.\r\n\r\nWhen this experiment is performed in the main thread, video reading takes roughly 1.4x time of the baseline. However, when it is performed in a new thread (this is my use case), the time is about 2.4x of the baseline.\r\n\r\n## To Reproduce\r\nThe code below will perform three experiments to show the issue.\r\nThe `read_video_test(filename, use_magic_code)` function samples every 30th frame from a video.\r\nWhen `use_magic_code` is `True`, after a frame retrieval of the magic tensor is created, which will slows the grabbing. In reality this is the place I send the images to the deep network.\r\n\r\nThe three experiments are as follows:\r\n* Baseline. `read_video_test` is performed without creating torch tensor.\r\n* Reproduction in the main thread. This slows the video reading by ~50%.\r\n* Reproduction in a new thread. This slows  the video reading by ~150%.\r\n\r\nThe test video I use has size of 1920x1080 and is about 20s long. I can reproduce it on other videos too, including videos of different duration and of different resolution (e.g. 360p). Enabling or disabling CUDA seems has not influence on this issue.\r\n\r\n```python\r\nimport threading\r\nimport time\r\n\r\nimport cv2\r\nimport torch\r\n\r\ndef read_video_test(filename, use_magic_code):\r\n    cap = cv2.VideoCapture(filename)\r\n    assert cap.isOpened()\r\n    \r\n    frame_idx = 0\r\n    total_grab_time = 0\r\n    total_time = time.time()\r\n    while cap.isOpened():\r\n      # --------- Measure the grabbing time ---------\r\n      grab_time = time.time()\r\n      ret = cap.grab()\r\n      grab_time = time.time() - grab_time\r\n      total_grab_time += grab_time\r\n      # ---------------------------------\r\n      if not ret:\r\n        break\r\n      if not frame_idx % 30: # decode every 30th frame\r\n        ret, frame = cap.retrieve()\r\n        if use_magic_code:\r\n          # magic code:\r\n          # trigger condition: size >= 100000, regardless of data type\r\n          magic_tensor = torch.zeros(100000, dtype=torch.int8)\r\n      frame_idx += 1\r\n    total_time = time.time() - total_time\r\n    \r\n    print(\"total time: {:.4}, cap.grab() took {:.4}s\".format(\r\n      total_time, total_grab_time))\r\n\r\nif __name__ == '__main__':\r\n    \r\n    test_vid = \"data/VID_20190407_170747.mp4\"\r\n    \r\n    # 1.0x time  Baseline\r\n    print('Exp#1: baseline, torch code disabled')\r\n    read_video_test(test_vid, False)\r\n    print(\"------------------------------------\")\r\n    \r\n    # 1.4x time\r\n    print('Exp#2: main thread execution, torch code enabled')\r\n    read_video_test(test_vid, True)\r\n    print(\"------------------------------------\")\r\n    \r\n    # 2.4x time\r\n    print('Exp#3: new thread execution, torch code enabled')\r\n    t = threading.Thread(\r\n      target=read_video_test, args=(test_vid, True))\r\n    t.start()\r\n    t.join()\r\n    print(\"------------------------------------\")\r\n```\r\n## Result\r\n\r\n```\r\nExp#1: baseline, torch code disabled\r\ntotal time: 1.41, cap.grab() took 1.279s\r\n------------------------------------\r\nExp#2: main thread execution, torch code enabled\r\ntotal time: 1.963, cap.grab() took 1.841s\r\n------------------------------------\r\nExp#3: new thread execution, torch code enabled\r\ntotal time: 3.594, cap.grab() took 3.389s\r\n------------------------------------\r\n```\r\n\r\nIdeally there should not be any difference\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Tesla P100-PCIE-16GB\r\nNvidia driver version: 410.48\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n\r\nVersions of relevant libraries:\r\nopencv                    3.4.2            py37h6fd60c2_1\r\n[pip] numpy==1.17.2\r\n[pip] torch==1.2.0\r\n[pip] torchsummary==1.5.1\r\n[pip] torchvision==0.4.0a0\r\n[conda] _pytorch_select           0.2                       gpu_0  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.2.0           cuda100py37h938c94c_0  \r\n[conda] torchsummary              1.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.4.0           cuda100py37hecfc37a_0\r\n\r\nI am able to reproduce it on a google cloud server and my laptop.\r\nBoth have the same software configuration.\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @VitalyFedyunin @ngimel", "labels": ["topic: performance", "triage review", "triaged"], "number_of_comments": 4, "created_at": "2020-02-14 01:28:48", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "565013498": {"author_username": "krshrimali", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33322, "title": "[WIP] [Type Promotion] Allow Unary Ops dtype promotion to match NumPy", "body": "This issue changes `unary_op_impl` helper function to allow `int` to `float` dtype promotion for Unary Ops based on NumPy's conversion rules. \r\n\r\nRules used:\r\n```\r\n/* Input Type          Promoted Type */\r\n\r\nint8                   float32 (CPU), float16 (CUDA)\r\nint16                  float32\r\nint32                  float64\r\nint64                  float64\r\nbool                   float32 (CPU), float16 (CUDA)\r\n```\r\n(See: https://github.com/pytorch/pytorch/issues/28703#issuecomment-566241090)\r\n\r\nThis PR solves #28703.\r\n\r\ncc: @nairbv, @mcarilli ", "labels": ["module: numpy", "open source"], "number_of_comments": 4, "created_at": "2020-02-13 23:36:31", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564987781": {"author_username": "smessmer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33315, "title": "[wip] hack together funcptr based boxed dispatch", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33315 [wip] hack together funcptr based boxed dispatch**\n* #33313 Improve boxed dispatch performance\n* #32521 Use codegen'ed unboxing wrappers\n\nDifferential Revision: [D19892341](https://our.internmc.facebook.com/intern/diff/D19892341/)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 22:27:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564986742": {"author_username": "smessmer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33314, "title": "Use separate bitsets for Tensor and TensorList", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33314 Use separate bitsets for Tensor and TensorList**\n* #33313 Improve boxed dispatch performance\n* #32521 Use codegen'ed unboxing wrappers\n\nDifferential Revision: [D19892144](https://our.internmc.facebook.com/intern/diff/D19892144/)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-13 22:25:19", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564986690": {"author_username": "smessmer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33313, "title": "Improve boxed dispatch performance", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33315 [wip] hack together funcptr based boxed dispatch\n* **#33313 Improve boxed dispatch performance**\n* #32521 Use codegen'ed unboxing wrappers\n\nInstead of just remembering the number of arguments and iterating over the stack,\nthe DispatchKeyExtractor now remembers the exact locations of the dispatch relevant arguments\n(i.e. Tensor arguments) and only looks at those.\n\nDifferential Revision: [D19748549](https://our.internmc.facebook.com/intern/diff/D19748549/)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-13 22:25:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564957447": {"author_username": "lly-zero-one", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33312, "title": "Add quantized avgpool1d[WIP]", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-02-13 21:22:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564932691": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33309, "title": "Need to support clone for per channel quantized tensor", "body": "https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/quantized/QTensor.cpp#L160\r\n\r\nreported in: https://discuss.pytorch.org/t/pretrained-quantized-models-export-to-onnx-fails/67334\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a", "labels": ["quantization", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 20:30:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564887108": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33307, "title": "Include PyTorch version in serialization formats", "body": "Based on discussion in #31877, it is useful for tooling (and users) to know which version of PyTorch produced a particular binary. The easiest way to do this is probably to put `torch.__version__` into the serialization format somewhere. This will probably only go into the \"Eager v2\" and \"Script v4\" formats described [in this comment](https://github.com/pytorch/pytorch/issues/31877#issuecomment-572761949) (as well as any future formats).\r\n \r\n* add version information to C++ (since the jit serialization needs to know this to add it to the serialized binary)\r\n* add a file to the serialized zip called `archive/pytorch_version` or something that will contain the version string", "labels": ["enhancement", "module: serialization", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 18:59:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564881655": {"author_username": "qwhelan", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33306, "title": "[pytorch] Fix broken hypothesis timeout setting", "body": "Summary:\nFor `hypothesis` versions ranging from `3.16.0` to `5.0.0`, we attempt to override the `timeout` value with `hypothesis.unlimited`. As settings objects are immutable, we have to modify `__dict__` directly, bypassing the validator that has the following definition:\n```\ndef _validate_timeout(n):\n    if n is unlimited:\n        return -1\n    else:\n        return n\n```\nAccordingly, the correct setting for us is `-1`, not `hypothesis.unlimited`.\n\nAs a result of this error, it appears that any test that imported `hypothesis_utils` for the above `hypothesis` versions never ran successfully. A number of tests seem to have been disabled due to the side-effects of this bug, so re-enabling as part of the fix.\n\nTest Plan: `buck test`, broken hypothesis tests now pass\n\nDifferential Revision: D19873984\n\n", "labels": ["fb-exported"], "number_of_comments": 4, "created_at": "2020-02-13 18:48:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564877491": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33305, "title": "Remove special case codegen for tril_indices/triu_indices.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33362 Standardize expanded TensorOptions representation in native_functions.yaml\n* #33347 Update documentation on why _cudnn_init_dropout_state looks the way it is.\n* **#33305 Remove special case codegen for tril_indices/triu_indices.**\n\nThe current TensorOptions code is written to exactly extract out\nTensorOptions based on exact struct match, including default arguments.\nThat meant that tril_indices/triu_indices which had a different\ndefault argument didn't match, and thus needed a special case.\n\nI resolve this special case by instead replacing the explicit long\ndefault argument with a None default argument, and then adjusting\nthe actual implementations to select the correct dtype when none\nwas specified.  I think the general rule I'm following here is that\nit is always acceptable to replace an explicit default argument,\nwith a None argument (assuming the backend will compute it appropriately);\nthe documentation gets modestly worse, but everything that was\npreviously expressible continues to be expressible.  Maybe later\nwe should switch the default argument back to long, but for now\nthe simplification in code is worth it.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D19975411](https://our.internmc.facebook.com/intern/diff/D19975411)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-13 18:42:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564861071": {"author_username": "dfalbel", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33303, "title": "torch::var_out and dimnames", "body": "## \ud83d\udc1b Bug\r\n\r\nUsing `torch::var_out` in the C++ API with dimnames instead of the index of the dimension seems to return the standard deviation instead of the variance.\r\n\r\n## To Reproduce\r\n\r\nRunning the following program:\r\n\r\n```\r\n#include <torch/torch.h>\r\n#include <iostream>\r\n\r\nint main() {\r\n    auto d = torch::Dimname::fromSymbol(torch::Symbol::dimname(\"a\"));\r\n    std::vector<torch::Dimname> ds;\r\n    ds.push_back(d);\r\n    \r\n    auto x = torch::rand(100, ds);\r\n    \r\n    auto y = torch::zeros(1);\r\n    torch::var_out(y, x, {0});\r\n    std::cout << y << std::endl;\r\n    \r\n    auto z = torch::zeros(1);\r\n    torch::var_out(z, x, ds);\r\n    std::cout << z << std::endl;\r\n}\r\n```\r\n\r\nReturns: \r\n\r\n```\r\n(base) dfalbel@Daniels-MacBook-Pro build % ./example-app\r\nWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (operator() at ../c10/core/TensorImpl.h:845)\r\n0.073097\r\n[ CPUFloatType{} ]\r\n0.270365\r\n[ CPUFloatType{} ]\r\n```\r\n\r\nSince:\r\n\r\n```\r\nsqrt(0.073097)\r\n[1] 0.2703646\r\n```\r\n\r\nIt seems that when using dimnames `torch_var_out` is returning the standard deviation.\r\n\r\n## Expected behavior\r\n\r\nExpected both values to be identical.\r\n\r\n## Environment\r\n\r\nI am using Pytorch C++ 1.4 - downloaded from here: https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.4.0.zip on MacOS\r\n\n\ncc @yf225 @zou3519", "labels": ["module: cpp", "module: named tensor", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 18:09:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564852967": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33301, "title": "Don't take TensorOptions by reference", "body": "The class was explicitly designed to be two words large, we should pass it by value.\n\ncc @yf225", "labels": ["module: cpp", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 17:53:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564842961": {"author_username": "ggoossen", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33300, "title": "segfaults on .numpy() on cuda tensor", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. torch.zeros((7, ), device=\"cuda\").numpy()\r\n\r\nproduces segfaults.\r\n\r\nWith stacktrace include raising the exception with message to call .cpu method:\r\nhttps://our.intern.facebook.com/intern/diffusion/FBS/browse/master/fbcode/caffe2/torch/csrc/utils/tensor_numpy.cpp?commit=cea4fcd5898cf33ec83ebd2eac23eb035047fe27&lines=78-80\r\n\r\n## Expected behavior\r\n\r\nruntime error with message about having to call .cpu first.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): trunk\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source):\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version:\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519", "labels": ["high priority", "module: numpy", "topic: crash", "triage review", "triaged"], "number_of_comments": 2, "created_at": "2020-02-13 17:34:47", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564830266": {"author_username": "pbelevich", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33299, "title": "Tensor.random_ should be able to generate all 64 bit numbers including min and max value", "body": "```\r\nt = torch.empty(... , dtype=torch.int64)\r\nt.random_(torch.iinfo(torch.int64).min, None)\r\n```\r\nshould be able to fill tensor with all 64 bit number including `torch.iinfo(torch.int64).min` and `torch.iinfo(torch.int64).max` ", "labels": ["module: random", "triaged"], "number_of_comments": 1, "created_at": "2020-02-13 17:12:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564823904": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33298, "title": "[feature request] torch.expand to match -1 to existing dimensions if shape.count(-1) == ndim", "body": "```python\r\ntorch.rand(2).expand(-1, 3)\r\n#RuntimeError: The expanded size of the tensor (3) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [-1, 3].  Tensor sizes: [2]\r\n\r\ntorch.rand(2).unsqueeze(-1).expand(-1, 3)\r\n# works\r\n```\r\n\r\nDocs:\r\n```\r\nPassing -1 as the size for a dimension means not changing the size of that dimension.\r\n\r\nTensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.\r\n```\r\n\r\nHere I'm asking a new dimension at the end, not in the front. So it's not perfectly clear why it doesn't like leading -1.\r\n\r\nIf number of -1's is equal to the number of dimensions in the original tensor, the leading -1 limitation could be alleviated?", "labels": ["enhancement", "triaged"], "number_of_comments": 4, "created_at": "2020-02-13 17:01:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564820086": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33297, "title": "[JIT] Allow mutated values as functional graph inputs", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\r\n* **#33297 [JIT] Allow mutated values as inputs but not outputs**\r\n* #33199 [JIT] remove list appends\r\n* #33186 [JIT] Pass To Safely Remove Aten Inplace Ops\r\n* #33020 [JIT] Functional Graph Pass\r\n\r\nAllowing mutated values as inputs but not outputs has the effect of buffering up all mutated values as inputs to the graph. Just as we values which escape scope as graph inputs but not graph outputs - we should also allow values that get mutated. In both cases, the contract is that that the functional graph cannot write to graph inputs.\r\n\r\nWithout this patch, if there is a single write to the Tensor wildcard set it would disable all optimization.", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 16:55:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564809123": {"author_username": "YuechengLi", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33296, "title": "Training got stuck due to timeout from dataloader", "body": "\r\nThe same training script works well with Pytorch 1.4 before. Trying to test some new stuff in master branch (built from source), but training always got stuck after a few hundreds iterations without triggering any error info. If I ctrl-C it, it was traced down to some timeout function in dataloader. Again the same training code and configuration worked well with 1.4. Any clue?\r\n\r\nIteration 198: train = 1.8057, g_train = 0.9733, t_train = 0.8221, kl_train = 1.02579618\r\nIteration 199: train = 0.9988, g_train = 0.2920, t_train = 0.6974, kl_train = 0.93473649\r\nIteration 200: train = 1.3745, g_train = 0.4477, t_train = 0.9169, kl_train = 0.99940717\r\nsaved tex images for 200\r\nIteration 201: train = 1.1959, g_train = 0.3795, t_train = 0.8027, kl_train = 1.37421489\r\n^CTraceback (most recent call last):\r\n......\r\n......\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\r\n    data = self._next_data()\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 841, in _next_data\r\n    idx, data = self._get_data()\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 808, in _get_data\r\n    success, data = self._try_get_data()\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 761, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\r\n    if not self._poll(timeout):\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\r\n    return self._poll(timeout)\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\r\n    r = wait([self], timeout)\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\r\n    ready = selector.select(timeout)\r\n  File \"/mnt/home/xxxx/anaconda3/envs/pytorch-py36/lib/python3.6/selectors.py\", line 376, in select\r\n    fd_event_list = self._poll.poll(timeout)\r\nKeyboardInterrupt\r\n\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0a0+ab14375\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): USE_MPI=OFF python setup.py install\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.0\r\n\r\nThanks in advance!\r\n\n\ncc @SsnL @VitalyFedyunin @ngimel", "labels": ["module: dataloader", "topic: performance", "triaged"], "number_of_comments": 1, "created_at": "2020-02-13 16:38:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564796952": {"author_username": "iseeyuan", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33295, "title": "[Lite Interpreter] Serialize/deserialize/run __setstate__", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33295 [Lite Interpreter] Serialize/deserialize __setstate__**\n* #33294 register ops for quantized linear.\n\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 16:19:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564796792": {"author_username": "iseeyuan", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33294, "title": "register ops for quantized linear.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33295 [Lite Interpreter] Serialize/deserialize __setstate__\n* **#33294 register ops for quantized linear.**\n\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 16:19:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564748851": {"author_username": "daniilhayrapetyan", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33290, "title": "Fix error message when using as_strided with negative striding", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nRun following from jupyter lab console\r\n```python\r\nimport torch\r\nfoo = torch.arange(5)\r\nfoo.as_strided((5,), (-1,), storage_offset=4)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/daniil/.local/lib/python3.6/site-packages/torch/tensor.py\", line 159, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File \"/home/daniil/.local/lib/python3.6/site-packages/torch/_tensor_str.py\", line 311, in _str\r\n    tensor_str = _tensor_str(self, indent)\r\n  File \"/home/daniil/.local/lib/python3.6/site-packages/torch/_tensor_str.py\", line 209, in _tensor_str\r\n    formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n  File \"/home/daniil/.local/lib/python3.6/site-packages/torch/_tensor_str.py\", line 82, in __init__\r\n    for value in tensor_view:\r\n  File \"/home/daniil/.local/lib/python3.6/site-packages/torch/tensor.py\", line 462, in <lambda>\r\n    return iter(imap(lambda i: self[i], range(self.size(0))))\r\nRuntimeError: setStorage: sizes [], strides [], and storage offset 5 requiring a storage size of 6 are out of bounds for storage with numel 5\r\n\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nGive output:\r\n```\r\ntensor([4, 3, 2, 1, 0])\r\n```\r\n\r\n## Environment\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.4\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.4\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.4.2\r\n", "labels": ["good first issue", "topic: error checking", "triaged"], "number_of_comments": 2, "created_at": "2020-02-13 15:10:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564593294": {"author_username": "TTitcombe", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33287, "title": "Round to specified decimal place", "body": "## \ud83d\ude80 Feature\r\n`torch.round` to take an optimal argument which specifies the decimal place to which rounding should occur\r\n\r\n## Pitch\r\n`torch.round` should have the same functionality as `numpy.around`. Specifying a decimal place will round _up to_ that decimal place, using the same rounding strategy as `torch.round` currently uses. The default value of the decimal parameter should be 0 to keep current behaviour.\r\nE.g.\r\n```\r\n>>> tensor = torch.tensor([0.1234, 0.1237])\r\n>>>torch.round(tensor, decimals=3)\r\ntensor([0.123, 0.124])\r\n```\r\n\r\n## Alternatives\r\nYou can use `numpy.around` on the tensor, but I assume this has some performance hit?\r\n\n\ncc @yf225", "labels": ["enhancement", "module: cpp", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 10:43:32", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564540149": {"author_username": "xcnick", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33286, "title": "Migrate equal from the TH to Aten (CPU)", "body": "#24697 \r\n@VitalyFedyunin", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 3, "created_at": "2020-02-13 09:12:25", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564510568": {"author_username": "match08", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33285, "title": "iOS libtorch superpoint model bug", "body": "c++ mac libtorch SuperPoint model forward is success,  c++ ios libtorch SuperPoint model forward is loss.\r\n\r\nforward image size 640x480, ios  print  coordinates.total() is zero, is loss, mac print coordinates.total() not zero, is success.\r\n\r\nlibtorch info:\r\n\r\ncpu mode\r\nversion 1.4.0\r\n\r\ncode:\r\n \r\n ```\r\n auto qengines = at::globalContext().supportedQEngines();\r\n  if (std::find(qengines.begin(), qengines.end(), at::QEngine::QNNPACK) != qengines.end()) {\r\n    at::globalContext().setQEngine(at::QEngine::QNNPACK);\r\n  }\r\n  module = std::make_shared<torch::jit::script::Module>(torch::jit::load(modelPath));\r\n  module->eval();\r\n\r\n   cv::cvtColor(img, img, CV_BGR2GRAY);\r\n\r\n     std::vector<torch::jit::IValue> input;\r\n     torch::Tensor x = torch::from_blob(img.data, {1, 1, img.rows, img.cols}, at::kByte).toType(at::kFloat) / 255;\r\n     torch::autograd::AutoGradMode guard(false);\r\n     at::AutoNonVariableTypeMode non_var_type_mode(true);\r\n\r\n     input.push_back(x);\r\n    \r\n    // inference\r\n      auto outputs = module->forward(input).toTuple();\r\n      torch::Tensor semi = outputs->elements()[0].toTensor();\r\n      torch::Tensor coarse_desc = outputs->elements()[1].toTensor();\r\n\r\n      //    std::cout << semi << std::endl;\r\n      const int CELL = 8;\r\n      // Deal with feature points locations\r\n      semi = semi.squeeze();\r\n      torch::Tensor dense = semi.exp();\r\n      dense = dense / (dense.sum(0) + 0.00001);\r\n      torch::Tensor nodust = dense.slice(0, 0, dense.size(0) - 1);\r\n      int Hc = img.rows / CELL;\r\n      int Wc = img.cols / CELL;\r\n      nodust = nodust.transpose(0, 1).transpose(1, 2);\r\n      torch::Tensor heatmap = nodust.reshape({Hc, Wc, CELL, CELL});\r\n      heatmap = heatmap.transpose(1, 2);\r\n      heatmap = heatmap.reshape({Hc * CELL, Wc * CELL});\r\n      torch::Tensor heatmap_cpu = heatmap.to(torch::kCPU, /*non_blocking=*/true);\r\n\r\n      // Eigen::Map<Eigen::MatrixXf> eig_heatmap(heatmap_cpu.data<float>(),\r\n      //                                         heatmap.size(0), heatmap.size(1));\r\n      cv::Mat mat_heatmap(heatmap_cpu.size(0), heatmap_cpu.size(1), CV_32F,\r\n                        heatmap_cpu.data<float>());\r\n\r\n     /**\r\n     *  Filtered to binary mat first and than get non zero value coordinates\r\n     *  cv::threshold(src, dst, threshold, max_binary_value, threshold_type)\r\n     *  threshold_type: 0, Binary\r\n     *                  1, Binary Inverted\r\n     *                  2, Threshold Truncated\r\n     *                  3, Threshold to Zero\r\n     *                  4, Threshold to Zero Inverted\r\n     */\r\n    cv::Mat bin_mat;\r\n    cv::threshold(mat_heatmap, bin_mat, 0.015, 1, 0);\r\n    bin_mat.convertTo(bin_mat, CV_8UC1);\r\n    cv::Mat coordinates;\r\n    cv::findNonZero(bin_mat, coordinates);\r\n\r\n    printf(\"size: %d\\n\",coordinates.total());\r\n```\r\n\r\n**model download:**\r\n[super_point_scripted_model.pt.zip](https://github.com/pytorch/pytorch/files/4197136/super_point_scripted_model.pt.zip)\r\n\r\nabout SuperPoint: https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork\r\n\n\ncc @ezyang", "labels": ["mobile", "module: ios", "module: osx", "shadow review"], "number_of_comments": 0, "created_at": "2020-02-13 08:12:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564471747": {"author_username": "chenzhekl", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33284, "title": "[C++] Add strong wolfe line search to the LibTorch interface", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd the same strong wolfe line search option to the LibTorch interface as the Python counterpart.\r\n\r\nhttps://pytorch.org/docs/master/optim.html\r\n\r\n## Motivation\r\n\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\n\r\nUnify the interface between Python and C++\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\nImplement the strong wolfe line search algorithm and add a flag to the `LBFGSOptions` struct\r\n\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @yf225", "labels": ["enhancement", "module: cpp", "triaged"], "number_of_comments": 1, "created_at": "2020-02-13 06:36:11", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564419616": {"author_username": "riyadshairi979", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33281, "title": "Uninitialised value was created by a stack allocation, reported by valgrind", "body": "```\r\n==17== Memcheck, a memory error detector\r\n==17== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==17== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info\r\n==17== Command: ros/src/inference_api/test_graph_pytorch\r\n==17== \r\n==17== Warning: set address range perms: large range [0x14dfa000, 0x373f0000) (defined)\r\n==17== Warning: set address range perms: large range [0x5ddf4000, 0x72c1b000) (defined)\r\n2020-02-13 03:23:28.469999: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2020-02-13 03:23:30.154755: I external/org_tensorflow/tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n==17== Conditional jump or move depends on uninitialised value(s)\r\n==17==    at 0x4B7E905: IValue (ivalue.h:66)\r\n==17==    by 0x4B7E905: optional (Optional.h:401)\r\n==17==    by 0x4B7E905: c10::Argument::Argument(c10::Argument const&) (function_schema.h:34)\r\n==17==    by 0x4B7DD97: construct<c10::Argument, c10::Argument &> (memory:1825)\r\n==17==    by 0x4B7DD97: __construct<c10::Argument, c10::Argument &> (memory:1717)\r\n==17==    by 0x4B7DD97: construct<c10::Argument, c10::Argument &> (memory:1560)\r\n==17==    by 0x4B7DD97: __construct_range_forward<c10::Argument *, c10::Argument *> (memory:1645)\r\n==17==    by 0x4B7DD97: __construct_at_end<c10::Argument *> (vector:1074)\r\n==17==    by 0x4B7DD97: std::__1::vector<c10::Argument, std::__1::allocator<c10::Argument> >::vector(std::__1::vector<c10::Argument, std::__1::allocator<c10::Argument> > const&) (vector:1257)\r\n==17==    by 0x12AC2F6B: c10::FunctionSchema::FunctionSchema(c10::FunctionSchema const&) (function_schema.h:132)\r\n==17==    by 0x139D366A: c10::RegisterOperators::registerOp_(c10::RegisterOperators::Options&&) (op_registration.cpp:138)\r\n==17==    by 0x139D247D: c10::RegisterOperators::checkSchemaAndRegisterOp_(c10::RegisterOperators::Options&&) (op_registration.cpp:59)\r\n==17==    by 0x138BF44D: op (op_registration.h:455)\r\n==17==    by 0x138BF44D: __cxx_global_var_init (Copy.cpp:168)\r\n==17==    by 0x138BF44D: _GLOBAL__sub_I_Copy.cpp (Copy.cpp:0)\r\n==17==    by 0x4010732: call_init (dl-init.c:72)\r\n==17==    by 0x4010732: _dl_init (dl-init.c:119)\r\n==17==    by 0x40010C9: ??? (in /lib/x86_64-linux-gnu/ld-2.27.so)\r\n==17==  Uninitialised value was created by a stack allocation\r\n==17==    at 0x12DAD3CD: torch::jit::script::(anonymous namespace)::SchemaParser::parseArgument(unsigned long, bool, bool) (function_schema_parser.cpp:112)\r\n==17== \r\n{\r\n   <insert_a_suppression_name_here>\r\n   Memcheck:Cond\r\n   fun:IValue\r\n   fun:optional\r\n   fun:_ZN3c108ArgumentC2ERKS0_\r\n   fun:construct<c10::Argument, c10::Argument &>\r\n   fun:__construct<c10::Argument, c10::Argument &>\r\n   fun:construct<c10::Argument, c10::Argument &>\r\n   fun:__construct_range_forward<c10::Argument *, c10::Argument *>\r\n   fun:__construct_at_end<c10::Argument *>\r\n   fun:_ZNSt3__16vectorIN3c108ArgumentENS_9allocatorIS2_EEEC2ERKS5_\r\n   fun:_ZN3c1014FunctionSchemaC2ERKS0_\r\n   fun:_ZN3c1017RegisterOperators11registerOp_EONS0_7OptionsE\r\n   fun:_ZN3c1017RegisterOperators25checkSchemaAndRegisterOp_EONS0_7OptionsE\r\n   fun:op\r\n   fun:__cxx_global_var_init\r\n   fun:_GLOBAL__sub_I_Copy.cpp\r\n   fun:call_init\r\n   fun:_dl_init\r\n   obj:/lib/x86_64-linux-gnu/ld-2.27.so\r\n}\r\n==17== \r\n==17== Exit program on first error (--exit-on-first-error=yes)\r\n```\n\ncc @ezyang @gchanan @zou3519 @suo", "labels": ["high priority", "jit", "shadow review", "triage review"], "number_of_comments": 0, "created_at": "2020-02-13 03:41:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564415352": {"author_username": "XiaobingSuper", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33280, "title": "Move cumprod and cumsum to Aten(CPU)", "body": "This PR is about move cumprod and cumsum to Aten.\r\nTest script:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport time\r\n\r\ntorch.manual_seed(0)\r\n\r\ndef _time():\r\n    return time.time()\r\n\r\ndevice = \"cpu\"\r\n\r\n#torch.set_num_threads(1)\r\n\r\n#warm up\r\nfor n in [10, 300]:\r\n    input = torch.randn(n, n, n, requires_grad=False, device=device)\r\n    for dim in range(input.dim()):\r\n        for i in range(100):\r\n            #output = input.cumsum(dim)\r\n            output = input.cumprod(dim)\r\n\r\nfor n in [10, 300]:\r\n    input = torch.randn(n, n, n, requires_grad=False, device=device)\r\n    for dim in range(input.dim()):\r\n        fwd_t = 0\r\n        for i in range(1000):\r\n            t1 = _time()\r\n            #input.cumsum(dim)\r\n            input.cumprod(dim)\r\n            t2 = _time()\r\n            fwd_t = fwd_t + (t2 -t1)\r\n        fwd_avg = fwd_t / 1000 * 1000\r\n        print(\"size = (%d, %d, %d); reduce dim=%d; compute time is %.4f(ms)\" % (n, n, n, dim, fwd_avg))\r\n```\r\nTest device: **skx-8180**.\r\nPerformance:\r\n```\r\ncumsum:\r\nBefore:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0100(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0092(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0091(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 212.2306(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 249.1231(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 66.3095(ms)\r\nnumber_threads =1:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0094(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0089(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0087(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 215.6530(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 251.7798(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 66.7845(ms)\r\n\r\nAfter:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0036(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0036(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0036(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 78.5254(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 74.6820(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 16.9497(ms)\r\n\r\nnumber_threads =1:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0035(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0035(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0035(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 81.7005(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 77.0945(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 47.1614(ms)\r\n```\r\n```\r\ncumprod:\r\nBefore:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0102(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0095(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0094(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 215.0319(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 248.8498(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 67.2285(ms)\r\nnumber_threads =1:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0100(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0093(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0092(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 215.4908(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 250.2004(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 67.1437(ms)\r\n\r\nAfter:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0037(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0037(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0037(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 165.0736(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 162.5086(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 48.1611(ms)\r\n\r\nnumber_threads =1:\r\nsize = (10, 10, 10); reduce dim=0; compute time is 0.0038(ms)\r\nsize = (10, 10, 10); reduce dim=1; compute time is 0.0037(ms)\r\nsize = (10, 10, 10); reduce dim=2; compute time is 0.0038(ms)\r\nsize = (300, 300, 300); reduce dim=0; compute time is 168.8640(ms)\r\nsize = (300, 300, 300); reduce dim=1; compute time is 164.8462(ms)\r\nsize = (300, 300, 300); reduce dim=2; compute time is 143.2414(ms)\r\n```\r\nFix https://github.com/pytorch/pytorch/issues/24668, https://github.com/pytorch/pytorch/issues/24669.", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 10, "created_at": "2020-02-13 03:27:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414347": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33279, "title": "[quant][graphmode] Add quantized conv2d-relu fusion pattern", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* **#33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern**\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 03:23:31", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414287": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33277, "title": "[WIP][quant][graphmode] Observing input/output values in callsite", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* **#33277 [WIP][quant][graphmode] Observing input/output values in callsite**\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nCurrently we insert observer in the called graph, which is incorrect since graphs can be shared\nand the decision of whether to insert observer or not might dependend on where the graph is called.\nFor example, for a call sequence `self.conv1(self.conv2(x))`, we can't inserting observer correctly\nif `self.conv1` and `self.conv2` are sharing the same type in the current implementation, because we insert\nobserver in the graph of the forward meethod of Conv2d right now and this call sequence requires us to insert\nonly one observer for the output of self.conv1/intpu of self.conv2.\nWe'll need to insert observers for input/output values of the graph in callsite instead.\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-02-13 03:23:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414237": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33276, "title": "[quant][graphmode][refactor] Move check for weight outside of insertObserverFor", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* **#33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor**\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 03:23:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414213": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33275, "title": "[quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* **#33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized**\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 03:23:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414177": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33274, "title": "[quant][graphmode][refactor] Simplify signature for insertObserverFor", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* **#33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor**\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 03:22:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564414144": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33273, "title": "[quant][graphmode][refactor] Checks for bias and weight", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* **#33273 [quant][graphmode][refactor] Checks for bias and weight**\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\n- Move the check for bias to valueNeedsToBeQuantized\n- Move TORCH_CHECK inside the functions for checking if a value is bias or weight\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-13 03:22:48", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564409526": {"author_username": "HenryJia", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33272, "title": "torch.onnx.export fails with negative dimensions for torch.tensor.flatten", "body": "## \ud83d\udc1b Bug\r\n\r\nThe if statement at\r\n\r\nhttps://github.com/pytorch/pytorch/blob/f9ad5528e0dc7d0af055b2214a14a89c1853fa2c/torch/onnx/symbolic_opset9.py#L1815\r\n\r\nbreaks if flatten is fed a negative dimension, making models using torch.tensor.flatten with negative dimension not ONNX exportable\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\nclass foo(nn.Module):\r\n    def forward(self, x):\r\n        return x.flatten(start_dim=-2, end_dim=-1)\r\n\r\n\r\nbar = foo()\r\n\r\nout = bar(torch.rand(100, 200, 300))\r\nprint(out.shape)\r\n\r\ntorch.onnx.export(bar, torch.rand(100, 200, 300), 'bar.onnx', verbose=True)\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe above code will cause an IndexError: list index out of range error at \r\n\r\nhttps://github.com/pytorch/pytorch/blob/f9ad5528e0dc7d0af055b2214a14a89c1853fa2c/torch/onnx/symbolic_opset9.py#L1815\r\n\r\nas a negative dimension will cause the if statement to execute and try and index into an empty list\r\n\r\n## Environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.2.0\r\nCMake version: version 3.16.4\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce RTX 2070\r\n\r\nNvidia driver version: 440.59\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.0\r\n[pip3] numpydoc==0.9.2\r\n[pip3] torch==1.4.0a0+640109a\r\n[pip3] torchaudio==0.4.0a0+bdf9255\r\n[pip3] torchtext==0.4.0\r\n[pip3] torchvision==0.5.0a0+227027d\r\n[conda] Could not collect\r\n```\r\n## Additional context\r\nNone applicable\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 0, "created_at": "2020-02-13 03:06:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564404830": {"author_username": "yf225", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33270, "title": "TestCppExtensionAOT.test_cuda_extension is flaky on Windows", "body": "It seems that `test_cuda_extension (__main__.TestCppExtensionAOT)` test is flaky on Windows now, starting from https://github.com/pytorch/pytorch/pull/33084.\r\n```\r\n======================================================================\r\nFAIL: test_cuda_extension (__main__.TestCppExtensionAOT)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"test_cpp_extensions_aot.py\", line 67, in test_cuda_extension\r\n    self.assertEqual(z, torch.ones_like(z))\r\n  File \"C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\testing\\_internal\\common_utils.py\", line 862, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File \"C:\\Users\\circleci\\project\\build\\win_tmp\\build\\torch\\testing\\_internal\\common_utils.py\", line 832, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: tensor(1.) not less than or equal to 1e-05 : \r\n\r\n----------------------------------------------------------------------\r\n```\r\nInstances of the failure:\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4469204\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4470030\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4470035\r\nhttps://app.circleci.com/jobs/github/pytorch/pytorch/4479604\r\n\n\ncc @peterjc123 @ezyang @gchanan @zou3519", "labels": ["module: windows", "topic: flaky-tests", "triaged"], "number_of_comments": 4, "created_at": "2020-02-13 02:49:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564401411": {"author_username": "mingfeima", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33269, "title": "port masked_select from TH to ATen and optimize perf on CPU", "body": "This PR ports `masked_select` from TH to ATen and optimize the performance on CPU with TensorIterator.\r\n\r\nhttps://github.com/pytorch/pytorch/issues/33053\r\n\r\n1. single socket run: up to **5.4x** speedup;\r\n2. single core run: up to **1.16x** speedup.", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 2, "created_at": "2020-02-13 02:37:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564385566": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33268, "title": "Added tensor.is_complex(), is_complex and dtype.is_complex py binding, tensor printing, and dixed the scalar type returned for complex float", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33507 Gradcheck for complex\n* #33361 torch.tensor can infer complex dtype now\n* **#33268 Added tensor.is_complex(), is_complex and dtype.is_complex py binding, tensor printing, and dixed the scalar type returned for complex float**\n\nDifferential Revision: [D19907698](https://our.internmc.facebook.com/intern/diff/D19907698)", "labels": [], "number_of_comments": 5, "created_at": "2020-02-13 01:41:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564377645": {"author_username": "wanchaol", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33263, "title": "[jit] allow RRef local creation with IValue objects", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33263 [jit] allow RRef local creation with IValue objects**\n* #33526 [jit] make RRef type annotation available in Python\n* #33369 [jit] infer RRef type as container type\n\nThis PR allow PyRRef local creation to inspect the pyobject, if it\nfounds that we could turn it to an IValue, turn to an IValue first,\notherwise hold it as a PyObjectType\n\nDifferential Revision: [D19871243](https://our.internmc.facebook.com/intern/diff/D19871243)", "labels": ["jit"], "number_of_comments": 3, "created_at": "2020-02-13 01:14:56", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564369798": {"author_username": "bzinodev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33258, "title": "Freezing TorchScript Modules ", "body": "PyTorch is a powerful programming language. It provides several attractive features to support ML community. However, it poses significant challenges to optimize and/or to target hardware capabilities. One of the roadblocks is that PyTorch Modules carry global state that can be mutated from outside the module. This limits Torch JIT ability to perform optimizations. \r\n\r\nFreezing allows the assumption all attribute values are **final** which enables the following optimizations: \r\n\r\n- Internalization of all immutable attributes as a constant.\r\n- Aggressively apply Torch JIT optimizations to clean up and lower the graph. \r\n- Enable Post-freezing optimizations, For example: data layout optimizations, quantization and related optimizations, or lowering PyTorch to foreign backend specific IR.\r\n\r\nAt 10.000 feet, freezing works as described below: \r\n\r\n- Clone the original module, including its class type and deep copy its attribute values.\r\n- Inlined attribute values as well as calls to method into forward function.\r\n- Run Torch JIT optimizations.\r\n- Clean up cloned module by deleting all attributes, submodules, and non forward method.\r\n\r\nSee   [PR](https://github.com/pytorch/pytorch/pull/32178). Several tests are included describing the handling of some edge cases. \r\n \r\n## Motivation\r\nFreezing is suitable for inference where all attributes/parameter are final. Freezing provides a safe/sound IR that allows TorchScript to perform aggressive optimizations. The IR is lowered and becomes more suitable to lower to low-level IR for targeting various hardware. \r\n \r\n[optimization pipeline.pdf](https://github.com/pytorch/pytorch/files/4195286/optimization.pipeline.pdf)\r\n\r\n## Proposed API\r\n\r\n```python\r\norig_module = torch.jit.script(MyModel())   # or trace, or load\r\nfrozen_module = torch.jit.freeze(orig_module)   # In current PR torch._C._freeze_module(orig_module)\r\nprint(orig_module.weight) # Works\r\nexpect_throw(print(frozen_module.weight)) # Fails because all attributes are removed\r\n```\r\n\r\nUser can preserve non forward methods using interfaces: \r\n\r\n```python\r\n@torch.jit.interface\r\nclass RequestedIFace(object):\r\n    def one(self, x, y):\r\n        # type: (Tensor, Tensor) -> Tensor\r\n        pass\r\n\r\n    def two(self, x):\r\n        # type: (Tensor) -> Tensor\r\n        pass\r\n      \r\norig_module = torch.jit.script(MyModel()) # or trace, or load\r\nfrozen_module = freeze(orig_module, iface=RequestedIFace)\r\nfrozen_module.two(x,y) # works\r\n```\r\nBefore folding an attribute as a constant value, Freezing is responsible to verify that attributes do not mutate internally. In this case, the attribute is preserved. \r\n\r\n### inplace freezing \r\nOne useful feature is to enable inplace freezing which allows to reuse the data of the original module without copying. To enable such a functionality we need to freeze tensor (make it read only, or deleting tensor so that no other alias outside module can access it). Inplace freezing is beyond the current proposal as it requires further support in Pytorch.\r\n\r\n\r\n### Alternatives\r\n- Tensor freezing? It benefits eager mode too. As discussed above freezing tensors is a useful feature for TorchScript as well. \r\n\r\n\r\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-13 00:48:53", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564351870": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33255, "title": "[jit] Add type tags to lists/dicts in pickle", "body": "Stacked PRs\n * #33474 - [jit] Remove list specializations from pickler\n * **#33255 - [jit] Add type tags to lists/dicts in pickle**\n\nThis adds a global call to `torch.jit._pickle.restore_type_tags` for\nlists and dicts so that we can preserve their types after serialization.\n\nDifferential Revision: [D19868637](https://our.internmc.facebook.com/intern/diff/19868637/)", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-02-12 23:51:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564332894": {"author_username": "xuhdev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33252, "title": "Vectorize in-place comparison operators", "body": "Benchmark: (Debian 10, Release build, gcc 8.3, no turbo, Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz)\r\n\r\n```python\r\nimport timeit\r\nfor op in ('gt', 'lt', 'ge', 'le', 'eq', 'ne'):\r\n    for dtype in ('torch.float', 'torch.double', 'torch.int16', 'torch.int32', 'torch.int64'):\r\n        for n, t in [(10_000, 100000),\r\n                    (100_000, 10000)]:\r\n            print(f'a.{op}_(b), numel() == {n} for {t} times, dtype={dtype}')\r\n            print(timeit.timeit(f'a.{op}_(b)', setup=f'import torch; a = torch.arange(1, {n}, dtype={dtype}); b = torch.arange({n}, 1, -1, dtype={dtype})', number=t))\r\n```\r\n\r\nBefore:\r\n\r\n```\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n1.2309490199986612\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n1.0022814019976067\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n1.3803057660006743\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n1.2055585509988305\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7466133359994274\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.6369958010000119\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7527669789997162\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6489553680003155\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.7773899399981019\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6871988669990969\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n1.0753146550014208\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.9385063910012832\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n1.1030586669985496\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.9390859359991737\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7597423450024507\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.638558884998929\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7599794150009984\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6476956129990867\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.7988086400000611\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6782525559974601\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n1.0889462890008872\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.9374298619986803\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n1.235068689999025\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n1.1786019999999553\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7625467520010716\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.6376865020029072\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7675555499990878\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6537884379977186\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.800744506999763\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6780484070004604\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.76876403499773\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.647443950001616\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.7914792369992938\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.6717289250009344\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7636175869993167\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.6386979790004261\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7622551630011003\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6472297239997715\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.7912157330029004\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6849328239986789\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.8129878280014964\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.6680980550008826\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n1.1244862209969142\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.9494725500007917\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7664825959982409\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.6396376529992267\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7707611549994908\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6490703500021482\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.8109213290008483\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6802137279992166\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.7736273650007206\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.6607353870022052\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.8238997540029231\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.7072973960021045\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.7543337829993106\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.6374589000006381\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.7579922389995772\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.6471768829978828\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.7908669329990516\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.6808502029998635\r\n```\r\n\r\nAfter:\r\n\r\n```\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.2540436880008201\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.1820384839993494\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.3486944219985162\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.34815859499940416\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.17254955400130711\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09453885599941714\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.2270369040015794\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.17212342399943736\r\na.gt_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.3539326970021648\r\na.gt_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.3344709220000368\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.23946997299935902\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.19209146699722623\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.37651707099939813\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.3665395339994575\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.17921338000087417\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09568335600124556\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.23324955700081773\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.19112185400081216\r\na.lt_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.3575552209986199\r\na.lt_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.37249238900039927\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.24360957899989444\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.1754653829993913\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.3841557770028885\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.33180867199916975\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.18581743699905928\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09396228900004644\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.23997853699984262\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.18289255000127014\r\na.ge_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.3743895369989332\r\na.ge_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.3344830099995306\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.2267126599981566\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.19904153700190363\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.37588445900109946\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.3675915179992444\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.17487468800027273\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09487799699854804\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.22027382500164094\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.1992355910006154\r\na.le_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.36730866299694753\r\na.le_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.3702153049998742\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.23196656099753454\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.18266681599925505\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.3674931280002056\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.33750609300113865\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.1786003349989187\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09138609599904157\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.22916236699893489\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.18246461099988665\r\na.eq_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.3624721829983173\r\na.eq_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.33055394500115653\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.float\r\n0.2329704199983098\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.float\r\n0.19987284200033173\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.double\r\n0.37779013600084\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.double\r\n0.3676817659979861\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int16\r\n0.18078503400101908\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int16\r\n0.09844630900261109\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int32\r\n0.23319009599799756\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int32\r\n0.20099951900192536\r\na.ne_(b), numel() == 10000 for 100000 times, dtype=torch.int64\r\n0.370754808998754\r\na.ne_(b), numel() == 100000 for 10000 times, dtype=torch.int64\r\n0.37584513000183506\r\n```\r\n\r\n", "labels": ["module: vectorization", "open source", "triaged"], "number_of_comments": 2, "created_at": "2020-02-12 23:03:42", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564265025": {"author_username": "charleshofer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33249, "title": "Core dumps being created when running test_c10d.py and test_multiprocessing_spawn.py", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen running `test/distributed/test_c10d.py` and `test/test_multiprocessing_spawn.py` in Power CI (and when run on my local Power system), some core files appear in the repo even though all the tests pass. I dug a little deeper and it looks like they're being caused by `NcclErrorHandlingTest.test_nccl_errors_blocking_abort` and `ForkTest.test_terminate_signal`. Both of these tests seem to be testing that terminating processes exit when some errors are thrown or a signal is sent. When running in CI, `.jenkins/pytorch/test.sh` checks that the repository is clean every tests are run. Isn't it reasonable to expect that these tests cases would create core dumps sometimes?\r\n\r\nI've only seen this on Power. `NcclErrorHandlingTest.test_nccl_errors_blocking_abort` seems to be passing fine on x86 without creating any core dumps, but `ForkTest.test_terminate_signal` isn't run on x86 CI.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Build latest master\r\n2. Run `test/test_multiprocessing_spawn.py` and `test/distributed/test_c10d.py`.\r\n3. These should create core dumps in the directory you ran the tests from.\r\n4. When run in CI with `.jenkins/pytorch/test.sh`, creating core dumps causes the script to fail.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nEither no core dumps should be created, or if it's okay for those test cases to create core dumps, the `.jenkins/pytorch/test.sh` script should somehow account for that.\r\n", "labels": ["module: tests", "triaged"], "number_of_comments": 1, "created_at": "2020-02-12 20:54:47", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564225160": {"author_username": "rothn", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33248, "title": "PyTorch 1.4.0 CUDA initialization error with CPU-only (multiprocessing) on Python 3.7.5", "body": "## Bug\r\nUsing only the CPU device, PyTorch 1.4.0 crashes on Python 3.7.5 when training a neural network on the main process and then training a different network on child processes.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Train a neural network on the parent process\r\n2. Create 5 child processes and train the neural network on them, sleeping to wait for background activity\r\n3. Observe the error\r\n\r\n**Code to Reproduce**\r\n```import random\r\nfrom multiprocessing import Process, Queue\r\nimport time\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\n\r\nprint(\"PyTorch: {}\".format(torch.__version__))\r\n\r\ndef init_weights(m):\r\n    if type(m) == nn.Linear:\r\n        m.weight.data.uniform_()\r\n        m.bias.data.uniform_()\r\n\r\ndef do_neural_stuff():\r\n    # Initialize the network\r\n    fcffnn = nn.Sequential(\r\n            nn.Linear(1, 10),\r\n            nn.Sigmoid(),\r\n            nn.Linear(10, 10),\r\n            nn.Sigmoid(),\r\n            nn.Linear(10, 2)\r\n            )\r\n    fcffnn.apply(init_weights)\r\n\r\n    # Run the network\r\n    loss_f = nn.CrossEntropyLoss()\r\n    opt = optim.Adam(fcffnn.parameters(), lr = 3e-4, weight_decay = 0.001)\r\n\r\n    # Compute loss, backpropagate, adjust params along neg error gradient\r\n    output = fcffnn(torch.tensor([0], dtype=torch.float))\r\n    loss = loss_f(output.view(1, 2), torch.tensor([1], dtype=torch.long))\r\n    fcffnn.zero_grad()\r\n    loss.backward()\r\n    opt.step()\r\n    \r\n# Run the network on the parent process\r\ndo_neural_stuff() # PyTorch works as expected when this line is commented out\r\n\r\ndef parallelModelRunnerProc(proc_nb, response_queue):\r\n    print(\"Forked Process {}\".format(proc_nb))\r\n    do_neural_stuff()\r\n    \r\n    # This is important -- without the sleep(), the threads exit\r\n    # without ever crashing due to a native error in this simplified example\r\n    time.sleep(5)\r\n    r_queue.put(True)\r\n\r\n# 1) Run the network on 5 child processes\r\n# 2) Wait for background activity (CUDA initialization -- even though it is not asked for)\r\n# 3) Observe and report error\r\nr_queue = Queue()\r\nservers = []\r\nnb_responses = 0\r\nchild_ct = 5\r\nfor proc_nb in range(child_ct):\r\n    server = Process(target=parallelModelRunnerProc, args=(proc_nb, r_queue))\r\n    server.start()\r\n    servers.append(server)\r\n\r\nwhile nb_responses < child_ct:\r\n    nb_dead = sum([not x.is_alive() for x in servers])\r\n    if not r_queue.empty():\r\n        r_queue.get()\r\n        nb_responses += 1\r\n    \r\n    if nb_dead > nb_responses:\r\n        raise Exception(\"Something died unexpectedly\")\r\n\r\n```\r\n\r\n**Program Output**\r\n```\r\nPyTorch: 1.4.0\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: initialization error (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fe417f65193 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xd0c8 (0x7fe4181980c8 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fe3bd8a8fd9 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: torch::autograd::Engine::thread_init(int) + 0x1a (0x7fe3bd8a995a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #4: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7fe40449891a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0xedef (0x7fe418b80def in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\r\nframe #6: <unknown function> + 0x76db (0x7fe41ca896db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fe41c7b288f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: initialization error (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fe417f65193 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xd0c8 (0x7fe4181980c8 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fe3bd8a8fd9 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: torch::autograd::Engine::thread_init(int) + 0x1a (0x7fe3bd8a995a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #4: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7fe40449891a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0xedef (0x7fe418b80def in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\r\nframe #6: <unknown function> + 0x76db (0x7fe41ca896db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fe41c7b288f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: initialization error (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fe417f65193 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xd0c8 (0x7fe4181980c8 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fe3bd8a8fd9 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: torch::autograd::Engine::thread_init(int) + 0x1a (0x7fe3bd8a995a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #4: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7fe40449891a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0xedef (0x7fe418b80def in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\r\nframe #6: <unknown function> + 0x76db (0x7fe41ca896db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fe41c7b288f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: initialization error (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fe417f65193 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xd0c8 (0x7fe4181980c8 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fe3bd8a8fd9 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: torch::autograd::Engine::thread_init(int) + 0x1a (0x7fe3bd8a995a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #4: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7fe40449891a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0xedef (0x7fe418b80def in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\r\nframe #6: <unknown function> + 0x76db (0x7fe41ca896db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fe41c7b288f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nterminate called after throwing an instance of 'c10::Error'\r\n  what():  CUDA error: initialization error (setDevice at /pytorch/c10/cuda/impl/CUDAGuardImpl.h:42)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7fe417f65193 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: <unknown function> + 0xd0c8 (0x7fe4181980c8 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\r\nframe #2: torch::autograd::Engine::set_device(int) + 0x159 (0x7fe3bd8a8fd9 in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #3: torch::autograd::Engine::thread_init(int) + 0x1a (0x7fe3bd8a995a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch.so)\r\nframe #4: torch::autograd::python::PythonEngine::thread_init(int) + 0x2a (0x7fe40449891a in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #5: <unknown function> + 0xedef (0x7fe418b80def in /home/nick.roth/ml_python/lib/python3.7/site-packages/torch/_C.cpython-37m-x86_64-linux-gnu.so)\r\nframe #6: <unknown function> + 0x76db (0x7fe41ca896db in /lib/x86_64-linux-gnu/libpthread.so.0)\r\nframe #7: clone + 0x3f (0x7fe41c7b288f in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\nTraceback (most recent call last):\r\n  File \"pytorch_crash.py\", line 69, in <module>\r\n    raise Exception(\"Something died unexpectedly\")\r\nException: Something died unexpectedly\r\n```\r\n\r\n## Expected behavior\r\n\r\nPyTorch should be usable on both child and parent processes if the CUDA device is never used.\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Tesla V100-SXM2-16GB\r\nNvidia driver version: 435.21\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n```\r\n\r\nCUDA version is 10.1 based on output from `nvidia-smi`.\r\n\r\nPyTorch was installed on a virtualenv using pip. The machine used to reproduce this bug is an AWS p3.2xlarge instance with CUDA support provided by nvidia-driver-435 and nvidia-utils-435 packages.\r\n\r\nThe box was deployed in the `us-west-2` AWS region with the `ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-20190212.1 (ami-005bdb005fb00e791)` base image.\r\n\r\n## Additional context\r\n\r\nSimilar reports to this exist, but they either lack reproducibility or are classified as user error since the programmer attempts to use CUDA on multiple processes in a tree. See #30900, #2517 and #3491. In our environment, using `spawn` instead of `fork` to create the child processes does not resolve the issue.\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @ngimel", "labels": ["module: autograd", "module: cuda", "triaged"], "number_of_comments": 9, "created_at": "2020-02-12 19:42:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564214488": {"author_username": "wanchaol", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33246, "title": "[rpc] make UserRRef/OwnerRRef constructor private for future C++ API", "body": "in https://github.com/pytorch/pytorch/pull/33189 we switch RREf to be managed by intrusive_ptr, and we made the UserRRef/OwnerRRef public to make `c10::make_intrusive<OwnerRRef>(getWorkerId(), rrefId, type)` work since it does not support private constructor, and `intrusive_ptr<OwnerRRef>(new OwnerRRef())` does not work because of intrusive_ptr limitation. \r\n\r\nWe should figure out a way to make it private again before we announce C++ API for rpc.\n\ncc @yf225 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: cpp", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-12 19:23:46", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564206887": {"author_username": "houseroad", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33245, "title": "Automatic update of fbcode/onnx to f8e140a9c4e7b42fb970e247478667093b5c748b", "body": "Summary:\nPrevious import was 8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e\n\nIncluded changes:\n- **[f8e140a9](https://github.com/onnx/onnx/commit/f8e140a9)**: Kezhan/function update (#2596) <Ke Zhang>\n- **[6185faae](https://github.com/onnx/onnx/commit/6185faae)**: fix the attribute types section in IR.md (#2590) <Ke Zhang>\n- **[f254647a](https://github.com/onnx/onnx/commit/f254647a)**: Allow Constant operator to promote scalar and list to tensors. (#2592) <Jeremy Cochoy>\n- **[f12ec799](https://github.com/onnx/onnx/commit/f12ec799)**: Add NegativeLogLikelihood(NllLoss) op (#2551) <liqunfu>\n\nTest Plan: ci\n\nReviewed By: hl475\n\nDifferential Revision: D19860358\n\n", "labels": ["fb-exported"], "number_of_comments": 6, "created_at": "2020-02-12 19:09:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564186533": {"author_username": "zasdfgbnm", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33244, "title": "Make ELU great again", "body": "Due to compiler bug, we have to make some workaround on ELU for CUDA. A necessary condition for this bug to happen is `invoke_with_array` in `Loops.cuh`. Now, #33222 will kill that function, and we need to remove that workaround once #33222 is landed.", "labels": ["open source", "triaged"], "number_of_comments": 3, "created_at": "2020-02-12 18:31:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564158524": {"author_username": "neginraoof", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33242, "title": "[ONNX] Reduce ONNX test time on CI", "body": "Among all ONNX tests, ONNXRuntime tests are taking the most time on CI (almost 60%).\r\nThis is because we are testing larger models (mainly torchvision RCNNs) for multiple onnx opsets.\r\nI decided to divide tests between two jobs for older/newer opsets. This is now reducing the test time from 2h to around 1h10mins.", "labels": ["jit", "open source", "triaged"], "number_of_comments": 4, "created_at": "2020-02-12 17:38:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564140402": {"author_username": "pyscorcher", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33241, "title": "[feature request] Add \"groups\" argument to nn.Fold and nn.Unfold", "body": "## \ud83d\ude80 Feature\r\nAdd a `groups` argument to the `Fold` and `Unfold` modules (and the corresponding functions in `functional`)\r\n\r\n## Motivation\r\n\r\nCurrently it is a little bit cumbersome to implement e.g. a custom `AvgPool2d` or `MaxPool2d` filter using `Fold` and `Unfold`, as the channel information is not retained. For an input of shape `[B, C, H, W]` `Fold` will output an array of `[B, #pixels_per_window, #windows]` and it is cumbersome to divide that up into the channels again in this stage.\r\n\r\n## Pitch\r\n\r\nAdding a `group` argument (just like in the `Convxd` modules) would simplify that (by choosing `groups=C`), and provide a natural extension of these modules. I'd suggest for `Fold` to output a shape of `[B, groups, #pixels_per_window, #windows]` instead. \r\n\r\n## Alternatives\r\n\r\nI don't think there are any really elegant alternatives. `Fold` and `Unfold` are provide to implement custom filtering and pooling functionality.\r\n\r\n## Additional context\r\n\r\nnone\r\n", "labels": ["feature", "module: nn", "triaged"], "number_of_comments": 0, "created_at": "2020-02-12 17:06:39", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564025946": {"author_username": "ousou", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33238, "title": "GRU model learns very slowly when using DataParallel with multiple GPUs", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using DataParallel on a GRU model with multiple GPUs the model seems to learn very slowly during training, compared to when running on a single GPU. The issue is present in PyTorch 1.4.0 but not PyTorch 1.3.0.\r\n\r\n## To Reproduce\r\n\r\nRun the following script on a multi-GPU machine (slightly modified from [Pytorch RNN tutorial](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/recurrent_neural_network/main.py))\r\n\r\n```import torch \r\nimport torch.nn as nn\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\nimport time\r\n\r\nprint('Torch version', torch.__version__)\r\n\r\n# Device configuration\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n\r\n# Hyper-parameters\r\nsequence_length = 28\r\ninput_size = 28\r\nhidden_size = 128\r\nnum_layers = 2\r\nnum_classes = 10\r\nbatch_size = 100\r\nnum_epochs = 15\r\nlearning_rate = 0.001\r\nseed = 123\r\ntorch.manual_seed(seed)\r\ntorch.cuda.manual_seed(seed)\r\n\r\n# MNIST dataset\r\ntrain_dataset = torchvision.datasets.MNIST(root='data/',\r\n                                           train=True, \r\n                                           transform=transforms.ToTensor(),\r\n                                           download=True)\r\n\r\ntest_dataset = torchvision.datasets.MNIST(root='data/',\r\n                                          train=False, \r\n                                          transform=transforms.ToTensor())\r\n\r\n# Data loader\r\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\r\n                                           batch_size=batch_size, \r\n                                           shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\r\n                                          batch_size=batch_size, \r\n                                          shuffle=False)\r\n\r\n# Recurrent neural network (many-to-one)\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\r\n        super(RNN, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.num_layers = num_layers\r\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n    \r\n    def forward(self, x):\r\n        self.gru.flatten_parameters()\r\n        # Set initial hidden and cell states \r\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \r\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\r\n        \r\n        # Forward propagate LSTM\r\n        out, _ = self.gru(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\r\n        \r\n        # Decode the hidden state of the last time step\r\n        out = self.fc(out[:, -1, :])\r\n        return out\r\n\r\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\r\n\r\nmodel = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())), output_device=0)\r\n\r\n\r\n# Loss and optimizer\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\nprint ('Training starts')\r\n# Train the model\r\ntotal_step = len(train_loader)\r\nfor epoch in range(num_epochs):\r\n    model.train()\r\n    epoch_start_time = time.time()\r\n    total_loss = 0\r\n    for i, (images, labels) in enumerate(train_loader):\r\n        images = images.reshape(-1, sequence_length, input_size).to(device)\r\n        labels = labels.to(device)\r\n        \r\n        # Forward pass\r\n        outputs = model(images)\r\n        loss = criterion(outputs, labels)\r\n        \r\n        # Backward and optimize\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        if (i+1) % 100 == 0:\r\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \r\n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n        total_loss += loss.item()\r\n    epoch_duration = time.time() - epoch_start_time\r\n    epoch_loss = total_loss / i\r\n    print ('Epoch [{}/{}], Duration {:.4f} s, Epoch average loss {:.4f}' \r\n           .format(epoch+1, num_epochs, epoch_duration, epoch_loss))\r\n\r\n    if (epoch+1) % 5 == 0:\r\n        model.eval()\r\n        with torch.no_grad():\r\n            correct = 0\r\n            total = 0\r\n            for images, labels in test_loader:\r\n                images = images.reshape(-1, sequence_length, input_size).to(device)\r\n                labels = labels.to(device)\r\n                outputs = model(images)\r\n                _, predicted = torch.max(outputs.data, 1)\r\n                total += labels.size(0)\r\n                correct += (predicted == labels).sum().item()\r\n\r\n            print('Epoch [{}/{}], Test Accuracy of the model on the 10000 test images: {} %'.format(epoch+1,num_epochs, 100 * correct / total)) \r\n\r\n# Test the model\r\nmodel.eval()\r\nwith torch.no_grad():\r\n    correct = 0\r\n    total = 0\r\n    for images, labels in test_loader:\r\n        images = images.reshape(-1, sequence_length, input_size).to(device)\r\n        labels = labels.to(device)\r\n        outputs = model(images)\r\n        _, predicted = torch.max(outputs.data, 1)\r\n        total += labels.size(0)\r\n        correct += (predicted == labels).sum().item()\r\n\r\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \r\n```\r\n\r\n### Output\r\n\r\nTorch version 1.4.0\r\nTraining starts\r\nEpoch [1/15], Step [100/600], Loss: 2.2840\r\nEpoch [1/15], Step [200/600], Loss: 2.2771\r\nEpoch [1/15], Step [300/600], Loss: 2.2441\r\nEpoch [1/15], Step [400/600], Loss: 2.2385\r\nEpoch [1/15], Step [500/600], Loss: 2.2250\r\nEpoch [1/15], Step [600/600], Loss: 2.2158\r\nEpoch [1/15], Duration 23.8333 s, Epoch average loss 2.2584\r\nEpoch [2/15], Step [100/600], Loss: 2.1954\r\nEpoch [2/15], Step [200/600], Loss: 2.1744\r\nEpoch [2/15], Step [300/600], Loss: 2.1570\r\nEpoch [2/15], Step [400/600], Loss: 2.1537\r\nEpoch [2/15], Step [500/600], Loss: 2.1515\r\nEpoch [2/15], Step [600/600], Loss: 2.1356\r\nEpoch [2/15], Duration 9.2600 s, Epoch average loss 2.1780\r\nEpoch [3/15], Step [100/600], Loss: 2.1289\r\nEpoch [3/15], Step [200/600], Loss: 2.1365\r\nEpoch [3/15], Step [300/600], Loss: 2.1135\r\nEpoch [3/15], Step [400/600], Loss: 2.1153\r\nEpoch [3/15], Step [500/600], Loss: 2.0827\r\nEpoch [3/15], Step [600/600], Loss: 2.0771\r\nEpoch [3/15], Duration 9.2692 s, Epoch average loss 2.1162\r\nEpoch [4/15], Step [100/600], Loss: 2.1000\r\nEpoch [4/15], Step [200/600], Loss: 2.1413\r\nEpoch [4/15], Step [300/600], Loss: 2.0644\r\nEpoch [4/15], Step [400/600], Loss: 2.0573\r\nEpoch [4/15], Step [500/600], Loss: 2.0968\r\nEpoch [4/15], Step [600/600], Loss: 2.0494\r\nEpoch [4/15], Duration 9.2563 s, Epoch average loss 2.0668\r\nEpoch [5/15], Step [100/600], Loss: 2.0678\r\nEpoch [5/15], Step [200/600], Loss: 2.0399\r\nEpoch [5/15], Step [300/600], Loss: 2.0628\r\nEpoch [5/15], Step [400/600], Loss: 1.9648\r\nEpoch [5/15], Step [500/600], Loss: 1.9510\r\nEpoch [5/15], Step [600/600], Loss: 1.9990\r\nEpoch [5/15], Duration 9.2674 s, Epoch average loss 2.0261\r\nEpoch [5/15], Test Accuracy of the model on the 10000 test images: 37.12 %\r\n\r\n\r\n## Expected behavior\r\n\r\nWhen running the same script using PyTorch 1.3.0 and torchvision 0.4.1 the model learns normally:\r\n\r\nTorch version 1.3.0\r\nTraining starts\r\nEpoch [1/15], Step [100/600], Loss: 0.8091\r\nEpoch [1/15], Step [200/600], Loss: 0.3172\r\nEpoch [1/15], Step [300/600], Loss: 0.3350\r\nEpoch [1/15], Step [400/600], Loss: 0.2331\r\nEpoch [1/15], Step [500/600], Loss: 0.1132\r\nEpoch [1/15], Step [600/600], Loss: 0.3318\r\nEpoch [1/15], Duration 27.2189 s, Epoch average loss 0.4798\r\nEpoch [2/15], Step [100/600], Loss: 0.1276\r\nEpoch [2/15], Step [200/600], Loss: 0.0696\r\nEpoch [2/15], Step [300/600], Loss: 0.1202\r\nEpoch [2/15], Step [400/600], Loss: 0.0390\r\nEpoch [2/15], Step [500/600], Loss: 0.0975\r\nEpoch [2/15], Step [600/600], Loss: 0.0764\r\nEpoch [2/15], Duration 9.0211 s, Epoch average loss 0.1134\r\nEpoch [3/15], Step [100/600], Loss: 0.0369\r\nEpoch [3/15], Step [200/600], Loss: 0.0832\r\nEpoch [3/15], Step [300/600], Loss: 0.0255\r\nEpoch [3/15], Step [400/600], Loss: 0.1506\r\nEpoch [3/15], Step [500/600], Loss: 0.2035\r\nEpoch [3/15], Step [600/600], Loss: 0.0542\r\nEpoch [3/15], Duration 9.0659 s, Epoch average loss 0.0693\r\nEpoch [4/15], Step [100/600], Loss: 0.0173\r\nEpoch [4/15], Step [200/600], Loss: 0.0687\r\nEpoch [4/15], Step [300/600], Loss: 0.0878\r\nEpoch [4/15], Step [400/600], Loss: 0.0255\r\nEpoch [4/15], Step [500/600], Loss: 0.0944\r\nEpoch [4/15], Step [600/600], Loss: 0.0198\r\nEpoch [4/15], Duration 9.0609 s, Epoch average loss 0.0523\r\nEpoch [5/15], Step [100/600], Loss: 0.0432\r\nEpoch [5/15], Step [200/600], Loss: 0.1001\r\nEpoch [5/15], Step [300/600], Loss: 0.0589\r\nEpoch [5/15], Step [400/600], Loss: 0.1240\r\nEpoch [5/15], Step [500/600], Loss: 0.0341\r\nEpoch [5/15], Step [600/600], Loss: 0.0303\r\nEpoch [5/15], Duration 9.0712 s, Epoch average loss 0.0408\r\nEpoch [5/15], Test Accuracy of the model on the 10000 test images: 98.61 %\r\n\r\n\r\nAlso when using PyTorch 1.4.0 with just one GPU (without DataParallel) the model learns as it should (results are the same as above). With PyTorch 1.3.0 it doesn't matter whether one GPU (without DataParallel) or multiple GPUs (with DataParallel) is used - the results are the same.\r\n\r\n## Environment\r\n\r\nThe tests were run on an AWS g4dn.12xlarge instance.\r\n\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.13.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: \r\nGPU 0: Tesla T4\r\nGPU 1: Tesla T4\r\nGPU 2: Tesla T4\r\nGPU 3: Tesla T4\r\n\r\nNvidia driver version: 418.87.00\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n```\r\n\r\n## Additional context\r\n\r\nThe problem does not seem to be related to torchvision even though the example uses it. We've noticed similar issues in our actual models that use GRUs but do not use torchvision at all.\r\n\r\nPossible related issue: #33081 ", "labels": ["module: data parallel", "triaged"], "number_of_comments": 2, "created_at": "2020-02-12 14:11:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "564018317": {"author_username": "Baranowski", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33237, "title": "Migrate _cat from TH to ATen (CUDA)", "body": "Fixes #24520\r\n\r\nBenchmarks:\r\n\r\nUpstream:\r\n\r\n```\r\n$ python -m pt.cat_test --tag_filter all --device cuda  --omp_num_threads 1 --mkl_num_threads 1\r\n# ----------------------------------------\r\n# PyTorch/Caffe2 Operator Micro-benchmarks\r\n# ----------------------------------------\r\n# Tag : all\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\r\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\r\nForward Execution Time (us) : 17.355\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\r\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 30.718\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\r\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 17.329\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\r\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 30.176\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\r\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\r\nForward Execution Time (us) : 74.417\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\r\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 75.728\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\r\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\r\nForward Execution Time (us) : 190.165\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa8876fcf28>,111,65]_N5_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa8876fcf28>, 111, 65], N: 5, dim: 0, device: cuda\r\nForward Execution Time (us) : 57.711\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[96,<function<lambda>at0x7fa886237048>,64]_N5_dim1_cuda\r\n# Input: sizes: [96, <function <lambda> at 0x7fa886237048>, 64], N: 5, dim: 1, device: cuda\r\nForward Execution Time (us) : 49.903\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[128,64,<function<lambda>at0x7fa7b57bb840>]_N5_dim2_cuda\r\n# Input: sizes: [128, 64, <function <lambda> at 0x7fa7b57bb840>], N: 5, dim: 2, device: cuda\r\nForward Execution Time (us) : 84.181\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bba60>,32,64]_N50_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa7b57bba60>, 32, 64], N: 50, dim: 0, device: cuda\r\nForward Execution Time (us) : 82.339\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[32,<function<lambda>at0x7fa7b57bbae8>,64]_N50_dim1_cuda\r\n# Input: sizes: [32, <function <lambda> at 0x7fa7b57bbae8>, 64], N: 50, dim: 1, device: cuda\r\nForward Execution Time (us) : 82.312\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[33,65,<function<lambda>at0x7fa7b57bbb70>]_N50_dim2_cuda\r\n# Input: sizes: [33, 65, <function <lambda> at 0x7fa7b57bbb70>], N: 50, dim: 2, device: cuda\r\nForward Execution Time (us) : 90.715\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\r\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\r\nForward Execution Time (us) : 129.021\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\r\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\r\nForward Execution Time (us) : 142.966\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\r\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\r\nForward Execution Time (us) : 387.023\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbbf8>]_N100_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbbf8>], N: 100, dim: 0, device: cuda\r\nForward Execution Time (us) : 36.647\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbc80>]_N1000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbc80>], N: 1000, dim: 0, device: cuda\r\nForward Execution Time (us) : 278.890\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbd08>]_N2000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbd08>], N: 2000, dim: 0, device: cuda\r\nForward Execution Time (us) : 557.752\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7fa7b57bbd90>]_N3000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7fa7b57bbd90>], N: 3000, dim: 0, device: cuda\r\nForward Execution Time (us) : 842.512\r\n\r\n```\r\n\r\nNew version:\r\n\r\n```\r\n$ python -m pt.cat_test --tag_filter all --device cuda  --omp_num_threads 1 --mkl_num_threads 1\r\n# ----------------------------------------\r\n# PyTorch/Caffe2 Operator Micro-benchmarks\r\n# ----------------------------------------\r\n# Tag : all\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1,1,1)_N2_dim0_cuda\r\n# Input: sizes: (1, 1, 1), N: 2, dim: 0, device: cuda\r\nForward Execution Time (us) : 24.419\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\r\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 25.025\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(128,1024,2)_N2_dim1_cuda\r\n# Input: sizes: (128, 1024, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 24.247\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(512,512,2)_N2_dim1_cuda\r\n# Input: sizes: (512, 512, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 25.098\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1024,1024,2)_N2_dim0_cuda\r\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 0, device: cuda\r\nForward Execution Time (us) : 74.441\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1025,1023,2)_N2_dim1_cuda\r\n# Input: sizes: (1025, 1023, 2), N: 2, dim: 1, device: cuda\r\nForward Execution Time (us) : 74.866\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(1024,1024,2)_N2_dim2_cuda\r\n# Input: sizes: (1024, 1024, 2), N: 2, dim: 2, device: cuda\r\nForward Execution Time (us) : 189.280\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1c9b056048>,111,65]_N5_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1c9b056048>, 111, 65], N: 5, dim: 0, device: cuda\r\nForward Execution Time (us) : 57.629\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[96,<function<lambda>at0x7f1c9b0560d0>,64]_N5_dim1_cuda\r\n# Input: sizes: [96, <function <lambda> at 0x7f1c9b0560d0>, 64], N: 5, dim: 1, device: cuda\r\nForward Execution Time (us) : 49.975\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[128,64,<function<lambda>at0x7f1bce8f38c8>]_N5_dim2_cuda\r\n# Input: sizes: [128, 64, <function <lambda> at 0x7f1bce8f38c8>], N: 5, dim: 2, device: cuda\r\nForward Execution Time (us) : 83.643\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3ae8>,32,64]_N50_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3ae8>, 32, 64], N: 50, dim: 0, device: cuda\r\nForward Execution Time (us) : 82.307\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[32,<function<lambda>at0x7f1bce8f3b70>,64]_N50_dim1_cuda\r\n# Input: sizes: [32, <function <lambda> at 0x7f1bce8f3b70>, 64], N: 50, dim: 1, device: cuda\r\nForward Execution Time (us) : 82.323\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[33,65,<function<lambda>at0x7f1bce8f3bf8>]_N50_dim2_cuda\r\n# Input: sizes: [33, 65, <function <lambda> at 0x7f1bce8f3bf8>], N: 50, dim: 2, device: cuda\r\nForward Execution Time (us) : 90.549\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(64,32,4,16,32)_N2_dim2_cuda\r\n# Input: sizes: (64, 32, 4, 16, 32), N: 2, dim: 2, device: cuda\r\nForward Execution Time (us) : 129.022\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(16,32,4,16,32)_N8_dim2_cuda\r\n# Input: sizes: (16, 32, 4, 16, 32), N: 8, dim: 2, device: cuda\r\nForward Execution Time (us) : 142.969\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes(9,31,5,15,33)_N17_dim4_cuda\r\n# Input: sizes: (9, 31, 5, 15, 33), N: 17, dim: 4, device: cuda\r\nForward Execution Time (us) : 386.973\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3c80>]_N100_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3c80>], N: 100, dim: 0, device: cuda\r\nForward Execution Time (us) : 43.800\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3d08>]_N1000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3d08>], N: 1000, dim: 0, device: cuda\r\nForward Execution Time (us) : 279.023\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3d90>]_N2000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3d90>], N: 2000, dim: 0, device: cuda\r\nForward Execution Time (us) : 565.790\r\n\r\n# Benchmarking PyTorch: cat\r\n# Mode: Eager\r\n# Name: cat_sizes[<function<lambda>at0x7f1bce8f3e18>]_N3000_dim0_cuda\r\n# Input: sizes: [<function <lambda> at 0x7f1bce8f3e18>], N: 3000, dim: 0, device: cuda\r\nForward Execution Time (us) : 845.153\r\n```", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 2, "created_at": "2020-02-12 13:58:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563918801": {"author_username": "kli-crise", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33235, "title": "Small typo in the description of Pathwise derivative ", "body": "", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-12 10:57:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563773783": {"author_username": "alsrgv", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33229, "title": "MultiStepLR is broken with .step(epoch=xyz)", "body": "## \ud83d\udc1b Bug\r\n\r\nMultiStepLR is broken with `.step(epoch=xyz)`:\r\n\r\n```\r\nimport torch\r\nt = torch.tensor(1.0, requires_grad=True)\r\nopt = torch.optim.SGD([t], lr=0.01)\r\ns = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[19])\r\nprint('lr', opt.param_groups[0]['lr'])\r\n\r\nopt.step()\r\ns.step(0)\r\nprint('lr', opt.param_groups[0]['lr'])\r\n```\r\n\r\nOutputs:\r\n```\r\nlr 0.01\r\nlr 0.001\r\n```\r\n\r\nI'm aware that `.step(epoch=xyz)` is deprecated, but it's used by third-party libraries, like https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L341.\r\n\r\ncc @vincentqb @williamFalcon ^\r\n\r\nI've debugged the issue and it seems to be caused by `bisect_right()` not playing well with `Counter`: https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py#L399\r\n\r\nThis can be fixed by casting self.milestones to list before bisection.  If that's an acceptable fix, I'm happy to send a PR.\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 4.8.5-4ubuntu8) 4.8.5\r\nCMake version: version 3.12.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration:\r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.2\r\n[pip3] pytorch-lightning==0.5.3.2\r\n[pip3] torch==1.4.0\r\n[pip3] torch2trt==0.0.3\r\n[pip3] torchvision==0.6.0a0\r\n[conda] Could not collect\r\n```\r\n", "labels": ["module: optimizer", "triaged"], "number_of_comments": 1, "created_at": "2020-02-12 06:01:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563729629": {"author_username": "yf225", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33227, "title": "Python/C++ API parity master tracking task", "body": "Achieving API parity between our Python and C++ frontend is crucial for broadening the number of language communities PyTorch can address. By the PyTorch 1.5 release, we will provide the following improvements to the PyTorch C++ frontend:\r\n\r\n**Python/C++ API Parity:**\r\n- [ ] torch.nn modules and functional (tracking issue: https://github.com/pytorch/pytorch/issues/25883). Items that will be added in 1.5:\r\n  - [ ] RNN\r\n  - [ ] LSTM\r\n  - [ ] GRU\r\n  - [ ] RNNCell\r\n  - [ ] LSTMCell\r\n  - [ ] GRUCell\r\n- [ ] torch.optim optimizers (tracking issue: https://github.com/pytorch/pytorch/issues/28440)\r\n- [ ] tensor multi-dim indexing API (PR: https://github.com/pytorch/pytorch/pull/32841 and others)\r\n- [x] C++ tensor autograd API (tracking issue: https://github.com/pytorch/pytorch/issues/25874). Remaining items:\r\n  - [x] grad_fn (https://github.com/pytorch/pytorch/pull/28287)\r\n  - [x] register_hook (https://github.com/pytorch/pytorch/pull/28287)\r\n  - [x] retain_grad (https://github.com/pytorch/pytorch/pull/33349)\r\n  - [x] _base (https://github.com/pytorch/pytorch/pull/33316)\r\n\r\n**Bug fixes:**\r\n- [x] Allow skipping default arguments in module's forward method when module is used in Sequential (PR: https://github.com/pytorch/pytorch/pull/33027)\r\n- [ ] ModuleList compile error: error: 'begin' was not declared in this scope (Issue: https://github.com/pytorch/pytorch/issues/32414)\r\n- [x] C++ nn::FractionalMaxPool2d/3d output_ratio option is integer, should be double (Issue: https://github.com/pytorch/pytorch/issues/33240, PR: https://github.com/pytorch/pytorch/pull/33304)\r\n\r\n**Docs:**\r\n- [ ] For each `torch::nn` layer / functional, document how to use `*Options` to specify options\r\n- [ ] Fix torch::Tensor doc generation (Issue: https://github.com/pytorch/pytorch/issues/25845)\r\n\r\n**Tutorials:**\r\n- [ ] How to use C++ tensor multi-dim indexing\r\n- [ ] Autograd in C++\r\n  - [ ] How to use C++ custom autograd function\r\n  - [ ] How to use `at::Tensor::register_hook`\r\n  - [ ] How to compute higher-order gradients in C++ (e.g. issue: https://github.com/pytorch/pytorch/issues/18173)\r\n\r\n\r\n\r\n\r\ncc @yf225", "labels": ["module: cpp", "triaged"], "number_of_comments": 0, "created_at": "2020-02-12 03:39:46", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563708563": {"author_username": "RylanSchaeffer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33226, "title": "SummaryWriter.add_video() creates videos that don't match input tensors", "body": "## \ud83d\udc1b Bug\r\nI\u2019m trying to create a video by generating a sequence of 500 matplotlib plots, converting each to a numpy array, stacking them and then passing them to a `SummaryWriter()`'s `add_video()`. When I do this, the colorbar is converted from colored to black & white, and only a small number (~3-4) of the matplotlib plots comprise the video. I confirmed that my numpy arrays are correct by using them to recreate the matplotlib figure.\r\n\r\n## To Reproduce\r\n\r\nSee the comment below for a minimal working example.\r\n\r\n## Expected behavior\r\n\r\nThe video should loop over each frame in the sequence (length 500). Instead, only a few frames are displayed.\r\n\r\n## Environment\r\n\r\n\r\n - PyTorch Version (e.g., 1.0): 1.3.1\r\n - OS (e.g., Linux): Ubuntu 18.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Python version: 3.6.9\r\n - CUDA/cuDNN version: NA\r\n", "labels": ["module: numpy", "triaged"], "number_of_comments": 9, "created_at": "2020-02-12 02:59:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563679226": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33225, "title": "[profiler] remove redundant assert in record_function_ops", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33225 [profiler] remove redundant assert in record_function_ops**\n\nThis removes a redundant assert statement in `record_function_ops`. In\nthe else branch in question, we are guaranteed to have `current == &rec`, so\nthis assert will never fire.\n\nAlthough, maybe we should add an assert failure when `current == &rec` since it\nseems that `current` should always be profiler::record_function_exit.\n\nDifferential Revision: [D19849145](https://our.internmc.facebook.com/intern/diff/D19849145/)", "labels": [], "number_of_comments": 2, "created_at": "2020-02-12 01:38:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563677953": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33224, "title": "[wip][jit] Enum implementation", "body": "This adds enum as a sugared value, but we probably want to just make it first class", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-12 01:34:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563668849": {"author_username": "yns88", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33221, "title": "[pytorch_ci] Python target determinator", "body": "Summary:\nThis will make it so that if a pull request is just pure Python files, then we'll only run the Python tests that are connected to the dependency graph of the touched files.\n\nAssumptions made:\n- the Python code does not do dynamic imports\n- test_X.py never imports from test_Y.py\n\nRight now this is only done for test_nn (presumably the largest test entrypoint), but it's not much more work to do it for all the other test entrypoints too.\n\nTest Plan:\nTODO:\n\nExport to a PR, add commits to the PR that touch various files, check the CircleCI results.\n\nDifferential Revision: D19847340\n\n", "labels": ["fb-exported"], "number_of_comments": 25, "created_at": "2020-02-12 01:14:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563667023": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33220, "title": "[TensorExpr] Add IR Printer.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33220 [TensorExpr] Add IR Printer.**\n* #33219 [TensorExpr] Add IR visitor, IR mutator, and IR evaluator.\n* #33218 [TensorExpr] Add core classes for representing expressions and statements.\n* #33217 [TensorExpr] Add a class for representing data type.\n* #33216 [TensorExpr] Add classes for memory management in tensor expressions.\n\nDifferential Revision: [D19848379](https://our.internmc.facebook.com/intern/diff/D19848379)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-12 01:11:04", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 1, "hooray": 0}}, "563666961": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33219, "title": "[TensorExpr] Add IR visitor, IR mutator, and IR evaluator.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33220 [TensorExpr] Add IR Printer.\n* **#33219 [TensorExpr] Add IR visitor, IR mutator, and IR evaluator.**\n* #33218 [TensorExpr] Add core classes for representing expressions and statements.\n* #33217 [TensorExpr] Add a class for representing data type.\n* #33216 [TensorExpr] Add classes for memory management in tensor expressions.\n\nDifferential Revision: [D19848381](https://our.internmc.facebook.com/intern/diff/D19848381)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-12 01:10:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563666883": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33218, "title": "[TensorExpr] Add core classes for representing expressions and statements.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33220 [TensorExpr] Add IR Printer.\n* #33219 [TensorExpr] Add IR visitor, IR mutator, and IR evaluator.\n* **#33218 [TensorExpr] Add core classes for representing expressions and statements.**\n* #33217 [TensorExpr] Add a class for representing data type.\n* #33216 [TensorExpr] Add classes for memory management in tensor expressions.\n\nDifferential Revision: [D19848378](https://our.internmc.facebook.com/intern/diff/D19848378)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-12 01:10:49", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 1, "hooray": 0}}, "563666813": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33217, "title": "[TensorExpr] Add a class for representing data type.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33220 [TensorExpr] Add IR Printer.\n* #33219 [TensorExpr] Add IR visitor, IR mutator, and IR evaluator.\n* #33218 [TensorExpr] Add core classes for representing expressions and statements.\n* **#33217 [TensorExpr] Add a class for representing data type.**\n* #33216 [TensorExpr] Add classes for memory management in tensor expressions.\n\nDifferential Revision: [D19848380](https://our.internmc.facebook.com/intern/diff/D19848380)", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-02-12 01:10:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563666741": {"author_username": "ZolotukhinM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33216, "title": "[TensorExpr] Add classes for memory management in tensor expressions.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33220 [TensorExpr] Add IR Printer.\n* #33219 [TensorExpr] Add IR visitor, IR mutator, and IR evaluator.\n* #33218 [TensorExpr] Add core classes for representing expressions and statements.\n* #33217 [TensorExpr] Add a class for representing data type.\n* **#33216 [TensorExpr] Add classes for memory management in tensor expressions.**\n\nAll tensor expressions belong to a kernel arena and are freed when the\narena is destroyed. Until it is destroyed, all expressions stay valid.\n\nDifferential Revision: [D19848382](https://our.internmc.facebook.com/intern/diff/D19848382)", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-12 01:10:36", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 1, "hooray": 0}}, "563649321": {"author_username": "LiboShen", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33215, "title": "nn.LSTM output shape is mis-documented", "body": "## \ud83d\udcda Documentation\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\r\nhttps://pytorch.org/docs/stable/nn.html#lstm\r\n\r\n> output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\r\n\r\nThe shape should actually be `(batch, seq_len, num_directions * hidden_size)`", "labels": ["module: docs", "triaged"], "number_of_comments": 2, "created_at": "2020-02-12 00:40:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563634389": {"author_username": "pritamdamania87", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33214, "title": "Unify gradient accumulation between distributed autograd and local autograd\nengine.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33427 Avoid clone for sparse tensors during accumulation of grads.\n* **#33214 Unify gradient accumulation between distributed autograd and local autograd\nengine.**\nengine.**\nengine.**\nengine.**\nengine.**\nengine.**\n\nengine.\n\nDistributed autograd had some custom logic in terms of how we\naccumulated gradients. This was mostly done early on to enable basic\nfunctionality. Although, in the long term we should merge this logic with what\nwe have in the local autograd engine. A lot of work has gone into ensuring we\naccumulate grads correctly and efficiently and we should reuse that as a\nstarting point.\n\nWe can investigate if we need further custom logic for distributed autograd\nlater on if we need additional optimizations.\n\nIn this PR I've merged the gradient accumulation logic and also the gradient\nhooks. As a result, now gradient hooks are called in distributed autograd as\nwell.\n\nDifferential Revision: [D19843284](https://our.internmc.facebook.com/intern/diff/D19843284/)\n\nDifferential Revision: [D19843284](https://our.internmc.facebook.com/intern/diff/D19843284)", "labels": [], "number_of_comments": 3, "created_at": "2020-02-12 00:15:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563605498": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33210, "title": "Add get_type() API to RRef", "body": "## \ud83d\ude80 Feature: Add get_type() API to RRef\r\n\r\nIt would be useful to add a `get_type()` function on `RRef` that returns the type of the object that the RRef is holding. This would be pretty useful for user debugging purposes, such as validating that a certain RRef holds an `nn.Module` that is created remotely and the like.\r\n\r\nIt would also be useful to look at this type information if we add support to gather all parameters that exist on remote workers, see https://github.com/pytorch/pytorch/issues/33115.\r\n\r\nThe API should look roughly as follows:\r\n\r\n```\r\n# on A and B\r\ndef create_tensor():\r\n    return torch.ones(1)\r\n\r\n# on B\r\nrref = rpc.remote(A, create_tensor, args=())\r\nrref_type = rref.get_type()\r\n>>> rref_type\r\n<class 'torch.Tensor'>\r\n```\r\n\r\nAfter speaking to @mrshenli offline, one way we could go about implementing this is wrapping the call and running it on the remote worker. We should be able to do this by sending an RPC with a new request type to the owning node, which returns and deserializes the type. We would then need to convert it to a Python type . Efficiency wise, this is not ideal since it requires an RPC round trip, so open to other suggestions as well.\r\n\r\nCode pointers/work items:\r\n1) Expose python `get_type` on RRef by adding it to the pybind layer: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/init.cpp#L112\r\n\r\n2) Python RRef:https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/py_rref.cpp\r\n\r\n3) RRef C++ API (PyRRef from above wraps around this): https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/rref_impl.cpp\r\n\r\nThere are many instances of `TypePtr rrefType` (see py_rref: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/py_rref.cpp) , we can use this attribute and then convert to a python type to get the type.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "triaged"], "number_of_comments": 4, "created_at": "2020-02-11 23:31:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563580092": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33208, "title": "rpc.shutdown() does not abort pending sends created in separate threads.", "body": "## \ud83d\udc1b Bug\r\n\r\nIf there was a recent call to `agent.send()` by some RPC thread and then we shutdown RPC with `rpc.shutdown()`, the shutdown process could hang because we could still be waiting on Gloo to wait for sends to be completed in `pendingSends->wait()` in`handleSend`: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/process_group_agent.cpp#L384\r\n\r\nA similar issue would have occured by calling into `recvWork->wait()` in `ProcessGroupAgent::listenLoop()`, however we abort these calls when shutting down: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/process_group_agent.cpp#L256\r\n\r\nWe have similar hooks in Gloo to abort waits on send as well, so we should abort these during shutdown also.\r\n\r\nThis issue was reported by @pritamdamania87 \r\n\n\ncc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["high priority", "module: rpc", "triage review"], "number_of_comments": 0, "created_at": "2020-02-11 23:07:53", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563564740": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33205, "title": "Bring up new-style registration API as wrapper around old-style", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33205 Bring up new-style registration API as wrapper around old-style**\n* #33097 Stop generating out full function type for registration, use decltype or infer it\n* #33093 Delete unnecessary aliasAnalysis specification from operator registrations.\n\r\nA number of important use-cases are implemented:\r\n\r\n- def(schema): defines a schema, with no implementation (alias\r\n  inferred from schema, by default)\r\n- def(schema, fn_ptr): registers fn_ptr as a catch-all kernel\r\n  for the operation\r\n- def(schema, lambda): registers lambda as a catch-all kernel\r\n  for the operation\r\n- def(schema, torch::dispatch(dispatch_key, fn)), and\r\n  def(schema, torch::dispatch(device_type, fn)): registers\r\n  the function to only be executed when dispatch_key/device_type\r\n  is selected for use\r\n- def(schema, CppFunction::makeUnboxedOnly(fn)): registers the function\r\n  as unboxed only, using the inline syntax\r\n- impl(...) variant of def, which will eventually not require full schema string\r\n\r\nAll of our code generated registrations in ATen are switched to\r\nthe new API.\r\n\r\nSome aspects of the API which are not fully implemented:\r\n\r\n- It's still not valid to omit the schema when registering a function\r\n  pointer, due to #32549\r\n- Although it's possible to take advantage of top-level namespaces\r\n  ala torch::import(\"aten\"), we don't use it because this results\r\n  in worse code (as we have to cat everything back together).  This\r\n  is not an essential problem, we just need the internals to be less\r\n  stupid.\r\n\r\nThere are some aspects of the API which don't semantically make sense,\r\nbut I chose not to fix them in this PR:\r\n\r\n- There's no reason to optional<DispatchKey>, DispatchKey would\r\n  work just fine (use DispatchKey::Undefined for the nullopt case)\r\n\r\nIn the long term, we should swap the wrapper around: the new-style\r\nAPI has the real implementation, and the old-style API is backwards\r\ncompatibility.  However, this implies a lot of internal refactoring,\r\nso I decided to short circuit around it to get this in faster\r\n\r\nAncillary changes:\r\n- I stopped moving optional<DispatchKey>, it's literally just two\r\n  words, pass it by value please.\r\n- Needed to add a & qualified version of RegisterOps::op, since\r\n  I'm storing RegisterOps as a member inside the new style\r\n  Namespace and I cannot conveniently get a rvalue reference\r\n  to it in that situation.  (BTW, register_ = std::move(register_)\r\n  really doesn't work, don't try it!)\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\r\n\r\nDifferential Revision: [D19856626](https://our.internmc.facebook.com/intern/diff/D19856626)", "labels": [], "number_of_comments": 7, "created_at": "2020-02-11 22:54:05", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563551005": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33204, "title": "[jit] Expose testing utilities", "body": "We've had a few asks for this internally and it seems reasonable, at least for `checkModule`. `checkScript` is a different story and probably needs a wrapper that doesn't run functions through the string frontend since users probably don't care about that working.\r\n\r\nPreview: https://driazati.github.io/pytorch_doc_previews/33204/jit.html#testing", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-11 22:41:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563544323": {"author_username": "ebonetti", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33202, "title": "RuntimeError in pack_sequence", "body": "## \ud83d\udc1b Bug\r\n\r\nSetting PyTorch default tensor type to cuda.FloatTensor on a Colab GPU instance leads pack_sequence to a Runtime error.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `torch.set_default_tensor_type(torch.cuda.FloatTensor)`\r\n2. `torch.nn.utils.rnn.pack_sequence([torch.tensor([1,2,3])])`\r\n\r\nThe second statement will throw the following:\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-7-054f3bedf067> in <module>()\r\n      1 torch.set_default_tensor_type(torch.cuda.FloatTensor)\r\n----> 2 torch.nn.utils.rnn.pack_sequence([torch.tensor([1,2,3])])\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py in pack_sequence(sequences, enforce_sorted)\r\n    377     \"\"\"\r\n    378     lengths = [v.size(0) for v in sequences]\r\n--> 379     return pack_padded_sequence(pad_sequence(sequences), lengths, enforce_sorted=enforce_sorted)\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/utils/rnn.py in pack_padded_sequence(input, lengths, batch_first, enforce_sorted)\r\n    231 \r\n    232     data, batch_sizes = \\\r\n--> 233         _VF._pack_padded_sequence(input, lengths, batch_first)\r\n    234     return PackedSequence(data, batch_sizes, sorted_indices, None)\r\n    235 \r\n\r\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor\r\n```\r\n\r\n## Expected behavior\r\n\r\nReturn the packed sequence without fail.\r\n\r\n## Environment\r\nA Colab GPU instance.\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.5\r\n[pip3] torch==1.4.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n## Additional context\r\n", "labels": ["module: nn", "triaged"], "number_of_comments": 0, "created_at": "2020-02-11 22:35:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563542811": {"author_username": "xuhdev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33201, "title": "Ensure that lambda is no less than zero in softshrink", "body": "Softshrink is ill-defined when `lambda < 0`.", "labels": ["merge-this-please", "open source"], "number_of_comments": 6, "created_at": "2020-02-11 22:33:59", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563513218": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33199, "title": "[JIT] remove list appends", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33297 Allow mutated values as inputs but not outputs\n* **#33199 [JIT] remove list appends**\n* #33186 [JIT] Pass To Safely Remove Aten Inplace Ops\n* #33020 [JIT] Functional Graph Pass\n\r\nRemove list appends when we can match them with a list construction. This helps create a larger functional graph", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-02-11 22:08:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563505003": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33198, "title": "[wip][jit] c++ api", "body": "", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-11 21:56:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563502906": {"author_username": "nairbv", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33197, "title": "Inconsistent behavior of zeros in certain operations with memory_format=channels_last", "body": "## \ud83d\udc1b Bug\r\n\r\nThere is some strange behavior of values stored in tensors with the channels_last format, where certain operations result in values inconsistent with the contiguous version, or where a zero diff is misinterpreted as non-zero. Probably vectorization related. Examples below.\r\n\r\nDiscovered while investigating strange test tolerances in:\r\nhttps://github.com/pytorch/pytorch/pull/32538#issuecomment-584387763\r\n\r\nA few operators (other than sqrt) appeared to have been affected. This was found in results from test_memory_format_operators.\r\n\r\ncc @VitalyFedyunin \r\n\r\n## To Reproduce\r\n\r\nOn a linux GPU server:\r\n\r\n```\r\nIn [2]: x = torch.randn((4, 3, 8, 8 ), device='cpu').contiguous(memory_format=torch.channels_last)\r\nIn [3]: r1=x.sqrt()\r\nIn [4]: r2=x.contiguous().sqrt()\r\nIn [5]: diff=torch.abs(r1-r2)\r\nIn [6]: diff.max()\r\nOut[6]: tensor(nan)\r\nIn [7]: diff[diff==diff].max()\r\nOut[7]: tensor(0.)\r\nIn [8]: diff[ (diff!=0) & (diff==diff) ]\r\nOut[8]: tensor([])\r\nIn [9]: (diff/1).max()\r\nOut[9]: tensor(5.3258e+34)```\r\n```\r\n\r\nOn OSX:\r\n```\r\n>>> x = torch.randn((4, 3, 8, 8 ), device='cpu').contiguous(memory_format=torch.channels_last)\r\n>>> r1=x.sqrt()\r\n>>> r2=x.contiguous().sqrt()\r\n>>> diff=torch.abs(r1-r2)\r\n>>> diff.max()\r\ntensor(2.1659)\r\n```\r\n\r\n## Expected behavior\r\n\r\nAll values in diff should be zero or nan. In cases where the values are zero, `diff/1` should still be zero in both environments.\r\n\r\n## Environment\r\n\r\nLinux Environment:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0a0+7b50e76\r\nIs debug build: Yes\r\nCUDA used to build PyTorch: 9.2\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration:\r\nGPU 0: Tesla M40\r\nGPU 1: Tesla M40\r\n\r\nNvidia driver version: 396.69\r\ncuDNN version: /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudnn.so.7.4.2\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.1\r\n[pip] numpydoc==0.8.0\r\n[pip] torch==1.5.0a0+1016716\r\n[conda] blas                      1.0                         mkl\r\n[conda] magma-cuda92              2.3.0                         1    pytorch\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl-include               2019.0                      118\r\n[conda] mkl-service               1.1.2            py37h90e4bf4_5\r\n[conda] mkl_fft                   1.0.4            py37h4414c95_1\r\n[conda] mkl_random                1.0.1            py37h4414c95_1\r\n[conda] mkldnn                    0.14.0                        0    mingfeima\r\n[conda] torch                     1.0.0a0+f0ed927           <pip>\r\n[conda] torch                     1.4.0a0+ebc216a           <pip>\r\n[conda] torch                     1.5.0a0+a7194e5           <pip>\r\n[conda] torch                     1.4.0a0+ec92711           <pip>\r\n[conda] torch                     1.2.0a0+646a7f9           <pip>\r\n[conda] torch                     1.0.0a0+f3eb196           <pip>\r\n[conda] torch                     1.5.0a0+1016716           <pip>\r\n[conda] torch                     1.5.0a0+107436a           <pip>\r\n[conda] torch                     1.1.0a0+71bdfe8           <pip>\r\n[conda] torch                     1.4.0a0+36d17f4           <pip>\r\n[conda] torch                     1.4.0a0+159835e           <pip>\r\n[conda] torch                     1.0.0a0+623bf65           <pip>\r\n[conda] torch                     1.0.0a0+5730d0c           <pip>\r\n[conda] torch                     1.4.0a0+540b9da           <pip>\r\n[conda] torch                     1.5.0a0+be6ffac           <pip>\r\n[conda] torch                     1.1.0a0+0676ba0           <pip>\r\n[conda] torch                     1.4.0a0+7692494           <pip>\r\n[conda] torch                     1.4.0a0+927588d           <pip>\r\n[conda] torch                     1.5.0a0+5ed92de           <pip>\r\n[conda] torch                     1.5.0a0+6b37086           <pip>\r\n[conda] torch                     1.0.0a0+d062069           <pip>\r\n[conda] torch                     1.5.0a0+57e76f4           <pip>\r\n[conda] torch                     1.5.0a0+738efa2           <pip>\r\n[conda] torch                     1.5.0a0+770d990           <pip>\r\n[conda] torch                     1.5.0a0+a9583c1           <pip>\r\n[conda] torch                     1.5.0a0+0217a9d           <pip>\r\n[conda] torch                     1.1.0a0+22babf1           <pip>\r\n[conda] torch                     1.0.0a0+3ab3061           <pip>\r\n[conda] torch                     1.5.0a0+a873850           <pip>\r\n[conda] torch                     1.0.0a0+b911ca9           <pip>\r\n[conda] torch                     1.5.0a0+b03e032           <pip>\r\n[conda] torch                     1.0.0a0+69a4608           <pip>\r\n[conda] torch                     1.0.0a0+8a64846           <pip>\r\n[conda] torch                     1.2.0a0+34536e2           <pip>\r\n[conda] torch                     1.4.0a0+cd3f05b           <pip>\r\n[conda] torch                     1.0.0a0+983c364           <pip>\r\n[conda] torch                     1.0.0a0+7270aaa           <pip>\r\n[conda] torch                     1.5.0a0+7b50e76           <pip>\r\n```\r\n\r\nOSX Environment:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0a0+e7f0b15\r\nIs debug build: Yes\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.15.2\r\nGCC version: Could not collect\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] numpydoc==0.8.0\r\n[pip3] torch==1.5.0a0+e7f0b15\r\n[pip3] torchserve==0.1.0b20191219\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.3                      199\r\n[conda] mkl-include               2019.4                      233\r\n[conda] mkl-service               1.1.2            py37hfbe908c_5\r\n[conda] mkl_fft                   1.0.10           py37h5e564d8_0\r\n[conda] mkl_random                1.0.2            py37h27c97d8_0\r\n[conda] torch                     1.4.0a0+68e5172          pypi_0    pypi\r\n[conda] torchserve                0.1.0b20191219           pypi_0    pypi\r\n\r\n```\r\n\r\n", "labels": ["module: internals", "triaged"], "number_of_comments": 1, "created_at": "2020-02-11 21:52:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563463753": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33193, "title": "[WIP] Add option to log subprocess output to files in DDP launcher.", "body": "Closes https://github.com/pytorch/pytorch/issues/7134. This request is to add an option to log the subprocess output (each subprocess is training a network with DDP) to a file instead of the default stdout.\r\n\r\nThe reason for this is that if we have N processes all writing to stdout, it'll be hard to decipher the output, and it would be cleaner to log these to separate files.", "labels": [], "number_of_comments": 2, "created_at": "2020-02-11 20:40:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563451806": {"author_username": "meganset", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33192, "title": "c++ make torch::nn::Sequential push_back(AnyModule) methods public", "body": "## \ud83d\ude80 Feature\r\ncan these private _push_back_ functions be made public?\r\n\r\n```\r\n/// Adds a type-erased `AnyModule` to the `Sequential`.\r\n  void push_back(AnyModule any_module) {\r\n    push_back(c10::to_string(modules_.size()), std::move(any_module));\r\n  }\r\n\r\n  void push_back(std::string name, AnyModule any_module) {\r\n    modules_.push_back(std::move(any_module));\r\n    const auto index = modules_.size() - 1;\r\n    register_module(std::move(name), modules_[index].ptr());\r\n  }\r\n```\r\n\r\n## Motivation\r\n\r\nIf i already have an `AnyModule a`,\r\nI can't figure out how to add it to a _Sequential_ without something like\r\n`if(auto* m=a.as<torch::nn::Conv2d>()) seq->push_back<torch::nn::Conv2d>(*m)`\r\nwhich will then turn it back into an AnyModule and add it.\r\n\r\nOr making the single AnyModule into a vector:\r\n`seq->extend(std::vector<AnyModule>{a})`\r\n\r\nThanks\n\ncc @yf225", "labels": ["module: cpp", "triaged"], "number_of_comments": 0, "created_at": "2020-02-11 20:16:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563428480": {"author_username": "csverma610", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33188, "title": "Pytorch with Cuda 10.2", "body": "Hello \r\n\r\nMy ubuntu 18.04 automatically upgraded the Nvidia driver to 440.59 \r\nand I see the following with nvidis-smi\r\n\r\n-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.59       Driver Version: 440.59       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+\r\n|   0  GeForce GTX TIT...  Off  | 00000000:65:00.0  On |                  N/A |\r\n| 26%   32C    P8    16W / 250W |    600MiB /  6080MiB |      0%      Default |\r\n\r\n\r\nNow, how do I install Pytorch which uses Cuda10.2 ? The website only support 10.1 and\r\ninstalling Pytorch from the source turned out to be a non-trivial task.\r\n\r\nThanks.\r\n\n\ncc @ngimel", "labels": ["module: cuda", "triaged"], "number_of_comments": 13, "created_at": "2020-02-11 19:34:04", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563376653": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33186, "title": "[JIT] Pass To Safely Remove Aten Inplace Ops", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33297 Allow mutated values as inputs but not outputs\n* #33199 [JIT] remove list appends\n* **#33186 [JIT] Pass To Safely Remove Aten Inplace Ops**\n* #33020 [JIT] Functional Graph Pass\n\r\nThis helps create larger functional graphs. It has the potential to increase memory use, so in order to land this on by default we would probably also do a reuse of buffers pass.\r\n\r\nThis is currently O(n * | Removed Nodes | ) because we have to rebuild the alias Db each time we make a change. This pass is critical to creating functional graphs, so this might be a compelling use case to build incremental updates to alias Db.", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-11 17:53:32", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563307012": {"author_username": "ngoldbaum", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33182, "title": "Add a private API that lists overridable functions", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\n\r\nAdd a way for library authors who depend on pytorch to get a list of all functions that they would need to implement to fully wrap the API surface that is overridable via `__torch_function__`.\r\n\r\n## Motivation\r\n\r\nSee e.g. https://github.com/pytorch/pytorch/pull/27064#issuecomment-584702304.\r\n\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\n\r\n\r\nCurrently it's not straightforward for authors of types that implement `__torch_function__` to test that they fully cover the API that is overridable via `__torch_function__`. It's important that they are able to wrap as much of the API as possible because once you have written a type that implements `__torch_function__` users of that type will see a `TypeError` if the `__torch_function__` implementation returns `NotImplemented` for unwrapped functions.\r\n\r\nIt would be nice if there were some private API that gave developers access to the functions that are overridable. It might also be nice to include the function signatures in an easily introspectable format as that makes it easier to write tests for the overrides.\r\n\r\n## Alternatives\r\n\r\nIt might also be sufficient to just have a list of functions in the documentation.", "labels": ["enhancement", "module: tests", "triaged"], "number_of_comments": 3, "created_at": "2020-02-11 16:01:47", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563147739": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33181, "title": "[feature request] [dataloader] Introduce Dataset.__collate__", "body": "And use it if `collate_fn` is unspecified and `dataset.__collate__` is specified. This makes sense, because batch collation logic is very often tied to `__getitem__` return values interface\r\n\r\n(As a workaround I currently create separately a `dataset` instance and then pass `collate_fn = dataset.collate_fn`. This prevents from creating `DataLoader(MyDataset())` in a single line)\n\ncc @SsnL", "labels": ["enhancement", "module: dataloader", "needs research", "triaged"], "number_of_comments": 2, "created_at": "2020-02-11 11:35:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563080531": {"author_username": "enjoykcc456", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33180, "title": "Loading pretrained model", "body": "Hi, \r\n\r\nRecently i have converted a detectron model using the tools/convert_pkl_to_pb.py from Detetron repo which results in model_init.pb,  model.pb and model.pbtxt. Then, i try to load the pretrained model following this tutorial (https://github.com/facebookarchive/tutorials/blob/master/Loading_Pretrained_Models.ipynb). By, following through the tutorial, i was able to make predictions using the provided model. However, when i try with the converted model, the following error occurs. \r\n\r\n```\r\n[E operator.cc:203] Cannot find operator schema for BatchPermutation. Will skip schema checking.\r\nTraceback (most recent call last):\r\n  File \"infer.py\", line 153, in <module>\r\n    p = workspace.Predictor(init_net, predict_net)\r\n  File \"/opt/conda/lib/python3.6/site-packages/caffe2/python/workspace.py\", line 186, in Predictor\r\n    return C.Predictor(StringifyProto(init_net), StringifyProto(predict_net))\r\nRuntimeError: [enforce fail at operator.cc:273] op. Cannot create operator of type 'BatchPermutation' on the device 'CUDA'. Verify that implementation for the corresponding device exist. It might also happen if the binary is not linked with the operator implementation code. If Python frontend is used it might happen if dyndep.InitOpsLibrary call is missing. Operator def: input: \"roi_feat_shuffled\" input: \"rois_idx_restore_int32_gpu_0\" output: \"roi_feat\" name: \"\" type: \"BatchPermutation\" device_option { device_type: 1 device_id: 0 }\r\nframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x5b (0x7fd70bf1341b in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libc10.so)\r\nframe #1: caffe2::CreateOperator(caffe2::OperatorDef const&, caffe2::Workspace*, int) + 0xc14 (0x7fd70eaec084 in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #2: caffe2::SimpleNet::SimpleNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x2e0 (0x7fd70eae4040 in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #3: <unknown function> + 0x278338e (0x7fd70eae638e in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #4: std::_Function_handler<std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*), std::unique_ptr<caffe2::NetBase, std::default_delete<caffe2::NetBase> > (*)(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*)>::_M_invoke(std::_Any_data const&, std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*&&) + 0xf (0x7fd70eabaf7f in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #5: caffe2::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, caffe2::Workspace*) + 0x464 (0x7fd70eaacb74 in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #6: caffe2::Workspace::CreateNet(std::shared_ptr<caffe2::NetDef const> const&, bool) + 0xfe (0x7fd70eb297ee in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #7: caffe2::Predictor::Predictor(caffe2::PredictorConfig) + 0x323 (0x7fd70edbb853 in /opt/conda/lib/python3.6/site-packages/caffe2/python/../../torch/lib/libtorch.so)\r\nframe #8: <unknown function> + 0x4f8a5 (0x7fd73e1298a5 in /opt/conda/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\nframe #9: <unknown function> + 0x4fb77 (0x7fd73e129b77 in /opt/conda/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\nframe #10: <unknown function> + 0x95696 (0x7fd73e16f696 in /opt/conda/lib/python3.6/site-packages/caffe2/python/caffe2_pybind11_state_gpu.cpython-36m-x86_64-linux-gnu.so)\r\n<omitting python frames>\r\nframe #30: __libc_start_main + 0xf0 (0x7fd76da8e830 in /lib/x86_64-linux-gnu/libc.so.6)\r\n```\r\nAnyone knows why this happen?\r\n\r\nThanks", "labels": ["caffe2"], "number_of_comments": 1, "created_at": "2020-02-11 09:42:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "563070559": {"author_username": "XiaobingSuper", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33179, "title": "Move glu to Aten(CPU)", "body": "This PR move glu to Aten(CPU). \r\nTest script:\r\n```\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport time\r\n\r\ntorch.manual_seed(0)\r\n\r\ndef _time():\r\n    if torch.cuda.is_available():\r\n        torch.cuda.synchronize()\r\n    return time.time()\r\n\r\ndevice = \"cpu\"\r\n\r\n#warm up\r\nfor n in [10, 100, 1000, 10000]:\r\n    input = torch.randn(128, n, requires_grad=True, device=device)\r\n    grad_output = torch.ones(128, n // 2, device=device)\r\n    for i in range(1000):\r\n        output = F.glu(input)\r\n        output.backward(grad_output)\r\n\r\nfor n in [10, 100, 1000, 10000]:\r\n    fwd_t = 0\r\n    bwd_t = 0\r\n    input = torch.randn(128, n, requires_grad=True, device=device)\r\n    grad_output = torch.ones(128, n // 2, device=device)\r\n    for i in range(10000):\r\n        t1 = _time()\r\n        output = F.glu(input)\r\n        t2 = _time()\r\n        output.backward(grad_output)\r\n        t3 = _time()\r\n        fwd_t = fwd_t + (t2 -t1)\r\n        bwd_t = bwd_t + (t3 - t2)\r\n    fwd_avg = fwd_t / 10000 * 1000\r\n    bwd_avg = bwd_t / 10000 * 1000\r\n    print(\"input size(128, %d) forward time is %.2f (ms); backwad avg time is %.2f (ms).\"\r\n          % (n, fwd_avg, bwd_avg))\r\n```\r\nTest device: **skx-8180.**\r\nBefore:\r\n```\r\ninput size(128, 10) forward time is 0.04 (ms); backwad avg time is 0.08 (ms).\r\ninput size(128, 100) forward time is 0.06 (ms); backwad avg time is 0.14 (ms).\r\ninput size(128, 1000) forward time is 0.11 (ms); backwad avg time is 0.31 (ms).\r\ninput size(128, 10000) forward time is 1.52 (ms); backwad avg time is 2.04 (ms).\r\n```\r\nAfter:\r\n```\r\ninput size(128, 10) forward time is 0.02 (ms); backwad avg time is 0.05 (ms).\r\ninput size(128, 100) forward time is 0.04 (ms); backwad avg time is 0.09 (ms).\r\ninput size(128, 1000) forward time is 0.07 (ms); backwad avg time is 0.17 (ms).\r\ninput size(128, 10000) forward time is 0.13 (ms); backwad avg time is 1.03 (ms).\r\n```\r\nFix #24707, #24708.", "labels": ["open source", "topic: porting", "triaged"], "number_of_comments": 10, "created_at": "2020-02-11 09:24:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562957059": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33175, "title": "Removed complex specific dispatch and backend keys, fixed complex tensor printing and torch.tensor, torch.zeros", "body": "1. Removed ComplexCPU and ComplexCUDA dispatch key\r\n2. Removed ComplexCPU and ComplexCUDA backend enum keys\r\n3. Updated is_floating_point to return False when the dtype is complex64 or complex128\r\n4. Added is_complex to native_functions and python bindings for is_complex\r\n5. Fixed printing logic for complex tensors\r\n6. torch.tensor (when dtype is provided) and torch.zeros working now\r\n7. fixed py object returned for complex float", "labels": [], "number_of_comments": 9, "created_at": "2020-02-11 03:29:50", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 1, "hooray": 0}}, "562943517": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33173, "title": "[quant][graphmode] Add a pass to handle ops that doesn't require observation", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* **#33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation**\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nHow to deal with ops that\u2019s defined for both floating point and quantized Tensor?\n\nCategory of ops: the ones that doesn\u2019t require observers, which means the quantization parameters(scale/zero_point) of the output of this op can be inferred from the quantization parameters of inputs.\nFor example:\navg_pool, max_pool, flatten, transpose, upsample\n\nAnother related topic to previous one is how do we deal with things like adaptive_avg_pool2d that does not require to be observed and it works with quantized tensor as well? If we insert quant/dequant for them, even the quant fusion becomes a numerically changing operation because the scale/zero_point for input and output are different.\n\nProposal\n\nWe can swap the operator with dequantize whenever we see it. For example, for pattern\nLet\u2019s say aten::general_op is defined for both floating point and quantized\n\n%r = aten::conv(...)\n%q = quantize(%r)\n%dq = dequantize(%q)\n%f = aten::general_op(%dq)\n...\n\nWe detect that all inputs of aten::general_op is produced by dequantize, we\u2019ll first delete all the dequantize for the inputs and then insert dequantize for each use of the output of the aten::general_op, note that this should work generally for all the case we might encounter.\n\nAfter transformation we\u2019ll have:\n\n%r = aten::conv(...)\n%q = quantize(%r)\n%x = aten::general_op(%q)\n%f = dequantize(%x)\n...\n\n1. Multiple inputs\n    1. We need to make sure all inputs of the aten::general_op are produced by dequantize before we do this transformation\n2. Input used by multiple operators\n    1. We already did this by inserting dequantize for each use of the value\n3. Output used by multiple operators\n    1. We\u2019ll reuse the code that inserts dequantize(might need some refactor)\n\nNote that current concat does not belong to this category right now since it does not inherit quantization parameters from inputs.\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz, raghuramank100\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-11 02:40:05", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562943487": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33172, "title": "[quant][graphmode][refactor] Factor out insertDequantCall", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* **#33172 [quant][graphmode][refactor] Factor out insertDequantCall**\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nFor code reuse\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-11 02:39:56", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562943463": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33171, "title": "[quant][graphmode] refactor nodeQuantizable", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* **#33171 [quant][graphmode] refactor nodeQuantizable**\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nFor better code reuse\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-11 02:39:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562937924": {"author_username": "PetrochukM", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33168, "title": "Conv Transposed + Layer Norm = \"RuntimeError: columns needs to be contiguous\"", "body": "## \ud83d\udc1b Bug\r\n\r\nI'm unable to run layer normalization after a transposed convolution in PyTorch 1.4.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nimport torch\r\n\r\nin_channels = 64\r\nout_channels = 128\r\nscale_factor = 8\r\nbatch_size = 8\r\nlength = 16\r\n\r\nconv = torch.nn.ConvTranspose1d(\r\n    in_channels, out_channels, kernel_size=scale_factor * 2, stride=scale_factor)\r\nlayer_norm = torch.nn.LayerNorm(out_channels)\r\n\r\ninput_ = torch.randn(batch_size, in_channels, length).contiguous()\r\ninput_ = conv(input_).contiguous()\r\ninput_ = layer_norm(input_.transpose(1, 2).contiguous()).contiguous()\r\ninput_.sum().backward()\r\n```\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"blah.py\", line 15, in <module>\r\n    input_.sum().backward()\r\n  File \"/Users/michaelp/Code/Text-to-Speech/venv/lib/python3.7/site-packages/torch/tensor.py\", line 195, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/Users/michaelp/Code/Text-to-Speech/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: columns needs to be contiguous\r\n```\n\ncc @VitalyFedyunin", "labels": ["module: cpu", "triaged"], "number_of_comments": 3, "created_at": "2020-02-11 02:16:39", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562924550": {"author_username": "jianyuh", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33167, "title": "[pytorch][quant] Debug NaN issues in quantized RNN", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33167 [pytorch][quant] Debug NaN issues in quantized RNN**\n* #32739 [pytorch][quant] Add assert for min, max, qmin, qmax for ChooseQuantizationParams\n\nDifferential Revision: [D19826983](https://our.internmc.facebook.com/intern/diff/D19826983/)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-11 01:20:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562915289": {"author_username": "supriyar", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33166, "title": "TensorIterator does not work with different input/output types", "body": "## \ud83d\udc1b Bug\r\n\r\nTensorIterator expects all the inputs and outputs to have the same type. This prevents us from using TensorIterator for operations like quantized batchnorm, where the input is quantized (quint8) but the alpha (scale) and beta (shift) values are in float.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Create a TensorIterator op that has different input/output dtypes \r\n2. build pytorch\r\n\r\nExample - \r\n```\r\nAT_DISPATCH_QINT_TYPES(input.scalar_type(), \"qbatch_norm\", [&]() {\r\n      using Vec = Vec256<quint8>;\r\n      cpu_kernel_vec(\r\n        iter,\r\n        [&] (uint8_t in, float a, float b) -> quint8 {\r\n          long quantized_down = out_zero_point +\r\n              std::lrintf(a * (in - in_zero_point) + b);\r\n          if (ReluFused) { // static if\r\n            quantized_down = std::max<long>(quantized_down, out_zero_point);\r\n          }\r\n          return quint8(std::min<long>(\r\n              std::max<long>(quantized_down, std::numeric_limits<uint8_t>::min()),\r\n              std::numeric_limits<uint8_t>::max()));\r\n\r\n        },\r\n        [&] (Vec in, Vec256<float> a, Vec256<float> b) -> Vec {\r\n          ...\r\n        });\r\n    });\r\n```\r\nYou should see compile error of the type - \r\n\r\n\r\n```\r\nIn file included from aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp.AVX2.cpp:5:\r\n../aten/src/ATen/native/cpu/Loops.h:70:10: error: no viable conversion from returned value of type 'tuple<[...], Vec256<c10::quint8>, Vec256<c10::quint8>>' to function return type 'tuple<[...], Vec256<float>, Vec256<float>>'\r\n  return std::make_tuple(\r\n         ^~~~~~~~~~~~~~~~\r\n../aten/src/ATen/native/cpu/Loops.h:80:10: note: in instantiation of function template specialization 'at::native::(anonymous namespace)::dereference_vec_impl<function_traits<(lambda at aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp.AVX2.cpp:992:5)>, 0, 1, 2>' requested here\r\n  return dereference_vec_impl<traits>(data, opt_scalar, S, i, Indices{});\r\n         ^\r\n../aten/src/ATen/native/cpu/Loops.h:149:18: note: in instantiation of function template specialization 'at::native::(anonymous namespace)::dereference_vec<function_traits<(lambda at aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp.AVX2.cpp:992:5)> >' requested here\r\n    auto args1 = dereference_vec<traits>(&data[1], opt_scalar, S, i);\r\n                 ^\r\n../aten/src/ATen/native/cpu/Loops.h:211:14: note: in instantiation of function template specialization 'at::native::(anonymous namespace)::vectorized_loop<(lambda at aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp.AVX2.cpp:992:5), (lambda at aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp.AVX2.cpp:992:5)>' requested here\r\n      return vectorized_loop(data, n, 0, std::forward<func_t>(op), std::forward<vec_func_t>(vop));\r\n             ^\r\n```\r\n\r\n\r\n## Expected behavior\r\n\r\nAllow different types for input and output tensors. \r\n\r\nSpecifically, don't restrict the type to be dependent on the return type - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/Loops.h#L137\r\n\r\n\r\ncc @jamesr66a, @raghuramank100 ", "labels": ["topic: TensorIterator", "triaged"], "number_of_comments": 0, "created_at": "2020-02-11 00:46:52", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562856948": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33158, "title": "[WIP] Move schema inference into KernelFunction factory.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33158 [WIP] Move schema inference into KernelFunction factory.**\n* #33097 Stop generating out full function type for registration, use decltype or infer it\n* #33093 Delete unnecessary aliasAnalysis specification from operator registrations.\n* #33011 Beef up documentation on DispatchKey.h\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "labels": [], "number_of_comments": 1, "created_at": "2020-02-10 22:05:39", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562847340": {"author_username": "wanchaol", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33157, "title": "[autograd] enable graph level thread parallelism on CPU", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33157 [autograd] enable graph level thread parallelism on CPU**\n\r\nThis PR enables graph level thread parallelism on CPU for the Autograd\r\nEngine. It replace https://github.com/pytorch/pytorch/pull/29574 for the\r\nreason of task level parallelism drawbacks with the existing autograd\r\nsystem.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/18333\r\n\r\nThe graph level parallelism on CPU design:\r\n\r\n1. Remove the single CPU thread that init in the Engine itself and allow\r\n   the owning thread (which calls Engine::execute) to drive the Engine\r\n   execution so that we could let outer threading to enable thread\r\n   parallelism.\r\n2. Maintain a separate ReadyQueue per CPU thread, and stash the\r\n   ReadyQueue for different devices/threads into the thread local\r\n   shared_ptr, the Engine itself will memorize the shared_ptr of the\r\n   ReadyQueue to different devices (other than CPU)\r\n3. The CPU thread local ReadyQueue is initialized per CPU thread\r\n   Engine::execute call (or `backward()`, `grad()` call), and memorized\r\n   the shared_ptr into the GraphTask since every `backward()` call have\r\n   its own GraphTask\r\n4. Cross device NodeTask push is accomplished by 2 and 3. we can refer\r\n   to device's ReadyQueue from Engine, and CPU's ReadyQueue from\r\n   GraphTask, which means if we can push to a different ReadyQueue\r\n   according to the device\r\n5. Termination of the CPU thread: if we mark the graph_task as\r\n   completed, we will exit the while loop and terminate the current\r\n   backward execution, because it's guranteed that all other NodeTasks\r\n   is finished before we mark a GraphTask as complete\r\n6. re-entrant thread logic keeps the same, reentrant thread detection is\r\n   similar as before, we set the worker_device to NO_DEVICE initially\r\n   and set to CPU afterward to detect if this is a reentrant call or not.\r\n7. we still have the reentrant thread pool that create new threads if it's\r\n   a deep reentrant case, and reuse the ReadyQueue with the parent thread \r\n   for performance.\r\n\r\nSince we introduce the thread parallelism on CPU, we have to ensure the\r\nthread safety of the GraphTask. This is not a problem if we execute all\r\nforward in different threads since we will build separate GraphTask in\r\ndifferent threads, and each GraphTask is a separate instance that share\r\nnothing, i.e. Hogwild training on CPU should be fine on this case.\r\n\r\nBut there might be case that user would like to do some part of the task in\r\na single thread, and do the rest of work in several threads\r\nconcurrently, so thread safety is crucial in those cases. The thread\r\nsafety strategy for the multithread autograd is as follows:\r\n\r\n1. Add a mutex to protect thread safety in Autograd Node/Function, and\r\n   hold the lock for different data racing cases\r\n2. Lock the mutex during Node::apply(), this is to ensure Node that\r\n   writing to the shared variable are not racing across threads (i.e.\r\n   AccumulateGrad and custom C++ Autograd Node if writing to shared\r\n   variables )\r\n3. Lock the mutex during Node::release_variables(), this serve the\r\n   purpose that when we release saved_variables from one thread, no\r\n   other threads can call the Node::apply(), this ensures the variable\r\n   references from other threads aren't dangling.\r\n4. If we don't release any variables and no shared data read/write in\r\n   the Node i.e. purely functional, we don't lock the mutex\r\n\r\nThis way we could protect the thread safety on Autograd Node, but we\r\ncould still not protect the thread safety on Node pre/post C++ hooks\r\n(python hooks are automatically thread safe), we rely on the user to\r\nwrite thread safe C++ hooks if they want the hook to be correctly\r\napplied in multithreading environment.\r\n\r\n**User visiable changes**:\r\nThere're not too much user visiable changes, since we use the owning\r\nthread to drive the autograd execution, user could write their own\r\nthreading code and does not block on the Autograd engine, some behaviors\r\nthat user should be aware of:\r\n\r\n**Non-determinism**:\r\nif we are calling backward() on multiple thread concurrently but with\r\nshared inputs (i.e. Hogwild CPU training). Since parameters are automatically shared across threads, gradient accumulation might become non-deterministic on backward calls across threads, because two backward calls might access and try to accumulate the same .grad attribute. This is technically not safe, and it might result in racing condition and the result might be invalid to use.\r\n\r\nBut this is expected pattern if user are using the multithreading\r\napproach to drive the whole training process but using shared\r\nparameters, user who use multithreading should have the threading model\r\nin mind and should expect this to happen. User should use the functional\r\ninterface `torch.autograd.grad()` to calculate the gradients instead of\r\n`backward()` on loss.\r\n\r\n**Graph retaining**:\r\nIf part of the autograd graph is shared between threads, i.e. run first\r\npart of forward single thread, then run second part in multiple threads,\r\nthen the first part of graph is shared. In this case different threads execute grad() or backward() on the same graph might\r\nhave issue of destroying the graph on the fly of one thread, and the\r\nother thread will crash in this case. We will error out to the user\r\nsimilar to what call `backward()` twice with out `retain_graph=True`, and let the user know they should use `retain_graph=True`.\r\n\r\n**TODOs**:\r\n\r\n[ ] benchmark the PR with example models and datasets to demonstrate\r\nthe performance gain in CPU training\r\n[ ] ensure that we don't regress the single thread autograd performance\r\n\r\n**Follow ups**:\r\n\r\n[ ] a correct and tight integration with distributed autograd\r\n[ ] try to unify the thread pool between JIT and Autograd, and see if\r\nthere's unifying pattern that we could apply universally", "labels": [], "number_of_comments": 6, "created_at": "2020-02-10 21:45:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562827596": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33155, "title": "[JIT] Don't run constant propagation on values that escape", "body": "We shouldn't run constant propagation on mutable values that escape, because they might be mutated outside of the current scope. This hasn't been a problem because we inline everything, but better to be more safe here.\r\n\r\nEDIT: maybe we don't want this PR until we track contained elements of lists / dictionaries", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-10 21:06:41", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562800863": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33153, "title": "Add interface to collect RPC-based training metrics.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33153 Add interface to collect RPC-based training metrics.**\n\r\nAdds the initial interface `RpcMetricsHandler.h` for tracking metrics during distributed model parallel/RPC-based training. Metric Handlers should implement this interface in order to log metrics and stream them to some metrics storage.\r\n\r\nDifferential Revision: [D19615364](https://our.internmc.facebook.com/intern/diff/D19615364/)\r\n\r\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D19615364/)!", "labels": [], "number_of_comments": 1, "created_at": "2020-02-10 20:13:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562792788": {"author_username": "anjali411", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33152, "title": "Complex Numbers Support", "body": "Previous discussion: https://github.com/pytorch/pytorch/issues/755\r\n\r\nTasks to be done:\r\n1. type promotion (eg. Float/Long/... tensor + a complex number = complex tensor)\r\n2. randn for complex dtype\r\n3. gradcheck\r\n4. Matrix Multiplication\r\n5. autograd for C -> C, C -> R, R -> C\r\n6. Fix print/possibly other for a=torch.randn(2,2).to(torch.complex64)\r\n\r\nReported Bugs:\r\n1. https://github.com/pytorch/pytorch/issues/33493\r\n2. https://github.com/pytorch/pytorch/issues/33494\r\n\r\ncc @ezyang @anjali411 @dylanbespalko", "labels": ["module: complex", "triaged"], "number_of_comments": 0, "created_at": "2020-02-10 19:59:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562776398": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33149, "title": "[wip][jit] Modulelist", "body": "", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-10 19:28:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562774135": {"author_username": "mrshenli", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33148, "title": "Support uneven DDP inputs", "body": "We have seen multiple users hit the problem of different DDP instance having different number of input batches. [[e.g.](https://discuss.pytorch.org/t/best-practice-for-uneven-dataset-sizes-with-distributeddataparallel/67308)] As a result, DDP instances processing more input batches will hang, as the peer with least input batch will not join those additional allreduce operations. \r\n\r\nApplications can address this by sth like:\r\n\r\n```python\r\nfor batch in get_batch():\r\n    x = torch.tensor(int(has_next())\r\n    op = all_reduce(x, async_op=True)\r\n    ddp(batch).sum().backward()\r\n    opt.step()\r\n    op.wait()\r\n    if x.item() > 0:\r\n        break\r\n```\r\n\r\nAs the allrecuce is async and can overlap with the forward+backward+optimizer computation, the extra overhead should be fine. The question is whether we should implement this as an helper API in DDP, or can we leave it out as the application-side solution is simple enough?\r\n\r\n@pritamdamania87 mentioned that this solution might not be sufficient for applications that cannot support `has_next()` API. In that case, we would need to support sth like below:\r\n\r\n```python\r\nfor data in iterator:\r\n  loss = ddp_model(data)\r\n  loss.backward()\r\n  optim.step()\r\n\r\n# This writes EOF to the store, and abort allreduce in other DDP instances.\r\nddp_model.mark_end_of_data()\r\n```\r\n\r\nThis approach is more versatile, but needs to expose `ProcessGroup` internal `Store` to DDP and also need to implement `abort` for all `ProcessGroup` backends.\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar ", "labels": ["module: distributed", "triaged"], "number_of_comments": 3, "created_at": "2020-02-10 19:23:52", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562752739": {"author_username": "xychang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33145, "title": "[caffe2] Add embedding empty ratio checker (disabled by default)", "body": "Differential Revision: D19716574\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-10 18:40:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562750903": {"author_username": "mickypaganini", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33144, "title": "Fix examples with updated pruning naming convention", "body": "Fix in docs requested by @vainaijr.\r\nCloses issue #32991 ", "labels": [], "number_of_comments": 1, "created_at": "2020-02-10 18:37:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562712475": {"author_username": "JosephDiPalma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33143, "title": "CUDA Out of Memory when trying to allocate 16 EiB", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using `torch.float16` as the dtype, 16EiB of GPU memory is requested for allocation for certain tensor shapes.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\n>>> import torch\r\n>>> import torchvision.models as models\r\n>>> resnet = models.resnet18().half().cuda()\r\n>>> t = torch.ones([1, 3, 10210, 8641], dtype=torch.float16, device=\"cuda\")\r\n>>> output = resnet(t)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torchvision/models/resnet.py\", line 198, in _forward\r\n    x = self.conv1(x)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 345, in forward\r\n    return self.conv2d_forward(input, self.weight)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: CUDA out of memory. Tried to allocate 17179869183.95 GiB (GPU 0; 23.65 GiB total capacity; 3.15 GiB already allocated; 18.80 GiB free; 3.15 GiB reserved in total by PyTorch)\r\n```\r\n\r\nChanging the tensor `t` while keeping the rest of the code the same doesn't cause an error as shown below:\r\n```\r\n>>> t = torch.ones([1, 3, 10500, 9000], dtype=torch.float16, device=\"cuda\")\r\n>>> output = resnet(t)\r\n>>> output.shape\r\ntorch.Size([1, 1000])\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nThe correct amount of memory should be allocated.  This code works correctly for `torch.float32` dtype.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0a0+a5b4d78\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: TITAN RTX\r\nNvidia driver version: 440.48.02\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy==0.4.3.2\r\n[pip] numpy==1.17.4\r\n[pip] pytorch-transformers==1.1.0\r\n[pip] torch==1.4.0a0+a5b4d78\r\n[pip] torchtext==0.4.0\r\n[pip] torchvision==0.5.0a0\r\n[conda] magma-cuda101             2.5.2                         1    local\r\n[conda] mkl                       2019.1                      144  \r\n[conda] mkl-include               2019.1                      144  \r\n[conda] nomkl                     3.0                           0  \r\n[conda] pytorch-transformers      1.1.0                    pypi_0    pypi\r\n[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi\r\n[conda] torchtext                 0.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.5.0a0                  pypi_0    pypi\r\n\r\n\r\n## Additional context\r\n\r\nI'm using the official NVIDIA Docker build from `nvcr.io/nvidia/pytorch:20.01-py3`.\r\n\n\ncc @ngimel @csarofeen @ptrblck", "labels": ["module: cuda", "module: cudnn", "triaged"], "number_of_comments": 7, "created_at": "2020-02-10 17:26:25", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562690723": {"author_username": "ghost", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33141, "title": "About parallel_for(const int64_t begin, const int64_t end, const int64_t grain_size, const F& f)", "body": "## \u2753 Questions and Help\r\nwhen I see aten/src/ATen/native/AveragePool2d.cpp, I don't understand at::parallel_for(0, nInputPlane, 0, [&](int64_t start, int64_t end) {...}), why is the value of third parameter equal to 0. I think the value of grain_size is not equal to zero. Can you tell me the reason of situation? \r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @VitalyFedyunin @ngimel", "labels": ["topic: performance", "triaged"], "number_of_comments": 1, "created_at": "2020-02-10 16:49:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562666086": {"author_username": "mpariente", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33140, "title": "Summary writer doesn't support None", "body": "## \ud83d\udc1b Bug\r\nIf a `None` is encountered [here](https://github.com/pytorch/pytorch/blob/master/torch/utils/tensorboard/summary.py#L139), `hparams` will raise an error.\r\nIs there a reason we don't want `None`? In this case, can we just convert them to string? \r\n\r\n## To Reproduce\r\n```\r\nfrom torch.utils.tensorboard.summary import hparams \r\nto_hparams = dict(this=1, and=2, that=None)\r\na, b, c = hparams(hparam_dict=to_hparams, metric_dict={})\r\n```\r\n## Expected behavior\r\nJust support `None` values.\r\n\r\n## Environment\r\nThis is not related to environments, the reason is pretty clear. \r\nI can make a PR if you agree. ", "labels": ["visualization"], "number_of_comments": 0, "created_at": "2020-02-10 16:11:26", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562617849": {"author_username": "albanD", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33138, "title": "Inconsistent handling of undefined Tensors in gradient formulas", "body": "In the cpp backend, undefined Tensors are used to efficiently represent a Tensor full of `0`s.\r\nEven though we do support this in many functions, there are a lot that almost never get undefined gradients and are thus not properly implemented:\r\n- All the backwards with direct formulas in `derivatives.yaml`\r\n- Most functions in `Functions.cpp`.\r\n\r\nIn particular, the assumption that a function with a single output cannot get undefined gradients is not correct.\r\nAn example of such issue can be found in https://github.com/pytorch/pytorch/issues/33037 and https://github.com/pytorch/pytorch/issues/32619\n\ncc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen", "labels": ["high priority", "module: autograd", "module: operators", "triage review", "triaged"], "number_of_comments": 0, "created_at": "2020-02-10 14:58:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562491356": {"author_username": "XiaobingSuper", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33137, "title": "Move cumsum to Aten(CPU)", "body": "This PR is about move cumsum to Aten.\r\nTest script:\r\n```\r\nimport torch\r\n import torch.nn as nn\r\n import time\r\n\r\n torch.manual_seed(0)\r\n\r\n def _time():\r\n     return time.time()\r\n\r\n device = \"cpu\"\r\n\r\n #warm up\r\n for n in [10, 100]:\r\n     for c in [10, 100]:\r\n         for h in [10, 100]:\r\n             input = torch.randn(n, c, h, requires_grad=False, device=device)\r\n             for dim in range(input.dim()):\r\n                 for i in range(1000):\r\n                     output = input.cumsum(dim)\r\nfor n in [10, 100]:\r\n     for c in [10, 100]:\r\n         for h in [10, 100]:\r\n             input = torch.randn(n, c, h, requires_grad=False, device=device)\r\n             for dim in range(input.dim()):\r\n                 fwd_t = 0\r\n                 for i in range(10000):\r\n                     t1 = _time()\r\n                     input.cumsum(dim)\r\n                     t2 = _time()\r\n                     fwd_t = fwd_t + (t2 -t1)\r\n                 fwd_avg = fwd_t / 10000 * 1000\r\n                 print(\"input size(%d, %d, %d) with reduce dim is %d : forward time is %.4f (ms).\" % (n, c, h, dim,      fwd_avg))\r\n```\r\nTest device: **skx-8180**.\r\nBefore:\r\n```\r\ninput size(10, 10, 10) with reduce dim is 0 : forward time is 0.0098 (ms).\r\ninput size(10, 10, 10) with reduce dim is 1 : forward time is 0.0090 (ms).\r\ninput size(10, 10, 10) with reduce dim is 2 : forward time is 0.0089 (ms).\r\ninput size(10, 10, 100) with reduce dim is 0 : forward time is 0.0745 (ms).\r\ninput size(10, 10, 100) with reduce dim is 1 : forward time is 0.0654 (ms).\r\ninput size(10, 10, 100) with reduce dim is 2 : forward time is 0.0204 (ms).\r\ninput size(10, 100, 10) with reduce dim is 0 : forward time is 0.0677 (ms).\r\ninput size(10, 100, 10) with reduce dim is 1 : forward time is 0.0204 (ms).\r\ninput size(10, 100, 10) with reduce dim is 2 : forward time is 0.0636 (ms).\r\ninput size(10, 100, 100) with reduce dim is 0 : forward time is 0.7003 (ms).\r\ninput size(10, 100, 100) with reduce dim is 1 : forward time is 0.1889 (ms).\r\ninput size(10, 100, 100) with reduce dim is 2 : forward time is 0.1858 (ms).\r\ninput size(100, 10, 10) with reduce dim is 0 : forward time is 0.0222 (ms).\r\ninput size(100, 10, 10) with reduce dim is 1 : forward time is 0.0606 (ms).\r\ninput size(100, 10, 10) with reduce dim is 2 : forward time is 0.0604 (ms).\r\ninput size(100, 10, 100) with reduce dim is 0 : forward time is 0.2116 (ms).\r\ninput size(100, 10, 100) with reduce dim is 1 : forward time is 0.6120 (ms).\r\ninput size(100, 10, 100) with reduce dim is 2 : forward time is 0.1750 (ms).\r\ninput size(100, 100, 10) with reduce dim is 0 : forward time is 0.1859 (ms).\r\ninput size(100, 100, 10) with reduce dim is 1 : forward time is 0.1709 (ms).\r\ninput size(100, 100, 10) with reduce dim is 2 : forward time is 0.5732 (ms).\r\ninput size(100, 100, 100) with reduce dim is 0 : forward time is 4.6547 (ms).\r\ninput size(100, 100, 100) with reduce dim is 1 : forward time is 4.8224 (ms).\r\ninput size(100, 100, 100) with reduce dim is 2 : forward time is 1.7931 (ms).\r\n```\r\nAfter:\r\n```\r\ninput size(10, 10, 10) with reduce dim is 0 : forward time is 0.0052 (ms).\r\ninput size(10, 10, 10) with reduce dim is 1 : forward time is 0.0082 (ms).\r\ninput size(10, 10, 10) with reduce dim is 2 : forward time is 0.0086 (ms).\r\ninput size(10, 10, 100) with reduce dim is 0 : forward time is 0.0106 (ms).\r\ninput size(10, 10, 100) with reduce dim is 1 : forward time is 0.0081 (ms).\r\ninput size(10, 10, 100) with reduce dim is 2 : forward time is 0.0087 (ms).\r\ninput size(10, 100, 10) with reduce dim is 0 : forward time is 0.0137 (ms).\r\ninput size(10, 100, 10) with reduce dim is 1 : forward time is 0.0098 (ms).\r\ninput size(10, 100, 10) with reduce dim is 2 : forward time is 0.0088 (ms).\r\ninput size(10, 100, 100) with reduce dim is 0 : forward time is 0.0723 (ms).\r\ninput size(10, 100, 100) with reduce dim is 1 : forward time is 0.0235 (ms).\r\ninput size(10, 100, 100) with reduce dim is 2 : forward time is 0.0150 (ms).\r\ninput size(100, 10, 10) with reduce dim is 0 : forward time is 0.0203 (ms).\r\ninput size(100, 10, 10) with reduce dim is 1 : forward time is 0.0096 (ms).\r\ninput size(100, 10, 10) with reduce dim is 2 : forward time is 0.0081 (ms).\r\ninput size(100, 10, 100) with reduce dim is 0 : forward time is 0.1101 (ms).\r\ninput size(100, 10, 100) with reduce dim is 1 : forward time is 0.0110 (ms).\r\ninput size(100, 10, 100) with reduce dim is 2 : forward time is 0.0108 (ms).\r\ninput size(100, 100, 10) with reduce dim is 0 : forward time is 0.1082 (ms).\r\ninput size(100, 100, 10) with reduce dim is 1 : forward time is 0.0104 (ms).\r\ninput size(100, 100, 10) with reduce dim is 2 : forward time is 0.0104 (ms).\r\ninput size(100, 100, 100) with reduce dim is 0 : forward time is 1.1523 (ms).\r\ninput size(100, 100, 100) with reduce dim is 1 : forward time is 0.0411 (ms).\r\ninput size(100, 100, 100) with reduce dim is 2 : forward time is 0.0324 (ms).\r\n```\r\nFix https://github.com/pytorch/pytorch/issues/24669.", "labels": ["open source", "triaged"], "number_of_comments": 4, "created_at": "2020-02-10 11:14:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562455164": {"author_username": "JakobHavtorn", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33136, "title": "Recursive KL divergences for Independent-wrapped distributions", "body": "This PR implements the changes proposed in issue https://github.com/pytorch/pytorch/issues/32406.", "labels": ["module: distributions", "open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-10 10:12:11", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562231674": {"author_username": "ppwwyyxx", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33133, "title": "Remove `clean_tag` from tensorboard", "body": "The function originally comes from https://github.com/tensorflow/tensorflow/blob/4279f99847e9fcce9410bda61d3b71065e0df65f/tensorflow/python/ops/summary_op_util.py#L45-L68\r\n\r\nAs its comment says:\r\n```   \r\n    # In the past, the first argument to summary ops was a tag, which allowed\r\n    # arbitrary characters. Now we are changing the first argument to be the node\r\n    # name. This has a number of advantages (users of summary ops now can\r\n    # take advantage of the tf name scope system) but risks breaking existing\r\n    # usage, because a much smaller set of characters are allowed in node names.\r\n    # This function replaces all illegal characters with _s, and logs a warning.\r\n    # It also strips leading slashes from the name.\r\n```\r\n\r\nThis function is only for compatibility with TF's operator name restrictions, and is therefore no longer valid in pytorch. By removing it, tensorboard summaries can use more characters in the names.\r\n\r\nBefore:\r\n![0209-12:10:14](https://user-images.githubusercontent.com/1381301/74109072-37382e00-4b35-11ea-8c9f-ab37a8bd5808.png)\r\n\r\n\r\nAfter:\r\n![0209-12:10:57](https://user-images.githubusercontent.com/1381301/74109081-4323f000-4b35-11ea-9dab-447f8466a41e.png)\r\n", "labels": [], "number_of_comments": 0, "created_at": "2020-02-09 20:12:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562225320": {"author_username": "fmigneault", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33132, "title": "Support multiple-build-type generators for CMake", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nBuilding torch with MSVC 2019.\r\nGetting : \r\n```\r\n3>------ Build started: Project: __aten_op_header_gen, Configuration: Debug x64 ------\r\n4>------ Build started: Project: torch_cpu, Configuration: Debug x64 ------\r\n4>cl : command line error D8016: '/Ox' and '/RTC1' command-line options are incompatible\r\n```\r\nSeems similar to #4475, but the cmake files are now completely different from that time.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nFlag /Ox not added for debug build.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\n$ python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows\u259210 Professionnel\r\nGCC version: Could not collect\r\nCMake version: version 3.16.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 441.22\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.15.1\r\n[pip] numpydoc==0.8.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.0                      118\r\n[conda] mkl-service               1.1.2            py37hb217b18_5\r\n[conda] mkl_fft                   1.0.4            py37h1e22a9b_1\r\n[conda] mkl_random                1.0.1            py37h77b88f5_1\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): master\r\n\r\n\n\ncc @peterjc123", "labels": ["module: windows", "triaged"], "number_of_comments": 4, "created_at": "2020-02-09 19:21:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562137182": {"author_username": "skn123", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33129, "title": "Python package using CMake", "body": "## \ud83d\ude80 Feature\r\n<!-- A clear and concise description of the feature proposal -->\r\nUse existing CMake build to create a python .whl file\r\n## Motivation\r\nThe CMake build is clear and well documented. It would be helpful if a corresponding Python WHL package is built using the existing CMake script.\r\n<!-- Please outline the motivation for the proposal. Is your feature request related to a problem? e.g., I'm always frustrated when [...]. If this is related to another GitHub issue, please link here too -->\r\nThe python build is esoteric in nature. This will benefit users who are prone to building CMake using a GUI rather than a command line approach.\r\n## Pitch\r\n\r\n<!-- A clear and concise description of what you want to happen. -->\r\nIf \"build python package\" is selected in CMake, then , during sudo make install, the python files and associated binaries and libraries should be installed in the /usr/local/lib/x86_64/python3/dist_packages folder. Currently, only the \"caffe2\" python binding is installed\r\n## Alternatives\r\n\r\n<!-- A clear and concise description of any alternative solutions or features you've considered, if any. -->\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n", "labels": ["enhancement", "low priority", "module: build", "triaged"], "number_of_comments": 1, "created_at": "2020-02-09 07:36:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562117235": {"author_username": "ptrblck", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33128, "title": "[ROCm] fails on pdist tests", "body": "## \ud83d\udc1b Bug\r\n\r\n1. While working on #31593 we added a test for the backward pass using existing shapes, which fail on ROCm devices as seen in #32758 .\r\n\r\nThe first failure is for an input shape of `[4, 5]`, `p='Inf'` and `dtype=torch.float64` with a max absolute error of  `grad.abs().max(): 1.7263`.\r\n\r\n2. We changed the indexing for `pdist_kernel_cuda_impl` to `int64_t`, since we encountered an overflow for large input shapes ([commit](https://github.com/pytorch/pytorch/pull/31593/commits/43cd34b0fbe872a5e5fd0e31a70596ddf7a7b656)).\r\nA test for this change using a tensor of `[50000, 1]` also fails for ROCm in the forward pass.\r\n\r\n3. To relax the shape limitation for the backward pass, we also changed `pdist_backward_kernel_cuda_impl`. Since 1. fails on ROCm devices without the changes, these particular changes might be unrelated.\r\n\r\nWe've disabled the tests for ROCm for now to unblock #31593\r\n\r\nCC @iotamudelta , @ngimel \r\n", "labels": ["module: rocm", "module: tests", "triaged"], "number_of_comments": 0, "created_at": "2020-02-09 03:53:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562097779": {"author_username": "jettify", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33126, "title": "Add missing weight_decay parameter validation for Adam and AdamW", "body": "Adam and AdamW are missing parameter validation for weight_decay. Other optimisers have this check present.\r\n\r\n", "labels": ["module: optimizer", "open source", "triaged"], "number_of_comments": 0, "created_at": "2020-02-09 00:12:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562075008": {"author_username": "Ed-Swarthout-NXP", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33124, "title": "QNNPACK: GNU aarch64 assembler does not support 4s on neon mov", "body": "Unlike glang, GNU assembler does not support 4s type on neon mov and gives these errors:\r\n\r\n> 8x8-dq-aarch64-neon.S: Assembler messages:\r\n> 8x8-dq-aarch64-neon.S:657:\r\n> Error: operand mismatch -- `mov V8.4s,V9.4s'\r\n> Info:    did you mean this?\r\n> Info:            mov v8.8b, v9.8b\r\n> Info:    other valid variant(s):\r\n> Info:            mov v8.16b, v9.16b\r\n\r\nChanging to 16b fixes the issue.", "labels": ["mobile"], "number_of_comments": 0, "created_at": "2020-02-08 20:40:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562006465": {"author_username": "driounet", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33122, "title": "[feature request] make torch.multinomial behaviour compliant with rnn output dimension", "body": "## Issue description\r\n\r\ntorch.multinomial only works on single and double dimension tensors, without giving ability for choosing probability dimension.\r\nThus, multinomial is not directly usable on rnn output tensor, which has three dimensions (the third one being the probability dimension).\r\n\r\nA practical improvement would be to allow torch.multinomial to work on the third dimension of any 3-dimension tensorr, in order to get the action for every batch sample and every sequence time.\r\n\r\nA more generic improvement would be to make it work regardless of the input tensor dimension, by allowing to choose probability dimension in a new 'dim=' parameter.\r\n\r\nOn the following example, multinomial is used on a 3-dimension tensor.\r\nThere is no crash, but the result is a 1-dimension tensor of size SEQ_SIZE, as if dimension 2 & 3 had been flattened into one single dimension.\r\nThe expected results would have been a SEQ_SIZE  x BATCH_SIZE  tensor.\r\n\r\n## Code example\r\n```python\r\nimport torch\r\n\r\nclass Network(torch.nn.Module):\r\n\r\n    def __init__(self,input_size,hidden_size,output_size):\r\n        super(Network,self).__init__()\r\n        self.gru         = torch.nn.GRU(input_size,hidden_size)\r\n        self.softmax     = torch.nn.functional.softmax\r\n        self.multinomial = torch.multinomial\r\n\r\n\r\n    def forward(self,input,hidden):\r\n        output,hidden = self.gru(input,hidden)\r\n        policy        = self.softmax(output,dim=2)\r\n        action        = self.multinomial(policy,num_samples=1)\r\n        return hidden,policy,action\r\n    \r\nSEQ_SIZE = 100\r\nBATCH_SIZE = 20\r\nINPUT_SIZE = 10\r\nHIDDEN_SIZE = 20\r\nOUTPUT_SIZE = 15\r\n\r\nnetwork = Network(INPUT_SIZE,HIDDEN_SIZE,OUTPUT_SIZE)\r\ninput = torch.rand(SEQ_SIZE,BATCH_SIZE,INPUT_SIZE)\r\nhidden = torch.rand(1,BATCH_SIZE,HIDDEN_SIZE)\r\n\r\nhidden,policy,action = network(input,hidden)\r\n\r\nprint(policy.size())\r\nprint(action.size())\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch version: 1.0.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Professionnel\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.5\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.0.1\r\n[conda] _pytorch_select           1.1.0                       cpu  \r\n[conda] _tflow_select             2.3.0                       mkl  \r\n[conda] blas                      1.0                         mkl  \r\n[conda] libmklml                  2019.0.5                      0  \r\n[conda] mkl                       2019.4                      245  \r\n[conda] mkl-service               2.3.0            py37hb782905_0  \r\n[conda] mkl_fft                   1.0.14           py37h14836fe_0  \r\n[conda] mkl_random                1.1.0            py37h675688f_0  \r\n[conda] pytorch                   1.0.1           cpu_py37h39a92a0_0  \r\n[conda] tensorflow                2.0.0           mkl_py37he1bbcac_0  \r\n[conda] tensorflow-base           2.0.0           mkl_py37hd1d5974_0\n\ncc @fritzo @neerajprad @alicanb @vishwakftw", "labels": ["feature", "module: distributions", "triage review", "triaged"], "number_of_comments": 1, "created_at": "2020-02-08 11:25:37", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "562002543": {"author_username": "cold-pumpkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33121, "title": "Adding description of dtype parameter for tensor.mean().", "body": "The dtype was missing from the description of the parameter\r\nin the tensor.mean() in the Pytorch Guide document.\r\nI added an example and an explanation(optional,\r\nbut floating point only) of the dtype to the document.\r\n\r\nThis PR is related to issue #29758  ", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-08 10:44:56", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561990786": {"author_username": "jithunnair-amd", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33118, "title": "Add ability to enable/disable MIOpen at runtime", "body": "1. Set `torch._C.has_cudnn` to `True` for ROCm\r\n2. Make MIOpen invocations respect value of `cudnn_enabled` or `at::globalContext().userEnabledCuDNN()`\r\n3. `torch/backends/cudnn/__init__.py`: Add hip-specific changes (use \"hide whitespace changes\" option to view simpler diff)", "labels": ["module: rocm", "open source", "triaged"], "number_of_comments": 6, "created_at": "2020-02-08 08:46:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561925472": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33115, "title": "[RFC] Add ability to get all remote parameters when constructing DistributedOptimizer.", "body": "## \ud83d\ude80 Feature\r\nCurrently with model parallel/RPC based training, the user is required to themselves pass in all parameters, as RRefs that participated in the backwards pass and pass those into the DistributedOptimizer. This requires the user to always add 1-2 boilerplate functions to their training code, see the tutorial by @mrshenli (https://pytorch.org/tutorials/intermediate/rpc_tutorial.html). This could be an even worse UX if it is recursive, such as if the forward pass went from node A --> B --> C and the Dist optimizer needs to be created on C.\r\n\r\nAfter a discussion with @mrshenli , we think it might be feasible to provide this API ourselves. It would be tricky since we would need to know all RRefs that have parameters that need to be optimized, and discover those ourselves. The following design was proposed by @mrshenli : \r\n\r\n`get_all_remote_params(model: nn.Module)`\r\n1. model.get_parameters() to create local rrefs\r\n2. vars(model) to get all attributes and pick out all RRef attributes. Then we visit all OwnerRRefs to recursively do this.\r\n\r\nWith this the user could build the DistributedOptimizer without writing code that needs to discover all parameter RRefs themselves.\r\n\r\nInterested in feedback on what this API/implementation could look like.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 23:37:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561911005": {"author_username": "seanprime7", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33114, "title": "Move the custom pass execution back to the beginning of runNondiffOptimization", "body": "## \ud83d\udc1b Bug\r\n\r\nThis is about the JIT compilation.\r\n\r\nhttps://github.com/pytorch/pytorch/pull/29256 moved the execution of custom passes to the end of `runNondiffOptimization`.\r\n\r\nThis change has an adverse effect on a custom pass that we have been working on.  With this change, our custom pass cannot see the graph in the original form and can only see the nodes that are not taken by the fusion pass.  This limits the efficacy of our custom pass.\r\n\r\nI believe that we are not the only one affected by this change.  And due to this issue, we have decided not to move to PyTorch 1.4.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.3.0a0+ee77ccb\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Quadro GV100\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.3.0a0+ee77ccb\r\n[pip] torchvision==0.4.2\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.4 243\r\n[conda] mkl-include 2019.4 243\r\n[conda] mkl-service 2.3.0 py37he904b0f_0\r\n[conda] mkl_fft 1.0.15 py37ha843d7b_0\r\n[conda] mkl_random 1.1.0 py37hd6b4f25_0\r\n[conda] torch 1.3.0a0+ee77ccb dev_0\r\n[conda] torchvision 0.4.2 pypi_0 pypi\r\n\r\ncc @suo @apaszke ", "labels": ["jit", "triaged"], "number_of_comments": 8, "created_at": "2020-02-07 22:47:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561910956": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33113, "title": "Enhance dist_autograd.get_gradients() API to not require a context_id", "body": "## \ud83d\ude80 Feature\r\nContext ids for dist autograd are kind of an implementation detail and not important for the user, but it is useful for users to see gradients that were computed for e.g. debugging purposes. \r\n\r\nCurrently `dist_autograd.get_gradients()` requires the user to pass in a context_id which can be determined from the context manager. However it may be a slightly better experience to support the following, which will hide the detail of context_ids completely:\r\n\r\n```\r\nwith dist_autograd.context():\r\n    dist_autograd.get_gradients()\r\n ```\r\n\r\nThere can be several contexts on a node but we can get the current context by internally calling `_get_current_context()` and fetch the ID from there, like `DistributedOptimizer` does.\r\n\r\n\r\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["enhancement", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 22:47:21", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561907961": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33112, "title": "Document `pickle_save` and `pickle_load`", "body": "These aren't documented anywhere and it's unclear how they're different from `torch::save`", "labels": ["module: docs", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 22:38:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561905732": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33111, "title": "torch.tensor([0.1, 999999999999999999999]) fails with \"Overflow when unpacking double\"", "body": "Work around is to add a decimal to the end of the big number.", "labels": ["enhancement", "small", "topic: operator", "triaged"], "number_of_comments": 2, "created_at": "2020-02-07 22:32:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561904687": {"author_username": "seanprime7", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33110, "title": "InlineAutodiffSubgraphs in JIT inlines non-differentiable custom groups unexpectedly.", "body": "## \ud83d\udc1b Bug\r\n\r\n## Issue description\r\n\r\nIt appears that the `InlineAutodiffSubgraphs` pass inlines non-differentiable custom groups unexpectedly.  As a result, `.backward(...)` cannot be executed on the output from the forward computation.\r\n\r\nTo fix this issue, `canRunWithAutograd` in `torch/csrc/jit/passes/inline_autodiff_subgraphs.cpp` needs to be updated as shown below:\r\n\r\n<pre><code>bool canRunWithAutograd(Node* node) {\r\n  return (node->kind().is_prim() && node->kind() != prim::FusionGroup) ||\r\n      node->kind().is_aten();\r\n}</code></pre>\r\n\r\n## Code example\r\n\r\nConsider the following code example:\r\n<pre><code>model = torchvision.models.resnet18().eval().to(dtype=torch.float, device='cuda')\r\nx = torch.randn(2, 3, 224, 224, dtype=torch.float, device='cuda', requires_grad=True)\r\nx_ = x.clone().detach().requires_grad_(True)\r\ntraced = torch.jit.trace(model, x_, check_trace=False)\r\nout = model(x)\r\ntout = traced(x_)\r\nprint(traced.graph_for(x_))\r\nprint(out)\r\nprint(tout)\r\nout.backward(torch.ones(2, 1000, dtype=float, device='cuda'))\r\ntout.backward(torch.ones(2, 1000, dtype=float, device='cuda'))\r\nprint(x.grad)\r\nprint(x_.grad)\r\n</code></pre>\r\n\r\nWithout a custom pass, this works fine and `tout` is printed out as\r\n<pre><code>tensor([[-1.7074,  0.4159,  1.1330,  ...,  1.2971, -0.7362,  0.6159],\r\n        [-1.7468,  0.4392,  1.1862,  ...,  1.1875, -0.6431,  0.6839]],\r\n       device='cuda:0', grad_fn=&lt;DifferentiableGraphBackward&gt;)\r\n</code></pre>\r\n\r\nBut with a custom pass enabled, I get the error\r\n<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</code></pre>\r\n\r\nand `tout` is printed out without `grad_fn` as\r\n<pre><code>tensor([[ 0.2352,  0.9813, -0.5402,  ..., -1.0934,  0.7645, -1.1978],\r\n        [ 0.4200,  0.8891, -0.5688,  ..., -1.0392,  0.7479, -1.2435]],\r\n       device='cuda:0')\r\n</code></pre>\r\n\r\nWith the custom pass, one of the two `prim::DifferentiableGraph` is inlined in the graph making custom groups appear as differentiable groups.\r\n\r\n## System Info\r\nPyTorch version: 1.3.0a0+ee77ccb\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1.243\r\n\r\nOS: Ubuntu 18.04.4 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: Quadro GV100\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.16.4\r\n[pip] torch==1.3.0a0+ee77ccb\r\n[pip] torchvision==0.4.2\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-include               2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] torch                     1.3.0a0+ee77ccb           dev_0    <develop>\r\n[conda] torchvision               0.4.2                    pypi_0    pypi\r\n\r\n\r\n\r\ncc @suo", "labels": ["actionable", "jit", "module: custom-operators"], "number_of_comments": 4, "created_at": "2020-02-07 22:29:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561874640": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33107, "title": "Modify process group timeout to be infinite when initializing RPC", "body": "## \ud83d\ude80 Feature\r\nCurrently the ProcessGroupAgent uses a ProcessGroup as the backend to send/receive messages, which has a default 30 min timeout that is set during init. This means that if there is no action on the rank during that timeout, then it will crash (with an exception which is being worked on in https://github.com/pytorch/pytorch/pull/32957)\r\n\r\nAs mentioned by @pritamdamania87 in the above PR we should change this to infinite so that it doesn't crash in 30 mins. We could have use cases where we want a PS running infinitely long and no action against it for 30 mins is expected (for example, consider a use case where several trainers train a model for several hours, and then report back parameters to the server).\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 21:12:59", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561853376": {"author_username": "pbelevich", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33106, "title": "Tensor.random_(from, to, ...) should check that from/to are in tensor's dtype bounds", "body": "Like numpy does:\r\n```\r\nnp.random.randint(low=-1000, high=0, size=10, dtype=np.int8)\r\n\r\nValueError: low is out of bounds for int8\r\n\r\nnp.random.randint(low=1000, size=10, dtype=np.int8)\r\n\r\nValueError: high is out of bounds for int8\r\n```\r\n", "labels": ["module: random", "triaged"], "number_of_comments": 1, "created_at": "2020-02-07 20:24:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561843735": {"author_username": "raghuramank100", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33104, "title": "Re-enable internal test runs", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33104 Re-enable internal test runs**\n\nFix internal error message due to old version of hypothesis\n   test_suite = self.load_tests()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/__fb_test_main__.py\", line 678, in load_tests\n    suite = loader.load_all()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/__fb_test_main__.py\", line 467, in load_all\n    __import__(module_name, level=0)\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/test_quantization.py\", line 45, in <module>\n    hu.assert_deadline_disabled()\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/torch/testing/_internal/hypothesis_utils.py\", line 322, in assert_deadline_disabled\n    assert settings().deadline is None\n  File \"/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/buck-out/dev/gen/caffe2/test/quantization#binary,link-tree/hypothesis/_settings.py\", line 127, in __getattr__\n    raise AttributeError('settings has no attribute %s' % (name,))\nAttributeError: settings has no attribute deadline\n\nDifferential Revision: [D19795232](https://our.internmc.facebook.com/intern/diff/D19795232/)", "labels": ["quantization"], "number_of_comments": 0, "created_at": "2020-02-07 20:02:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561752540": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33097, "title": "Stop generating out full function type for registration, use decltype or infer it", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33205 Bring up new-style registration API as wrapper around old-style\n* **#33097 Stop generating out full function type for registration, use decltype or infer it**\n* #33093 Delete unnecessary aliasAnalysis specification from operator registrations.\n\nPreviously, we had to specify full types because the functions we registering\nmight be overloaded, and the type was necessary to resolve the ambiguity.  I\ndisambiguate all of these names by mangling the names of the methods we\nplace on CPUType/CUDAType/TypeDefault with the overload name (these are\n*internal* wrappers which are not user visible), and then can strip\nthe generation of full function types from the registration.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D19837898](https://our.internmc.facebook.com/intern/diff/D19837898)", "labels": [], "number_of_comments": 2, "created_at": "2020-02-07 16:47:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561751508": {"author_username": "jmcarcell", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33096, "title": "Bad performance on 2080 Ti when training a neural network", "body": "## Issue description\r\n\r\nI have 3 machines: A laptop with a GTX 1050, a desktop with a Titan V and a cluster with 10 RTX 2080 Ti. Running the same code (single GPU) with the same data I get much worse performance on the cluster. Some numbers: 360 batches/s on my laptop, 430 on the desktop and 130 on the cluster. The three machines are running the same OS and the same version of pytorch. The model is a simple one with only LSTM and Linear layers. Training a similar model on tensorflow only gives a 10% or 20% difference in training time between the Titan V and the 2080 Ti.\r\nAny ideas?\r\n\r\n I noticed I can run 3 times the neural network training on the same 2080 and then it is effectively doing 130 + 130 + 130 batches/s = ~400. \r\n\r\n## Code example\r\n### Neural network\r\n\r\n```\r\nclass net(nn.Module):\r\n    def __init__(self):\r\n        super(net, self).__init__()\r\n\r\n        self.dense_2_ha = nn.Linear(2, 32)\r\n        self.dense_2_hb = nn.Linear(32, 200)\r\n        self.dense_2_ca = nn.Linear(2, 32)\r\n        self.dense_2_cb = nn.Linear(32, 200)\r\n\r\n        self.lstm1 = nn.LSTM(200, 200, 1, bidirectional=False)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(self, features, series):\r\n\r\n        features_h = self.dense_2_ha(features)\r\n        features_h = self.relu(features_h)\r\n        features_h = self.dense_2_hb(features_h)\r\n        features_h = self.relu(features_h)\r\n        features_h = features_h[None, :, :]\r\n\r\n        features_c = self.dense_2_ca(features)\r\n        features_c = self.relu(features_c)\r\n        features_c = self.dense_2_cb(features_c)\r\n        features_c = self.relu(features_c)\r\n        features_c = features_c[None, :, :]\r\n\r\n        series, hidden = self.lstm1(series, (features_h, features_c))\r\n        out = series\r\n        return out\r\n```\r\n### Fit\r\n```\r\n    def fit(self, data):\r\n        ...\r\n        \r\n\r\n        features_x = torch.tensor(features_x, dtype=torch.float32).to(self.device)\r\n        series_x = torch.tensor(series_x, dtype=torch.float32).to(self.device)\r\n        train_y = torch.tensor(train_y, dtype=torch.float32).to(self.device)\r\n\r\n        epochs = 30\r\n        batch_size = 256\r\n        for i in range(epochs):\r\n            epoch_time = time.time()\r\n            print(f'Epoch {i+1}/{epochs}')\r\n            permutation = torch.randperm(features_x.size()[0])\r\n            for j in range(0, features_x.size()[0], batch_size):\r\n                indices = permutation[j:j+batch_size] \r\n                features, series, batch_y = features_x[indices], series_x[:, indices], train_y[indices]\r\n                self.optimizer.zero_grad()\r\n                out = self.model.forward(features, series)[0, :, :]\r\n                y = batch_y[:, :, 0]\r\n                train_loss = torch.sqrt(((out - y) ** 2).mean())\r\n                train_loss.backward()\r\n                self.optimizer.step()\r\n\r\n```\r\n## System Info (cluster)\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.2.0\r\nCMake version: version 3.16.4\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: \r\nGPU 0: GeForce RTX 2080 Ti\r\nGPU 1: GeForce RTX 2080 Ti\r\nGPU 2: GeForce RTX 2080 Ti\r\nGPU 3: GeForce RTX 2080 Ti\r\nGPU 4: GeForce RTX 2080 Ti\r\nGPU 5: GeForce RTX 2080 Ti\r\nGPU 6: GeForce RTX 2080 Ti\r\nGPU 7: GeForce RTX 2080 Ti\r\nGPU 8: GeForce RTX 2080 Ti\r\nGPU 9: GeForce RTX 2080 Ti\r\n\r\nNvidia driver version: 440.59\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] efficientnet-pytorch==0.5.1\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n```\r\n## System Info (desktop)\r\n\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 9.2.0\r\nCMake version: version 3.16.4\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: TITAN V\r\nNvidia driver version: 440.59\r\ncuDNN version: /usr/lib/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n```\r\n\n\ncc @ngimel @VitalyFedyunin", "labels": ["module: cuda", "topic: performance", "triaged"], "number_of_comments": 4, "created_at": "2020-02-07 16:45:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561725276": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33094, "title": "Delete zero_dim_dispatch_when_scalar", "body": "This is for legacy TH. It is currently used by the following TH functions:\r\n\r\n- [ ] `_th_scatter_` #24621 #24757\r\n- [ ] `_th_fmod` #24565 #24701\r\n- [ ] `_th_remainder` #24753 #24615\n\ncc @ezyang @gchanan @zou3519", "labels": ["better-engineering", "high priority", "module: operators", "small", "topic: porting", "triaged"], "number_of_comments": 4, "created_at": "2020-02-07 15:58:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561718062": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33093, "title": "Delete unnecessary aliasAnalysis specification from operator registrations.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33205 Bring up new-style registration API as wrapper around old-style\n* #33097 Stop generating out full function type for registration, use decltype or infer it\n* **#33093 Delete unnecessary aliasAnalysis specification from operator registrations.**\n\nIn #30187 the aliasAnalysis field on operator registration was updated\nso that alias analysis could be specified in only some registration call\nsites, rather than requiring it be consistently specified in all call\nsites.  With this change, we can eliminate the requirement that all\nregistrations specify aliasAnalysis; as long as we know *one* site\nspecifies the correct aliasAnalysis, we don't have to specify it\nany of the other sites.\n\nIn this patch, the \"one site\" is TypeDefault.cpp (previously we only\ngenerated these stub declarations for manually registered functions,\nbut now we generate the stubs for everything).  Then I delete aliasAnalysis\nanywhere we register an op for an existing function (which is a lot\nof places).\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nDifferential Revision: [D19837897](https://our.internmc.facebook.com/intern/diff/D19837897)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-07 15:46:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561717964": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33092, "title": "Delete redundant aliasAnalysis annotations from VariableType.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33097 Stop generating out full function type for registration, use decltype or infer it\n* #33093 Delete unnecessary aliasAnalysis specification from operator registrations.\n* **#33092 Delete redundant aliasAnalysis annotations from VariableType.**\n* #33011 Beef up documentation on DispatchKey.h\n\nA lot of people look at VariableType to work out how they should\nwrite registrations, so it's important to not put extra goop in\nthem.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>", "labels": [], "number_of_comments": 2, "created_at": "2020-02-07 15:46:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561710779": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33089, "title": "test_baddbmm_cpu_float32 fails locally for me when built with DEBUG=1", "body": "```\r\n======================================================================\r\nFAIL: test_baddbmm_cpu_float32 (__main__.TestTorchDeviceTypeCPU)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_device_type.py\", line 197, in instantiated_test\r\n    result = test(self, device_arg, dtype)\r\n  File \"/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_device_type.py\", line 389, in only_fn\r\n    return fn(slf, device, *args, **kwargs)\r\n  File \"test/test_torch.py\", line 13558, in test_baddbmm\r\n    self.assertEqual(res4, res * 3)\r\n  File \"/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_utils.py\", line 862, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File \"/data/users/ezyang/pytorch-tmp/torch/testing/_internal/common_utils.py\", line 832, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: tensor(1.3351e-05) not less than or equal to 1e-05 : \r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nVersion information:\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.5.0a0+35b1486\r\nIs debug build: Yes\r\nCUDA used to build PyTorch: None\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 7.3.1 20180303 (Red Hat 7.3.1-5)\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.8\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.4\r\n[pip] torch==1.5.0a0+35b1486\r\n[conda] blas                      1.0                         mkl  \r\n[conda] magma-cuda80              2.3.0                         1    soumith\r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py38h962f231_0  \r\n[conda] torch                     1.5.0a0+35b1486           dev_0    <develop>\r\n```\n\ncc @ezyang @gchanan @zou3519", "labels": ["module: tests", "small", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 15:35:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561705086": {"author_username": "JamieMair", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33088, "title": "torch.h not included in LibTorch download.", "body": "## Issue description\r\n\r\nI have downloaded the LibTorch zip from [here](https://pytorch.org/get-started/locally/). I used the 1.4 stable build for windows with CUDA 10.1 support. However the torch folder within the include directory does not contain the \"torch.h\" header file used in many [examples](https://pytorch.org/cppdocs/installing.html#minimal-example). This error comes up when I use the line:\r\n`\r\n#include <Torch/torch.h>\r\n`\r\nThe error simply says \"cannot open torch.h\".\r\n\r\nI am using Microsoft Visual Studio 2019 and I have already added the additional include and library directories. The only \"torch.h\" file I can find is at \"....\\include\\torch\\csrc\\api\\include\\torch\\torch.h\" but this does not include all of the other header files I need such as for the torch::nn::Module class etc.\r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Pro\r\nGCC version: (MinGW.org GCC-6.3.0-1) 6.3.0\r\nCMake version: version 3.15.0-rc2\r\n\r\nPython version: 3.6\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti\r\nNvidia driver version: 431.36\r\ncuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin\\cudnn64_7.dll\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.14.3\r\n[pip3] numpydoc==0.8.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2018.0.2                      1\r\n[conda] mkl-service               1.1.2            py36h57e144c_4\r\n[conda] mkl_fft                   1.0.1            py36h452e1ab_0\r\n[conda] mkl_random                1.0.1            py36h9258bd6_0\r\n\n\ncc @ezyang @gchanan @zou3519 @yf225", "labels": ["high priority", "module: cpp", "triaged"], "number_of_comments": 5, "created_at": "2020-02-07 15:25:53", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561650019": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33086, "title": "F.max_pool1d and  F.min_pool1d should support integer dtypes", "body": "max_pool and min_pool are dilation and erosion operations which can be convenient in postprocessing, so binary masks input can happen in practice\r\n\r\nError: `RuntimeError: \"max_pool2d_with_indices_out_cuda_frame\" not implemented for 'Int'`", "labels": ["enhancement", "module: operators", "proposal accepted", "triaged"], "number_of_comments": 1, "created_at": "2020-02-07 13:52:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561537539": {"author_username": "akashkumar398", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33085, "title": "not able to import *  from fastai.vision  in Google collab", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nrun in Google collab\r\nfrom fastai.vision import *\r\n\r\nError:\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-3-c0e76450f370> in <module>()\r\n----> 1 from fastai.vision import *\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py in <module>()\r\n      6 import torch.nn as nn\r\n      7 import torch.nn.functional as F\r\n----> 8 from torch.jit.annotations import Optional\r\n      9 from torch import Tensor\r\n     10 from .utils import load_state_dict_from_url\r\n\r\nImportError: cannot import name 'Optional'\r\n\r\n", "labels": ["triaged"], "number_of_comments": 7, "created_at": "2020-02-07 10:03:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561475799": {"author_username": "Android0868", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33082, "title": "21250 2", "body": "Yutube ", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-07 07:49:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561432520": {"author_username": "siddheshk", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33081, "title": "DataParallel gives different gradients when using LSTMs", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen using DataParallel on a model with LSTMs the losses obtained compared to the same model run on a single GPU are different. \r\n\r\n## To Reproduce\r\n\r\nHere is a sample code block that seeds everything to ensure the weights are the same.\r\nModel: \r\n``` python\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.input_transform = nn.Linear(100, 512)\r\n        self.lstm = nn.LSTM(512,512,batch_first=True)\r\n        self.output_transform = nn.Linear(512, 10)\r\n    def forward(self, x):\r\n        self.lstm.flatten_parameters()\r\n        input_t = self.input_transform(x)\r\n        output, hidden = self.lstm(input_t)\r\n        return self.output_transform(hidden[0][0])\r\n```\r\n\r\nRunning on a single GPU\r\n```python\r\ntorch.manual_seed(1234)\r\nnp.random.seed(1234)\r\ndevice = torch.device(\"cuda:0\")\r\nmodel = Model().to(device)\r\noptimizer = torch.optim.Adam(model.parameters())\r\n\r\nfor i in range(5):\r\n    input = torch.rand(128,10,100).to(device)\r\n    output = torch.from_numpy(np.random.randint(10, size=(128,))).to(device)\r\n    pred = model(input)\r\n    loss = nn.functional.cross_entropy(pred, output)\r\n    print (loss)\r\n\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\nThese are the losses:\r\n```\r\ntensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3364, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3563, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.2963, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3315, device='cuda:0', grad_fn=<NllLossBackward>)\r\n```\r\n\r\nRunning on 4 P100s\r\n```python\r\ntorch.manual_seed(1234)\r\nnp.random.seed(1234)\r\ndevice = torch.device(\"cuda:0\")\r\nmodel = Model().to(device)\r\nmodel = DataParallel(model)\r\noptimizer = torch.optim.Adam(model.parameters())\r\n\r\nfor i in range(5):\r\n    input = torch.rand(128,10,100).to(device)\r\n    output = torch.from_numpy(np.random.randint(10, size=(128,))).to(device)\r\n    pred = model(input)\r\n    loss = nn.functional.cross_entropy(pred, output)\r\n    print (loss)\r\n\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n```\r\nThese are the results:\r\n```\r\ntensor(2.3043, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3137, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.2964, device='cuda:0', grad_fn=<NllLossBackward>)\r\ntensor(2.3243, device='cuda:0', grad_fn=<NllLossBackward>)\r\n```\r\n\r\nThis behaviour is on the stable version of PyTorch 1.4. I face convergence issues on my actual code that uses multiple LSTMs when using dataparallel.\r\nThings work fine when I downgrade to PyTorch 1.2. \r\n\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Ubuntu 18.04.2 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla P100-PCIE-12GB\r\nGPU 1: Tesla P100-PCIE-12GB\r\nGPU 2: Tesla P100-PCIE-12GB\r\nGPU 3: Tesla P100-PCIE-12GB\r\n\r\nNvidia driver version: 418.40.04\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2020.0                      166\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.4.0           py3.7_cuda10.0.130_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.5.0                py37_cu100    pytorch\r\n```", "labels": ["module: data parallel", "topic: determinism", "triaged"], "number_of_comments": 1, "created_at": "2020-02-07 05:39:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561418174": {"author_username": "ajyu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33079, "title": "[jit] Pretranspose addmm", "body": "Summary:\nJit transform to call contiguous on weight input to addmm, if it is a constant tensor.\nThis diff is implemented on top of frozen module.\n\nGraph before diff: P125668807\nGraph after diff (it's the same graph compared to only running freezing): P125679499\n\nTest Plan:\n```buck test caffe2/test:jit -- 'test_pretranspose_weights'```\n\nRun with ptvsc2 benchmark\n```MKL_NUM_THREADS=1 OMP_NUM_THREADS=1 numactl -m 0 -C 3 ./ptvsc2_predictor_bench --scripted_model=/data/ansha/snn_bench_125_frozen_cont/traced_model.pt --pt_inputs=/data/ansha/snn_bench_125_frozen_cont/batch_size_40/container.pt --iters=3000 --warmup_iters=100 --num_threads=1```\n\nOn SKL T6, 1 thread, bs=40, tiny model\nwithout diff: 24.2807 ms/iter\nwith diff: 20.5979 ms/iter\n\nDifferential Revision: D19651972\n\n", "labels": ["fb-exported", "jit"], "number_of_comments": 2, "created_at": "2020-02-07 04:46:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561362389": {"author_username": "zasdfgbnm", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33073, "title": "cuDNN convolution try multiple algo", "body": "Fixes: https://github.com/pytorch/pytorch/issues/31336 https://github.com/pytorch/pytorch/issues/1664\r\n\r\nSometimes cuDNN heuristics return algorithms that can not be used. Instead of just using the first algorithm returned, we should try these algorithms one by one until one of them succeed.\r\n\r\nBenchmark:\r\nhttps://github.com/zasdfgbnm/things/blob/master/2020Q1/conv-benchmark.ipynb\r\n```python\r\ni = torch.randn(256, 3, 256, 256).cuda()\r\nc = torch.nn.Conv2d(3, 3, 3, 3).cuda()\r\n\r\n%timeit c(i); torch.cuda.synchronize()\r\n```\r\nbefore vs after = 498 vs 490 \u00b5s\r\n\r\nThe performance is improved I guess because, before this PR, we always call the heuristics to get the algorithm, but after this PR, we only do at the first time.", "labels": ["open source", "triaged"], "number_of_comments": 5, "created_at": "2020-02-07 01:11:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561361435": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33072, "title": "Build a generic failure injection mechanism for RPC", "body": "## \ud83d\ude80 Feature\r\nDue to the distributed, multi-process nature of the RPC framework it can be challenging to test certain functionality, such as timeouts, error handling, and error conditions that arise from separate threads of execution. \r\n\r\nIt might be thus useful to have a generic failure injection mechanism where we could specify which, if any, failures to insert into the RPC agent when writing a unit test. This might reduce some of the hacks/methods that we have in the codebase that are currently unit test specific (though might also add some test-specific code hooks in the codebase).\r\n\r\nOne example where this could have been useful is the tests for the PR that added exception handling in `ProcessGroupAgent::enqueueSend()` (https://github.com/pytorch/pytorch/pull/31023), where should be able to insert a failure injection which causes errors that we can assert against.\r\n\r\nI can also see this being useful in https://github.com/pytorch/pytorch/pull/32602 that is implementing retryability for RPCs.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "module: tests", "triaged"], "number_of_comments": 0, "created_at": "2020-02-07 01:07:39", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561327957": {"author_username": "tmulc18", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33066, "title": "Fixed typo in comment.", "body": "", "labels": ["open source", "triaged"], "number_of_comments": 0, "created_at": "2020-02-06 23:20:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561326067": {"author_username": "v0dro", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33065, "title": "Atomic CPU operations for parallel addition, subtraction, multiplication and division.", "body": "## \ud83d\ude80 Feature\r\n\r\nPortable parallel atomic aritmetic operations like addition, subtraction, division and multiplication\r\nfor CPU.\r\n\r\n## Motivation\r\n\r\nScatter reduction operations came up in issue https://github.com/pytorch/pytorch/issues/22378 . Performing\r\nthese operations safely in parallel requires the use of atomic operations for performing various\r\noperations like addition, subtraction, division and multiplication. @nikitaved started porting\r\nthe `scatter_add` from TH to ATen in https://github.com/pytorch/pytorch/pull/31662. In this issue,\r\nhe implements a new kernel using `TensorIterator` called [cpu\\_scatter\\_gather\\_base\\_kernel](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L92),\r\nwhich currently cannot run in parallel since atomic operations in parallel would be buggy.\r\n\r\nThe issue raised by @ngimel regarding documentation for the scatter and scatter add methods (https://github.com/pytorch/pytorch/issues/31776)\r\nraises some interesting issues regarding use of non-unique indices for scatter operations.\r\nIf these operations are to made parallel, it very important that operations involving the\r\nsame index are atomic so that multiple threads accessing the same space in memory perform\r\ntheir operations correctly.\r\n\r\n## Pitch\r\n\r\nCurrently the `scatter_add` operation on CPU is serial due to lack of a `atomicAdd` operation\r\nsimilar to what exists in the [CUDA backend](https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCAtomics.cuh). Implementing this operation on CPU\r\nshould open up possibilities for paralellization of scatter reductions (not restricted to addition).\r\n\r\n## Additional context\r\n\r\nEventually, we want to implement deterministic scatter operations in pytorch (https://github.com/pytorch/pytorch/issues/23151),\r\nand performing these in parallel will of course require the use of atomic operations.\r\n\r\nAdditionally, @nikitaved has implemented fast gather operations (https://github.com/pytorch/pytorch/pull/32425), and he says that since the\r\n`scatter_add` is in the backward of gather, and gather is parallel, having a `scatter_add`\r\nthat is parallel is an important need.\r\n\n\ncc @VitalyFedyunin @ngimel", "labels": ["module: cpu", "topic: performance", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 23:15:10", "reactions": {"total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561322165": {"author_username": "krshrimali", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33063, "title": "[WIP] Adding dtype argument to the Unary Ops for dtype promotion (testing on expm1 function)", "body": "This PR is currently WIP and is intended for discussion on adding dtype argument for the type promotion support of Unary Ops. This PR:\r\n\r\n1. Adds `expm1` overload with `dtype` argument. \r\n2. Modifies `TensorIterator::unary_op` to have promoting argument (default to false, `CommonDTypeStrategy::CHECK`) - if true, then changes the flag to `PROMOTE`. \r\n3. Modifies the existing helper functions for Unary Ops (`unary_op_impl` and `unary_op_impl_out`).\r\n\r\nThe objective is to allow the Unary Ops to have a `dtype` argument, to allow the internal promotion logic to promote the dtype if required (and allowed). The question is, if these changes have any unexpected (and unwanted) effect on the current flow of PyTorch.\r\n\r\ncc: @mcarilli @nairbv ", "labels": ["open source"], "number_of_comments": 2, "created_at": "2020-02-06 23:04:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561239990": {"author_username": "jianyuh", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33059, "title": "PR 33000", "body": "For test of https://github.com/pytorch/pytorch/pull/33000", "labels": [], "number_of_comments": 1, "created_at": "2020-02-06 20:05:19", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561234321": {"author_username": "vainaijr", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33058, "title": "clip_grad_norm negative", "body": "https://github.com/pytorch/pytorch/issues/31367\r\n\r\n", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-06 19:54:21", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561233849": {"author_username": "jamesr66a", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33057, "title": "[WIP] Move profiler to a dispatch wrapper", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33057 [WIP] Move profiler to a dispatch wrapper**\n\r\nThis makes it so that the profiler is not called from the VariableType wrappers, but rather from a boxed wrapper that calls RECORD_FUNCTION, then redispatches to the underlying operator.\r\n\r\nQuestions:\r\n\r\n1. In general, does this look like a reasonable approach?\r\n2. There seems to be the concept of \"unboxed only\" kernels, would those work with this system?\r\n3. (probably a broader design question) Can we further extend this to include stuff like `prim::` ops? (which currently are sometimes/piecemeal supported in profiling)\r\n\r\nDifferential Revision: [D19775659](https://our.internmc.facebook.com/intern/diff/D19775659)", "labels": [], "number_of_comments": 10, "created_at": "2020-02-06 19:53:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561230361": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33055, "title": "[docs] Strange order of items in docs contents in left pane", "body": "```\r\ntorch.optim\r\nQuantization\r\nDistributed RPC Framework\r\ntorch.random\r\n```\r\nwhy this order? not even alphabetical. I'd suggest that big special topics such as Quantization are presented on top in a separate section, and the rest to be in alphabetical order.\r\n\r\n![image](https://user-images.githubusercontent.com/1041752/73972687-8c035b00-4921-11ea-82a9-01c4d09cc5b3.png)", "labels": ["module: docs", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 19:46:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561205580": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33054, "title": "Migrate `masked_select` from TH to ATen (CUDA)", "body": "Porting TH operators is essential for code simplicity and performance reasons.\r\n\r\nPorting guides and Q&A are available in umbrella issue: #24507\r\n\r\nFeel free to add @VitalyFedyunin as a reviewer to get a prioritized review.\r\n\r\ncc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko as this affects complex tensor bring up", "labels": ["better-engineering", "high priority", "module: complex", "module: operators", "topic: porting", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 18:56:59", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561204862": {"author_username": "ezyang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33053, "title": "Migrate `masked_select` from TH to ATen (CPU)", "body": "Porting TH operators is essential for code simplicity and performance reasons.\r\n\r\nPorting guides and Q&A are available in umbrella issue: #24507\r\n\r\nFeel free to add @VitalyFedyunin as a reviewer to get a prioritized review.\r\n\r\ncc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko  as this affects complex tensor bring up", "labels": ["better-engineering", "high priority", "module: complex", "module: operators", "topic: porting", "triaged"], "number_of_comments": 2, "created_at": "2020-02-06 18:55:30", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561182310": {"author_username": "zhaojuanmao", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33052, "title": "make script module object to pass over wire in rpc layer ", "body": "Right now there is no way to pass  script module object over wrie in rpc layer. \r\n\r\n1. When calling python myScriptModule constructor as follows, myScriptModule will be serialized by python pickler, but python pickler does not support scriptModule object serialization \r\nret = rpc_sync(work1, myScriptModule, args=()) \r\nor rref = remote(work1, myScriptModule, args=()), rref.to_here()\r\n\r\n2. When calling torchscript function to construct myScriptModule as follows, myScriptModule will be serialized by JIT pickler internally when we wrap the data to Message, but JIT pickler complained scriptModule is not supported type\r\n\r\n@torch.jit.script\r\ndef construct_my_script_module():\r\n   myScriptModule()\r\n\r\nret = rpc_sync(work1, construct_my_script_module, args=()) \r\nor rref = remote(work1, construct_my_script_module, args=()), rref.to_here()\r\n\r\n**Solution**\r\nWe need to do special handling for script module object by calling torch.save() and torch.load() before dumping it to Message and pass it over wire\r\n\r\n\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-06 18:11:00", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561163315": {"author_username": "ahyunSeo", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33051, "title": "Recover from CUDA runtime error", "body": "## \u2753 Questions and Help\r\n\r\nI wanted to recover from \r\nRuntimeError: inverse_cuda: For batch 0: U(2344,2344) is zero, singular U.\r\nso I tried to catch the error and do \r\ntorch.cuda.empty_cache()\r\nwhich leads to the another error\r\nRuntimeError: CUDA error: misaligned address\r\nwithout empty cache I got the illegal memory access error below.\r\nAny suggestions?\r\nI use CUDA 10 pytorch 1.1.0~ TitanXP *4\r\nAlso I use dataparallel (not distributed one) during training.\r\nThanks a lot.\r\n\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/src/sgetrf_batched.cpp:213\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/src/sgetrf_batched.cpp:214\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_sgetrf_batched at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/src/sgetrf_batched.cpp:215\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_sgetri_outofplace_batched at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/src/sgetri_outofplace_batched.cpp:137\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/interface_cuda/interface.cpp:944\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/interface_cuda/interface.cpp:945\r\nCUDA runtime error: an illegal memory access was encountered (77) in magma_queue_destroy_internal at /opt/conda/conda-bld/magma-cuda100_1549065924616/work/interface_cuda/interface.cpp:946\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @ngimel", "labels": ["module: cuda", "module: third_party", "triaged"], "number_of_comments": 2, "created_at": "2020-02-06 17:33:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561121403": {"author_username": "stereomatchingkiss", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33049, "title": "Warning when link libtorch and opencv4.2.0 together", "body": "## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Download official opencv4.2.0\r\n2. Download libtorch1.4.0 of windows 64 bits, cpu version\r\n3. Create CMakeLists.txt\r\n\r\n```\r\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\r\n     \r\nproject(person_reid)    \r\n     \r\nset(LIBTORCH_PATH ${CMAKE_CURRENT_LIST_DIR}/../../3rdLibs/libtorch/libtorch_cpu_1_4_0/)\r\nset(OPENCV_CV_PATH ${CMAKE_CURRENT_SOURCE_DIR}/../../3rdLibs/opencv/opencv_4_2_0/opencv/build/)\r\n\r\nset(CMAKE_PREFIX_PATH \"${OPENCV_CV_PATH};${LIBTORCH_PATH};${CMAKE_PREFIX_PATH};\")\r\n\r\nfind_package(OpenCV REQUIRED)\r\nfind_package(Torch REQUIRED)\r\n\r\nfile(GLOB UTILS_SRC\r\n    \"*.hpp\"\r\n    \"*.cpp\"\r\n)\r\n     \r\nadd_executable(person_reid ${UTILS_SRC})\r\ntarget_link_libraries(person_reid \"${TORCH_LIBRARIES};${OpenCV_LIBS}\")\r\nset_property(TARGET person_reid PROPERTY CXX_STANDARD 11)\r\n\r\n\r\n```\r\n\r\n4. Codes\r\n\r\n```\r\n#include <opencv2/core.hpp>\r\n#include <opencv2/highgui.hpp>\r\n#include <opencv2/imgproc.hpp>\r\n\r\n#include <iostream>\r\n#include <memory>\r\n\r\n//#undef slots\r\n#include <torch/script.h>\r\n//#def slots Q_SLOTS\r\n\r\nint main()try\r\n{\r\n    cv::Mat img = cv::imread(\"some.jpg\");\r\n    // Deserialize the ScriptModule from a file using torch::jit::load().\r\n    torch::jit::script::Module module = torch::jit::load(\"traced_osnet_ain_x1_0.pt\");   \r\n}catch(std::exception const &ex){\r\n    std::cerr << ex.what() << \"\\n\";\r\n    return -1;\r\n}\r\n\r\n```\r\n5. Build with release version\r\n\r\n## Expected behavior\r\n\r\nCompile without any warnings\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\n - PyTorch Version: 1.4.0\r\n - OS (e.g., Linux): windows 10 64 bits\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.6.7\r\n - CUDA/cuDNN version: none, cpu only\r\n - GPU models and configuration: use libtorch with cpu only\r\n - Compiler vc2017 64bits\r\n\r\n## Warnings\r\n\r\n```\r\ncl : Command line warning D9025 : overriding '/EHs' with '/EHa'\r\nC:\\Users\\yyyy\\programs\\Qt\\3rdLibs\\libtorch\\libtorch_cpu_1_4_0\\include\\torch/csrc/jit/tracer.h(289): warning C4273: 'torch::jit::tracer::addInputs': inconsistent dll linkage\r\nC:\\Users\\yyyy\\programs\\Qt\\3rdLibs\\libtorch\\libtorch_cpu_1_4_0\\include\\torch/csrc/jit/tracer.h(277): note: see previous definition of 'addInputs'\r\nC:\\Users\\yyyy\\programs\\Qt\\3rdLibs\\libtorch\\libtorch_cpu_1_4_0\\include\\torch/csrc/jit/tracer.h(296): warning C4273: 'torch::jit::tracer::addInputs': inconsistent dll linkage\r\nC:\\Users\\yyyy\\programs\\Qt\\3rdLibs\\libtorch\\libtorch_cpu_1_4_0\\include\\torch/csrc/jit/tracer.h(283): note: see previous definition of 'addInputs'\r\n```", "labels": ["better-engineering", "module: build", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 16:21:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561096493": {"author_username": "pbelevich", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33048, "title": "torch::allclose() should support bool tensors", "body": "Currently torch::allclose() fails with\r\n`C++ exception with description \"Subtraction, the `-` operator, with a bool tensor is not supported. If you are trying to invert a mask, use the `~` or `logical_not()` operator instead. (sub_check at ../aten/src/ATen/native/BinaryOps.h:24)`\r\non bool tensors\n\ncc @yf225 @izdeby", "labels": ["enhancement", "module: cpp", "topic: boolean tensor", "triaged"], "number_of_comments": 0, "created_at": "2020-02-06 15:43:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561091706": {"author_username": "IlichevSergey", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33047, "title": "torch.nn.functional import grid_sample", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\ntorch.nn.functional.grid_sample() behavior is differ from pytorch 1.2.0\r\n\r\n## Code example\r\n\r\npytorch 1.2.0:\r\ngrid_sample(input=inp, grid=grid, mode='bilinear', padding_mode='border')\r\n\r\npytorch 1.4.0:\r\nthe correct way to reproduce behavior of 1.2.0 is:\r\ngrid_sample(input=inp, grid=grid, mode='bilinear', padding_mode='border', align_corners=True)\r\nbut the default value of align_corners is None, so that produces some errors when update to pytorch 1.4 from 1.2\r\n\r\n\r\n## System Info\r\n\r\nPyTorch 1.2.0:\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.2.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0\r\nCMake version: version 3.15.5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.96\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-memlab==0.0.2\r\n[pip] torch==1.2.0\r\n[pip] torchvision==0.4.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] libmklml                  2019.0.3                      0\r\n[conda] mkl                       2019.5                      281    anaconda\r\n[conda] mkl-service               2.3.0            py36hb782905_0    anaconda\r\n[conda] mkl_fft                   1.0.12           py36h14836fe_0\r\n[conda] mkl_random                1.0.2            py36h343c172_0\r\n[conda] pytorch                   1.2.0           py3.6_cuda100_cudnn7_1    pytorch\r\n[conda] pytorch-memlab            0.0.2                    pypi_0    pypi\r\n[conda] torchvision               0.4.0                py36_cu100    pytorch\r\n\r\n\r\nPyTorch 1.4.0:\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0\r\nCMake version: version 3.15.5\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.105\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 418.96\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] pytorch-memlab==0.0.2\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl\r\n[conda] libmklml                  2019.0.3                      0\r\n[conda] mkl                       2019.5                      281    anaconda\r\n[conda] mkl-service               2.3.0            py36hb782905_0    anaconda\r\n[conda] mkl_fft                   1.0.12           py36h14836fe_0\r\n[conda] mkl_random                1.0.2            py36h343c172_0\r\n[conda] pytorch                   1.4.0           py3.6_cuda101_cudnn7_0    pytorch\r\n[conda] pytorch-memlab            0.0.2                    pypi_0    pypi\r\n[conda] torchvision               0.5.0                py36_cu101    pytorch", "labels": ["needs reproduction", "triaged"], "number_of_comments": 2, "created_at": "2020-02-06 15:35:41", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561087534": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33046, "title": "F.one_hot to return BoolTensor/ByteTensor instead of LongTensor?", "body": "What is the reason for F.one_hot to return a rather large int64 type intead of int8 types such as Bool or Byte?\r\nIts values are always 0/1 and with existing type promotion, manipulation with Bool/Byte in cross-entropy type formulas should be ok.\r\n\r\nRelated issue: https://github.com/pytorch/pytorch/issues/15457\r\nRelated PR about specifying dtype in one_hot call: https://github.com/pytorch/pytorch/pull/29672. The difference of this issue is about changing the default dtype.", "labels": ["enhancement", "topic: operator", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 15:29:08", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "561071950": {"author_username": "zou3519", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33045, "title": "Build ahead-of-time C++ extensions with ninja on windows", "body": "## \ud83d\ude80 Feature\r\n\r\nRight now, C++ extensions support building with ninja on unix-based systems. This task is to enable that support for windows. See #32495 for the PR that added support for building with ninja on unix-based systems.\r\n\n\ncc @yf225 @peterjc123", "labels": ["module: cpp-extensions", "module: windows", "triaged"], "number_of_comments": 3, "created_at": "2020-02-06 15:05:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560971348": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33042, "title": "[feature request] torch.cat to apply to as_tensor to inputs", "body": "This is useful for some easy padding scenarios. Having it infer device would be extra-nice.\r\n\r\n```\r\n>>> np.concatenate(([False], np.ones((3,), dtype = np.bool), [False] ))\r\narray([False,  True,  True,  True, False])\r\n\r\n>>> torch.cat(([False], torch.ones((3,), dtype = torch.bool), [False] ))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: expected Tensor as element 0 in argument 0, but got list\r\n\r\n>>> torch.cat((torch.as_tensor([False]), torch.ones((3,), dtype = torch.bool), torch.as_tensor([False]) ))\r\ntensor([False,  True,  True,  True, False])\r\n\r\n>>> torch.cat((torch.as_tensor([False]), torch.ones((3,), dtype = torch.bool, device = 'cuda'), torch.as_tensor([False]) ))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Expected object of backend CUDA but got backend CPU for sequence element 0 in sequence argument at position #1 'tensors'\r\n\r\n```", "labels": ["module: operators", "triaged"], "number_of_comments": 2, "created_at": "2020-02-06 12:08:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560953876": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33041, "title": "bytearray(tensor) behaves very differently from bytearray(tensor.numpy())", "body": "```python\r\nimport torch\r\n\r\na = torch.ShortTensor([10000])\r\nprint(bytearray(a.numpy()))\r\n# bytearray(b\"\\x10\\'\")\r\n\r\nprint(bytearray(a))\r\n# bytearray(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00 ............')\r\n```\r\n\r\nI thought this is the Python prescribed interface of getting byte representation of some object, but this fails with PyTorch arrays. My goal is to pass the obtained byte array or byte string to webrtcvad method working with audio byte frames.\r\n\r\nRelated: https://discuss.pytorch.org/t/getting-tensor-raw-data/18961/2", "labels": ["enhancement", "module: numpy", "module: operators", "small", "triaged"], "number_of_comments": 1, "created_at": "2020-02-06 11:33:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560948835": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33040, "title": "[debatable] Better infer dtype in torch.as_tensor", "body": "Supporting generator inputs in `torch.tensor` / `torch.as_tensor` would save some keystrokes to the user. Converting the input to list internally would do the trick. On the contrary note: numpy does not support this.\r\n\r\n```\r\n>>> torch.as_tensor(list(False for i in range(3)))\r\ntensor([False, False, False])\r\n\r\n>>> torch.as_tensor([False for i in range(3)])\r\ntensor([False, False, False])\r\n\r\n>>> torch.as_tensor(False for i in range(3))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: Could not infer dtype of generator\r\n```\r\n\r\nThe relevant line is in https://github.com/pytorch/pytorch/blob/62b06b9fae9b151b20d25c8e54ae9ae1081f23ad/torch/csrc/utils/tensor_new.cpp#L200", "labels": ["enhancement", "topic: operator", "triaged"], "number_of_comments": 5, "created_at": "2020-02-06 11:23:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560858695": {"author_username": "jecampagne", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33037, "title": "RuntimeError: tensor does not have a device?", "body": "## \ud83d\udc1b Bug\r\n\r\n\r\nRuntimeError: tensor does not have a device\r\n\r\n## To Reproduce\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import grad\r\n\r\nx= torch.randn(2,5,64,64,dtype=torch.float)\r\nx.requires_grad = True\r\nx1 = nn.Conv2d(5,64,5, padding=2, bias=True)(x)\r\nx1 = nn.PReLU(64, init=0.25)(x1)\r\nx2 = nn.Conv2d(5,32,3, padding=1, bias=True)(x)\r\nx2 = nn.PReLU(32, init=0.25)(x2)\r\nout = torch.cat((x1, x2), dim=1)\r\nout = nn.Linear(64,1)(out)\r\nout = nn.Sigmoid()(out)\r\ng, = grad(out.sum(), x, create_graph=True)\r\ngrad(g.sum(), x)\r\n```\r\n\r\n\r\n\r\n\r\n\r\n## Environment\r\n\r\n\r\n## Additional context\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux release 7.7.1908 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.2.1\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.2.1                    py37_0\r\n<!-- Add any other context about the problem here. -->\r\n\r\n\r\n```\r\nI have run with a recent night build '1.5.0.dev20200204'` and the problem  rises a different error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"//anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\", line 157, in grad\r\n    inputs, allow_unused)\r\nRuntimeError: tensor does not have a device (device at /opt/conda/conda-bld/pytorch_1580803991946/work/c10/core/TensorImpl.h:463)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x4e (0x2b5fdb439f5e in .../anaconda3/lib/python3.7/site-packages/torch/lib/libc10.so)\r\nframe #1: at::Tensor::options() const + 0x1ef (0x2b5fa2511adf in .../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #2: torch::autograd::generated::SliceBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0xea (0x2b5fac25fa9a in.../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #3: <unknown function> + 0x2e6e26d (0x2b5fac9da26d in .../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #4: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x152b (0x2b5fac9d675b in .../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #5: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x588 (0x2b5fac9d7bf8 in.../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #6: torch::autograd::Engine::thread_init(int) + 0x39 (0x2b5fac9cee19 in .../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\r\nframe #7: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x2b5fa28e0e38 in .../anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\r\nframe #8: <unknown function> + 0xc819d (0x2b5fa011919d in .../anaconda3/lib/python3.7/site-packages/torch/lib/../../../.././libstdc++.so.6)\r\nframe #9: <unknown function> + 0x7e65 (0x2b5f815e2e65 in /lib64/libpthread.so.0)\r\nframe #10: clone + 0x6d (0x2b5f818f588d in /lib64/libc.so.6)\r\n```\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen", "labels": ["module: autograd", "topic: double backwards", "triaged"], "number_of_comments": 2, "created_at": "2020-02-06 08:40:58", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560713764": {"author_username": "lshamis", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33035, "title": "Support batch linear transformation", "body": "## \ud83d\ude80 Feature\r\nhttps://github.com/pytorch/pytorch/blob/1b746b95fb6f3df45bd2883045de9fbd82b72100/torch/nn/functional.py#L1362\r\nexpects the weight and bias tensors to have shapes `(out_features, in_features)`  and `(out_features)`, respectively. We would like for this function to optionally support `(N, out_features, in_features)` and `(N, out_features)`, as well.\r\n\r\nThis request extends to other `torch.nn.functional` functions, but this is the painful one to us right now.\r\n\r\n## Motivation\r\n\r\nIt's currently a bit of a pain to use `torch.nn.functional` with batch networks.\r\n\r\n## Pitch\r\n\r\nThis request is effectively asking to replace \r\nhttps://github.com/pytorch/pytorch/blob/1b746b95fb6f3df45bd2883045de9fbd82b72100/torch/nn/functional.py#L1379 with\r\n```py\r\noutput = input.matmul(weight.transpose(-1, -2))\r\n```\r\n\r\n## Alternatives\r\n\r\nWriting our own `torch.nn.functional.linear`\r\n\r\n## Additional context\r\n\r\nIn writing this request, github helpfully pointed out https://github.com/pytorch/pytorch/issues/7500 as being similar. I suppose this is a request to take that beyond algebra operators and into nn operators.", "labels": ["feature", "module: operators", "topic: batching", "triaged"], "number_of_comments": 0, "created_at": "2020-02-06 00:47:48", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560712484": {"author_username": "bzinodev", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33034, "title": "Scripting fails to preserve attribute aliasing", "body": "Script fails to preserve aliasing between attributes. As shown in example below:\r\n\r\n    def test_module_with_aliased_attr(self):\r\n        class M(torch.nn.Module):\r\n            def __init__(self):\r\n                super(M, self).__init__()\r\n                self.a = [1, 2, 3, 4, 5, 6]\r\n                self.b = self.a\r\n\r\n            def forward(self, x):\r\n                self.b[1] += 10\r\n                return self.a\r\n\r\n        m = M()\r\n        m_s = torch.jit.script(m)\r\n        m_s.eval()\r\n        inp = torch.tensor([5])\r\n        out = m_s.forward(inp)\r\n        expected = m.forward(inp)\r\n        print(\"expected: \", expected, \"out \", out)\r\n        self.assertEqual(out, expected)\r\n\r\nPrints:  expected:  [1, 12, 3, 4, 5, 6] out  [1, 2, 3, 4, 5, 6]\r\n\n\ncc @suo", "labels": ["jit", "triaged"], "number_of_comments": 1, "created_at": "2020-02-06 00:43:02", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560712310": {"author_username": "glaringlee", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33033, "title": "ChannelsLast3d support is_contiguous, contiguous, suggest_memory_format, caching", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33033 ChannelsLast3d support is_contiguous, contiguous, suggest_memory_format, caching**\n\r\n- Support the following features for Channels Last 3d\r\n  is_contiguous()\r\n  contiguous()\r\n  suggest_memory_format\r\n  empty()\r\n  empty_like()\r\n  caching.\r\n  etc. (related functions that depends on above)\r\n- A bug fix\r\n  see is_channels_last_strides() for details\r\n\r\nAllow 5d channels last memory format. (dim() == 5 only for this PR) \r\nAdded all test cases.\r\n\r\nDifferential Revision: [D19759661](https://our.internmc.facebook.com/intern/diff/D19759661)", "labels": [], "number_of_comments": 1, "created_at": "2020-02-06 00:42:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560711475": {"author_username": "eholly1", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33032, "title": "Preserve Lengths Device in `pack_padded_sequence`", "body": "If the default tensor type is cuda, and this function gets called, it will internally move `lengths` from cpu to cuda, which the function subsequently complains about.\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/pytorch/pytorch/33032)\n<!-- Reviewable:end -->\n", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-06 00:38:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560687889": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33030, "title": "[profiler] for async RPC, use separate api in RecordFunction to handle nested scopes.", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33030 [profiler] for async RPC, use separate api in RecordFunction to handle nested scopes.**\n\r\nWe previously had an issue (described in https://github.com/pytorch/pytorch/issues/32517, https://github.com/pytorch/pytorch/issues/32884) where async RPCs would not be profiled correctly if there were nested scopes during profiling (nested scopes can be created by `with torch.autograd.profiler.record_function(\"foo\")...`)\r\n\r\nThe cause of this was that the logic for handling nested scopes was not correct when we invoked `RecordFunction::end()` from a separate thread.\r\n\r\nAs proposed in https://github.com/pytorch/pytorch/issues/32884, this change adds 2 APIs to RecordFunction:\r\n`runEndCallbacks()` which just runs the end callbacks for profiling, and doesn't reset thread local RecordFunction data\r\n`resetThreadLocalFunc` that resets the thread local RecordFunction data.\r\n\r\nFor profiling RPCs, we call `resetThreadLocalFunc` in the creating thread to avoid the above issue.\r\ntested by modifying unit test to add the nested scope.\r\n\r\nDifferential Revision: [D19739387](https://our.internmc.facebook.com/intern/diff/D19739387/)", "labels": [], "number_of_comments": 7, "created_at": "2020-02-05 23:22:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560685818": {"author_username": "junjimei", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33029, "title": "Torch not compiled with CUDA enabled", "body": "**Hi, I'm trying to run this code from the following repository (https://github.com/CorentinJ/Real-Time-Voice-Cloning), I already installed all requirements, however I'm getting this error when I run the demo_cli.py:**\r\n\r\nAssertionError: Torch not compiled with CUDA enabled\r\n\r\n\r\n**This is the full text I get when I run it:**\r\n\r\nrunfile('C:/Users/isabel/Desktop/VOICE CLONER/Real-Time-Voice-Cloning-master/demo_cli.py', wdir='C:/Users/isabel/Desktop/VOICE CLONER/Real-Time-Voice-Cloning-master')\r\nArguments:\r\n    enc_model_fpath:   encoder\\saved_models\\pretrained.pt\r\n    syn_model_dir:     synthesizer\\saved_models\\logs-pretrained\r\n    voc_model_fpath:   vocoder\\saved_models\\pretrained\\pretrained.pt\r\n    low_mem:           False\r\n    no_sound:          False\r\n\r\nRunning a test of your configuration...\r\n\r\nYour PyTorch installation is not configured to use CUDA. If you have a GPU ready for deep learning, ensure that the drivers are properly installed, and that your CUDA version matches your PyTorch installation. CPU-only inference is currently not supported.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-6-e2d49148a56d>\", line 1, in <module>\r\n    runfile('C:/Users/isabel/Desktop/VOICE CLONER/Real-Time-Voice-Cloning-master/demo_cli.py', wdir='C:/Users/isabel/Desktop/VOICE CLONER/Real-Time-Voice-Cloning-master')\r\n\r\n  File \"C:\\Users\\isabel\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 827, in runfile\r\n\r\n  File \"C:\\Users\\isabel\\Anaconda3\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\", line 110, in execfile\r\n    # =============================================================================\r\n\r\n  File \"C:/Users/isabel/Desktop/VOICE CLONER/Real-Time-Voice-Cloning-master/demo_cli.py\", line 47, in <module>\r\n    device_id = torch.cuda.current_device()\r\n\r\n  File \"C:\\Users\\isabel\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 377, in current_device\r\n    _lazy_init()\r\n\r\n  File \"C:\\Users\\isabel\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 196, in _lazy_init\r\n    _check_driver()\r\n\r\n  File \"C:\\Users\\isabel\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 94, in _check_driver\r\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\r\n\r\nAssertionError: Torch not compiled with CUDA enabled\r\n\r\n\r\n**CUDA 10.0 is installed. I already tried uninstalling Pytorch and installing it again with the command \"conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\", but it didn't work. Can anyone help with this issue?? Thank you so much!**\n\ncc @ezyang @ngimel", "labels": ["awaiting response (this tag is deprecated)", "module: binaries", "module: cuda", "triaged"], "number_of_comments": 1, "created_at": "2020-02-05 23:16:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560679053": {"author_username": "jspark1105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33028, "title": "D19753082", "body": "", "labels": [], "number_of_comments": 0, "created_at": "2020-02-05 22:59:02", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560673419": {"author_username": "Krovatkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33025, "title": "strict check for a device type in Fuser", "body": "", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-05 22:44:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560647848": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33023, "title": "[wip][jit] Add backwards compatibility tests", "body": "", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-05 21:45:33", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560551565": {"author_username": "eellison", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33020, "title": "[JIT] Functional Graph Pass", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33297 Allow mutated values as inputs but not outputs\n* #33199 [JIT] remove list appends\n* #33186 [JIT] Pass To Safely Remove Aten Inplace Ops\n* **#33020 [JIT] Functional Graph Pass**\n\r\nThis is a pass to create functional blocks. The other PRs in the stack help avoid some of the limitations that are are often found in graphs. It's possible that this would work well with a graph that is frozen. Follow up work items that will help this pass:\r\n\r\n- We don't currently have any capacity in alias analysis to tell whether a Value that came from the wildcard set \"re-escapes\" back into the wildcard set.\r\n- More comments on the semantics of the graph and correctness conditions\r\n- We could consider using dynamic dag if the perf of this is a limitation. \r\n- potential make Functional Graphs Functional Blocks instead, so that we do not repeatedly copy constants, also to make IR read easier.\r\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-05 18:29:25", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560537588": {"author_username": "agolynski", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33019, "title": "Implementing negative striding for python lists. Fix #25135", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#33019 Implementing negative striding for python lists. Fix #25135**\n\nDifferential Revision: [D19836031](https://our.internmc.facebook.com/intern/diff/D19836031)", "labels": ["jit"], "number_of_comments": 4, "created_at": "2020-02-05 18:01:02", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560511425": {"author_username": "Evpok", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33016, "title": "CuDNN backend not available in nightly (20200205)", "body": "## \ud83d\udc1b Bug\r\n\r\nCuDNN works fine if I use 1.4.0, but it is not found in nightly, see a few checks below\r\n\r\n```python\r\n>>> torch.version.git_version\r\n'1c420dd12b190e716be0f3e031254adcfc2a8a05'\r\n>>> torch.cuda.is_available()\r\nTrue\r\n>>> torch._C._get_cudnn_enabled()\r\nTrue\r\n>>> torch._C.has_cudnn\r\nTrue\r\n>>> torch.backends.cudnn.version() is None\r\nTrue\r\n```\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.5.0.dev20200205\r\nIs debug build: No          \r\nCUDA used to build PyTorch: 10.1     \r\n                                         \r\nOS: Debian GNU/Linux bullseye/sid\r\nGCC version: (Debian 9.2.1-25) 9.2.1 20200123\r\nCMake version: version 3.15.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.168\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 440.44\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] numpydoc==0.9.2\r\n[pip3] pytorch-ignite==0.2.1\r\n[pip3] pytorch-pretrained-bert==0.6.2\r\n[pip3] pytorch-transformers==1.1.0\r\n[pip3] torch==1.5.0.dev20200205\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @ezyang @gchanan @zou3519", "labels": ["high priority", "module: binaries", "nightly-announce", "small", "triaged"], "number_of_comments": 13, "created_at": "2020-02-05 17:10:38", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560453421": {"author_username": "t-vi", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33010, "title": "catch and propagate warnings for JIT ScriptMethods", "body": "We align it with ScriptFunctions by using the HANDLE_TH_ERRORS/END_HANDLE_TH_ERRORS_PYBIND macros.\r\n\r\nFixes #24155  or #24828 ?\r\n", "labels": ["jit", "open source", "triaged"], "number_of_comments": 2, "created_at": "2020-02-05 15:38:34", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560402031": {"author_username": "raphaeldeimel", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33007, "title": "Upper/Lower attributes for named dimensions for proper Ricci notation and to generalize matrix operations", "body": "## \ud83d\ude80 Feature\r\n1. Add an \"upper|lower|unspecified\" attribute to each (named) tensor dimension.\r\n2. Honor the upper/lower distinction when checking dimension equality (upper!=lower, upper==unspecified, lower==unspecified, and of course lower==lower, upper==upper) in operations such as add, mul\r\n3. Provide a method to match pairs of upper and lower dimensions which share the same name\r\n4. Make all operators and algorithms specified on matrices available to all tensors with more than one upper and lower dimension (transpose, inverses, decompositions,...)\r\n\r\n## Motivation\r\n\r\nIn multilinear/tensor algebra, there is a clear distinction between upper and lower indices, which (among other things) can be understood to distinguish between input and output dimensions.\r\nBy having a distinction between inputs and outputs, we can generalize any operations on linear maps (i.e. matrices) to tensors of arbitrary dimensionality, e.g. we can apply the matrix transpose, matrix inverses, matrix decompositions, and in general any algorithm defined on matrices in a mathematically consistent way:\r\n\r\n**Flatten:** \r\nReshapes any tensor into a _matrix_ with row index = cartesian product of all upper indices and column index = cartesian product of all lower indices.\r\n\r\nNote that with this definition of flattening, we can generalize any matrix operation to any tensor, e.g. transposes, inverses, decompositions,... by applying it to the flattened tensor and reshaping (folding) it afterwards.\r\n\r\nNote 2: The current flat() method basically assumes that all dimension have the same attribute value\r\n\r\n**Transpose:** \r\nFlip the upper/lower attribute (upper to lower and lower to upper)\r\n\r\nNote: this is the equivalent of swapping row and column in a matrix. \r\n\r\nNote: Currently, transpose() reverses the order of dimensions in the underlying data array by default. Dimension order should be an irrelevant implementation detail in a named tensor scheme, though (and maybe left to a compiler for optimization). For future compatibility, reordering could be restricted to tensors with at least one \"unspecified\" dimension.\r\n\r\n**Inverses:**\r\n Invert the flattened tensor, then fold it back into its original shape. Upper/lower attributes get flipped.\r\n \r\nNote: It may make sense to implicitly transmogrify some index names of the resulting output tensor, so that contracting the tensor with its inverse yields another tensor (instead of a scalar). for example, the lower indices of a left-sided pseudoinverse would match upper indices of its operand, but upper indices would not match the lower indices of the operand. This asymmetric name change effectively encodes the \"left-sidedness\" of the inverse, as tensor contraction (in contrast to matrix multiplication) is commutative. \r\n\r\n**contraction/tensordot:** \r\nTensordot sums over pairs of indices with the same name but differing upper/lower attribute. For backwards compatibility, tensordot matches names with \"unspecified\" attribute with any other name.\r\n\r\n(Here, no additional functionality is gained over named indices, but it makes tensordot a proper implementation of the tensor contraction as defined in the Ricci-notation)\r\n\r\n**Coordinate transformation / Change of basis**\r\nCoordinate transformation of tensors can be done automatically, as the upper/lower attribute distinguishes contravariant from covariant indices, i.e. it completely specifies how the coordinate values of a tensor need to be transformed under a change of basis. This is an inherent feature of proper tensors and extensively used in Physics and Mechanics,\r\n\r\n\r\nA complementary purpose of the upper/lower attribute is to explicitly distinguish between maps (inputs and outputs), metrics(only inputs), and multidimensional data (only outputs). Invalid operand combinations (in the Ricci notation and multilinear algebra sense) could raise exceptions.\r\n\r\nUnfortunately, we cannot simply use some naming scheme to handle the upper/lower distinction, as the semantics with respect to matching dimensions for tensor contraction differ (names use *equality* while upper/lower uses *inequality*).\r\n\r\n## Pitch\r\n\r\nCurrently, no popular \"tensor\" library implements tensors such that they are fully compatible with the Ricci-notation, which is the most common notation to specify tensor equations. Pytorch already is well positioned though by the recent addition of named dimensions. Adding (optional) upper/lower attributes to each dimension would allow Pytorch users to implement tensor equations verbatim, improving code readability and avoiding bugs when implementing textbook equations.\r\n\r\nFrom a practical, software engineering perspective, the upper/lower attribute provides an additional check when constructing elaborate computation graphs by distinguishing whether a dimension is supposed to consume data (input dimension, or lower index), or is supposed to provide data (output dimension, or upper index). \r\n\r\nFor tensors with two dimensions, \"upper\" and \"lower\" is equivalent to the row-and-column distinction in matrix notation, so it is a straightforward and robust generalization.\r\n\r\nIn the far future, the change may also enable Pytorch to natively support algorithms from multilinear algebra, e.g. tensor decomposition algorithms. \r\n\r\n\r\n## Alternatives\r\n\r\nThe alternative is to comment-code the input-output direction semantics, similar to how dimension semantics had to be handled before introducing named dimensions. This is obviously error-prone and bad from a code-as-documention perspective. \r\n\n\ncc @zou3519", "labels": ["module: named tensor", "triaged"], "number_of_comments": 0, "created_at": "2020-02-05 14:18:57", "reactions": {"total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560382882": {"author_username": "Stannislav", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33006, "title": "matmul: no warning when contracting differently named dimensions", "body": "## \ud83d\udc1b Bug\r\nAccording to [documentation](https://pytorch.org/docs/stable/named_tensor.html) named tensors \"use names to automatically check that APIs are being used correctly at runtime, providing extra safety\". However, `matmul` does not warn when the dimensions that are being contracted have different names.\r\n\r\n## To Reproduce\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\na = torch.randn((2, 3), names=('B', 'X'))\r\nb = torch.randn((3, 5), names=('C', 'E'))\r\ntorch.matmul(a, b)\r\n```\r\n\r\nThis executes without any warning.\r\n\r\n## Expected behavior\r\nA warning about non-matching dimensions might be useful\r\n\r\n## Environment\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: Tesla T4\r\nNvidia driver version: 418.67\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.5\r\n[pip3] torch==1.4.0\r\n[pip3] torchsummary==1.5.1\r\n[pip3] torchtext==0.3.1\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n\r\n\n\ncc @zou3519", "labels": ["module: named tensor", "triaged"], "number_of_comments": 0, "created_at": "2020-02-05 13:46:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560179636": {"author_username": "ppwwyyxx", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 33003, "title": "Segfault running pytorch + onnx", "body": "## \ud83d\udc1b Bug\r\nI'm getting a segfault when running this binary https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl together with onnx 1.6 installed from pypi.\r\n\r\nI'm getting the following stack trace:\r\n```\r\nThread 1 \"python3\" received signal SIGSEGV, Segmentation fault.\r\n0x00007f28b63c3cdf in std::_Hashtable<std::string, std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> >, std::allocator<std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> > >, std::__detail::_---Type <return> to continue, or q <return> to quit---\r\nSelect1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear() ()\r\n   from /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so\r\n#0  0x00007f28b63c3cdf in std::_Hashtable<std::string, std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> >, std::allocator<std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> > >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear()\r\n    () from /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch.so\r\n#1  0x00007f288c37dea7 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#2  0x00007f288c478dd6 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#3  0x00007f288c430e67 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n---Type <return> to continue, or q <return> to quit---\r\n#4  0x00007f288c42bffe in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#5  0x00007f288c433cf5 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#6  0x00007f288c4a045d in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#7  0x00007f288c4a0f46 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#8  0x00007f288c4a2484 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#9  0x00007f288c4a29cb in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#10 0x00007f288c35dec3 in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-36m-x86_64-linux-gnu.so\r\n#11 0x00007f288c37728b in ?? ()\r\n   from /usr/local/lib/python3.6/dist-packages/onnx/onnx_cpp2py_export.cpython-3---Type <return> to continue, or q <return> to quit---\r\n6m-x86_64-linux-gnu.so\r\n#12 0x000000000050ac25 in ?? ()\r\n#13 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#14 0x0000000000509d48 in ?? ()\r\n#15 0x000000000050aa7d in ?? ()\r\n#16 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#17 0x0000000000509d48 in ?? ()\r\n#18 0x000000000050aa7d in ?? ()\r\n#19 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#20 0x0000000000508245 in ?? ()\r\n#21 0x000000000050a080 in ?? ()\r\n#22 0x000000000050aa7d in ?? ()\r\n#23 0x000000000050d390 in _PyEval_EvalFrameDefault ()\r\n#24 0x0000000000508245 in ?? ()\r\n#25 0x000000000050a080 in ?? ()\r\n#26 0x000000000050aa7d in ?? ()\r\n#27 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#28 0x0000000000508245 in ?? ()\r\n#29 0x000000000050a080 in ?? ()\r\n#30 0x000000000050aa7d in ?? ()\r\n#31 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#32 0x0000000000509d48 in ?? ()\r\n#33 0x000000000050aa7d in ?? ()\r\n---Type <return> to continue, or q <return> to quit---\r\n#34 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#35 0x0000000000509d48 in ?? ()\r\n#36 0x000000000050aa7d in ?? ()\r\n#37 0x000000000050c5b9 in _PyEval_EvalFrameDefault ()\r\n#38 0x0000000000508245 in ?? ()\r\n#39 0x000000000050b403 in PyEval_EvalCode ()\r\n#40 0x0000000000635222 in ?? ()\r\n#41 0x00000000006352d7 in PyRun_FileExFlags ()\r\n#42 0x0000000000638a8f in PyRun_SimpleFileExFlags ()\r\n#43 0x0000000000639631 in Py_Main ()\r\n#44 0x00000000004b0f40 in main ()\r\n```\r\n\r\nso it looks like onnx is calling symbol defined in libtorch.so, and crashes due to binary compatibility issues. Recompiling onnx does fix the issue for me.\r\n\r\nSo I'm wondering whether the offending symbol:\r\n```\r\nstd::_Hashtable<std::string, std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> >, std::allocator<std::pair<std::string const, std::pair<std::unordered_set<std::string const*, std::hash<std::string const*>, std::equal_to<std::string const*>, std::allocator<std::string const*> >, std::string> > >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::clear()\r\n```\r\nshould be made invisible. \r\n\r\nI can prepare further instructions to reproduce if needed. But if it's an issue of visibility I perhaps don't have to.\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 0, "created_at": "2020-02-05 07:02:04", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560093543": {"author_username": "sbagchi12", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32998, "title": "torch batchwise max with indices", "body": "## \ud83d\ude80 Feature\r\nA batchwise verison of torch.max() or torch.argmax().\r\n\r\n## Motivation\r\nI have an array with elements coming from multiple examples where each example contributes an independent number of elements. I would like to have an efficient way to find the max element along with the index.\r\n\r\n## Pitch\r\n\r\ntorch.max() should support batches of max operations instead of a single tensor\r\n\r\n## Alternatives\r\n\r\nA hacky alternative \r\n\r\nc = torch.stack([torch.argmax(a[batch==i],dim=0,keepdim=True) for i in range(2)],dim=0)", "labels": ["module: operators", "needs reproduction", "triaged"], "number_of_comments": 1, "created_at": "2020-02-05 01:57:32", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560052374": {"author_username": "ngoldbaum", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32994, "title": "torch.nonzero issues a DeprecationWarning for non-deprecated usages", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nAfaics any usage of `torch.nonzero` is producing a deprecation warning\r\n\r\n## To Reproduce\r\n\r\n```\r\nIn [1]: import torch\r\n\r\nIn [2]: t = torch.arange(2)\r\n\r\nIn [3]: torch.nonzero(t)\r\n../torch/csrc/utils/python_arg_parser.cpp:738: UserWarning: This overload of nonzero is deprecated:\r\n        nonzero(Tensor input, Tensor out)\r\nConsider using one of the following signatures instead:\r\n        nonzero(Tensor input, bool as_tuple)\r\nOut[3]: tensor([[1]])\r\n```\r\n\r\n## Expected behavior\r\n\r\nNo deprecation warning\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n```\r\nPyTorch version: 1.5.0a0+d2098bd\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.2\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.16.1\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration:\r\nGPU 0: TITAN RTX\r\nGPU 1: TITAN RTX\r\n\r\nNvidia driver version: 440.33.01\r\ncuDNN version: /usr/local/cuda-10.2.89/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.3\r\n[pip] torch==1.5.0a0+d2098bd\r\n[conda] mkl                       2019.5                      281    conda-forge\r\n[conda] mkl-include               2019.5                      281    conda-forge\r\n[conda] torch                     1.5.0a0+d2098bd           dev_0    <develop>\r\n```\r\n", "labels": ["module: nn", "topic: logging", "triaged"], "number_of_comments": 0, "created_at": "2020-02-04 23:57:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560050987": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32993, "title": "[jit] Fix aug assign for non-tensor attributes", "body": "Instead of erroring out this de-sugars augmented assignments to class\r\nmembers from `self.a += 1` to `self.a = self.a + 1`.\r\n\r\nFixes #32973\n\nDifferential Revision: [D19737636](https://our.internmc.facebook.com/intern/diff/19737636/)", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-04 23:54:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560039101": {"author_username": "vainaijr", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32991, "title": "random_pruning, remove_pruning typo", "body": "## \ud83d\udcda Documentation\r\n\r\ndocumentation includes use of random_pruning, but there is nothing like random_pruning.\r\n```\r\nm = random_pruning(nn.Linear(5, 7), name='weight', amount=0.2)\r\nprune.random_pruning(m, name='weight', amount=0.2)\r\n```\r\nAttributeError: module 'torch.nn.utils.prune' has no attribute 'random_pruning'\r\n\r\nsame thing for remove_pruning also,\r\n```\r\n>>> m = remove_pruning(m, name='weight')\r\n```\r\nAttributeError: module 'torch.nn.utils.prune' has no attribute 'remove_pruning'", "labels": ["module: docs", "triaged"], "number_of_comments": 2, "created_at": "2020-02-04 23:22:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560035735": {"author_username": "zhaojuanmao", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32990, "title": "allow remote torchscript call to itself", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #32959 jit pickling rref\n* **#32990 allow remote torchscript call to itself**\n\nright now remote torchscript call can not call to itself, this diff is to support this in the same way as how is supported when calling remote python call to itself\n\nDifferential Revision: [D19731910](https://our.internmc.facebook.com/intern/diff/D19731910/)", "labels": [], "number_of_comments": 4, "created_at": "2020-02-04 23:13:53", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "560033032": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32988, "title": "[jit] Dict set item type mismatch error doesn't say the type was inferred", "body": "This error doesn't include the fact that the compiler inferred the type of `out_dict` to be `Dict[str, Tensor]`, but it should have\r\n\r\n```python\r\ndef fn():\r\n    out_dict = {}\r\n    M = torch.ones(2)\r\n    out_dict[0] = M\r\n    return out_dict\r\n```\r\n\r\n```\r\nRuntimeError: \r\nArguments for call are not valid.\r\nThe following variants are available:\r\n  \r\n  aten::_set_item(Tensor[](a!) l, int idx, Tensor(b -> *) el) -> (Tensor[](a!)):\r\n  Expected a value of type 'List[Tensor]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(int[](a!) l, int idx, int el) -> (int[](a!)):\r\n  Expected a value of type 'List[int]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(float[](a!) l, int idx, float el) -> (float[](a!)):\r\n  Expected a value of type 'List[float]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(bool[](a!) l, int idx, bool el) -> (bool[](a!)):\r\n  Expected a value of type 'List[bool]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(t[](a!) l, int idx, t(b -> *) el) -> (t[](a!)):\r\n  Could not match type Dict[str, Tensor] to List[t] in argument 'l': Cannot match List[t] to Dict[str, Tensor].\r\n  \r\n  aten::_set_item(Dict(str, t)(a!) l, str(b -> *) idx, t(c -> *) v) -> ():\r\n  Expected a value of type 'str' for argument 'idx' but instead found type 'int'.\r\n  \r\n  aten::_set_item(Dict(int, t)(a!) l, int(b -> *) idx, t(c -> *) v) -> ():\r\n  Expected a value of type 'Dict[int, t]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(Dict(float, t)(a!) l, float(b -> *) idx, t(c -> *) v) -> ():\r\n  Expected a value of type 'Dict[float, t]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n  \r\n  aten::_set_item(Dict(Tensor, t)(a!) l, Tensor(b -> *) idx, t(c -> *) v) -> ():\r\n  Expected a value of type 'Dict[Tensor, t]' for argument 'l' but instead found type 'Dict[str, Tensor]'.\r\n\r\nThe original call is:\r\n  File \"../test.py\", line 34\r\n    out_dict = {}\r\n    M = torch.ones(2)\r\n    out_dict[0] = M\r\n    ~~~~~~~~~~~~~~~ <--- HERE\r\n    return out_dict\r\n\r\n```\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-02-04 23:06:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559930712": {"author_username": "nvpohanh", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32983, "title": "PyTorch should hide CUDNN symbols in libtorch.so", "body": "## \ud83d\udc1b Bug\r\n\r\nPyTorch's `libtorch.so` exposes a lot of CUDNN API symbols. This causes issues when our application (independent from PyTorch) uses a different CUDNN version.\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nInstall PyTorch 1.4.0. Run `nm` on the `libtorch.so`, and there will be a lot of CUDNN API symbols.\r\n\r\n```\r\nnvpohanh@ubuntu:~$ nm /usr/local/lib/python3.5/dist-packages/torch/lib/libtorch.so | grep ' cudnnCreate'\r\n000000000f479c30 T cudnnCreate\r\n000000000f475ff0 T cudnnCreateActivationDescriptor\r\n000000000f4747e0 T cudnnCreateAlgorithmDescriptor\r\n000000000f474930 T cudnnCreateAlgorithmPerformance\r\n000000000f473e90 T cudnnCreateConvolutionDescriptor\r\n000000000f621a10 T cudnnCreateCTCLossDescriptor\r\n000000000fc8cb50 T cudnnCreateDropoutDescriptor\r\n000000000f47b3c0 T cudnnCreateFilterDescriptor\r\n000000000f476460 T cudnnCreateLRNDescriptor\r\n000000000f4752d0 T cudnnCreateOpTensorDescriptor\r\n000000000fbe5520 T cudnnCreatePersistentRNNPlan\r\n000000000f476af0 T cudnnCreatePoolingDescriptor\r\n000000000f474f00 T cudnnCreateReduceTensorDescriptor\r\n000000000fbe4f90 T cudnnCreateRNNDataDescriptor\r\n000000000fbe3880 T cudnnCreateRNNDescriptor\r\n000000000f475450 T cudnnCreateSpatialTransformerDescriptor\r\n000000000f47aca0 T cudnnCreateTensorDescriptor\r\n000000000f47cde0 T cudnnCreateTensorTransformDescriptor\r\n```\r\n\r\n## Expected behavior\r\n\r\nPyTorch should hide CUDNN symbols in libtorch.so\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: version 3.13.3\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: TITAN RTX\r\nNvidia driver version: 440.33.01\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.2.1\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @csarofeen @ptrblck", "labels": ["module: build", "module: cudnn", "triaged"], "number_of_comments": 1, "created_at": "2020-02-04 19:29:26", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559876561": {"author_username": "julianmack", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32976, "title": "[JIT] pytorch 1.4 breaks torch.jit.script(LSTM/GRU)", "body": "## \ud83d\udc1b Bug\r\n\r\n### LSTM/GRU cannot be torch scripted in new release (`1.4.0`)\r\n## To Reproduce\r\n\r\nSteps to reproduce the behaviour:\r\n```shell\r\nconda create -n pytorch1.4 python=3.7 pytorch=1.4 -c pytorch -y\r\nconda activate pytorch1.4\r\npython3\r\n```\r\nIn `python3` interpreter\r\n```python\r\nimport torch\r\n\r\ninput_size = 3\r\nnum_layers = 2\r\nseq_len = 3\r\nbatch = 3\r\nargs = (torch.randn(seq_len, batch, input_size),)\r\nscripted_lstm = torch.jit.script(torch.nn.LSTM(input_size, 3, num_layers))\r\nscripted_lstm(*args)\r\n```\r\n\r\nThrows error message:\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/julian/miniconda3/envs/pytorch1.4/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/julian/miniconda3/envs/pytorch1.4/lib/python3.7/site-packages/torch/jit/__init__.py\", line 1678, in __getattr__\r\n    return super(RecursiveScriptModule, self).__getattr__(attr)\r\n  File \"/home/julian/miniconda3/envs/pytorch1.4/lib/python3.7/site-packages/torch/jit/__init__.py\", line 1499, in __getattr__\r\n    return super(ScriptModule, self).__getattr__(attr)\r\n  File \"/home/julian/miniconda3/envs/pytorch1.4/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 576, in __getattr__\r\n    type(self).__name__, name))\r\nAttributeError: 'RecursiveScriptModule' object has no attribute 'forward'\r\n```\r\n\r\n\r\n## Expected behaviour\r\n\r\nThe expected behaviour is for the `scripted_lstm(*args)` not to output an error and return the tensor. \r\nI get this behaviour when I use `1.3` and replace the first x2 bash lines above with:\r\n```shell\r\nconda create -n pytorch1.3 python=3.7 pytorch=1.3 -c pytorch -y\r\nconda activate pytorch1.3\r\n```\r\n\r\n## Environment\r\n\r\n```shell\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce GTX 1050 Ti with Max-Q Design\r\nNvidia driver version: 430.64\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.13.3\r\n[pip3] torch-stft==0.1.4\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n```\r\n\r\n## Additional context\r\n\r\nN/A\r\n\n\ncc @ezyang @gchanan @zou3519 @suo", "labels": ["has workaround", "high priority", "jit", "triage review", "triaged"], "number_of_comments": 4, "created_at": "2020-02-04 17:41:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559811814": {"author_username": "alanhdu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32973, "title": "[jit] augmented assignment on int attributes doesn't work", "body": "## \ud83d\ude80 Feature\r\n\r\nIt'd be nice of TorchScript could support assigning to fields with simple Python types (e.g. non-tensors). For example:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass A(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.x: int = 0    # int field\r\n    def forward(self):\r\n        self.x += 1       # fails to script\r\n        return self.x\r\n```\r\n\r\n## Motivation\r\n\r\nI'm writing a real-time streaming wrapper around some of PyTorch's modules so that they an run online, sample-by-sample. For convolutions, this requires some bookkeeping to buffer up the receptive field that requires incrementing and decrementing various counters, which would be nice to support natively in Torchscript.\r\n\r\n## Alternatives\r\n\r\nCurrently, we're working around this by using 0d \"scalar\" tensors (e.g. `torch.tensor(0)`). This works, but it's not really ideal (it requires verbose `dtype` annotations, it means that these counters could be moved to the GPU unnecessarily, it's requires some amount of boilerplate).\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context or screenshots about the feature request here. -->\r\n\n\ncc @suo", "labels": ["jit", "triaged"], "number_of_comments": 1, "created_at": "2020-02-04 15:54:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559742648": {"author_username": "jaydeepb-inexture", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32972, "title": "which pytorch do i install for running this project in my windows.link for this is ::  https://github.com/xiaojunxu/SQLNet  .IN this they have used python 2.7 ,but i am unable to install pytorch on python 2.7 environment .help me with this", "body": "## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @ezyang", "labels": ["module: binaries", "triaged"], "number_of_comments": 3, "created_at": "2020-02-04 14:07:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559741381": {"author_username": "wanifarooq", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32971, "title": "Issue with transformer.compose ( )", "body": "When I was trying  to load 16 bit .png grayscale image with torchvision.datasets.imagefolder ,it is loading every image as white only. \r\nI solved this issue by doing transformation operations outside Compose function.\r\n\r\n\n\ncc @SsnL", "labels": ["module: dataloader", "triaged"], "number_of_comments": 0, "created_at": "2020-02-04 14:05:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559599125": {"author_username": "shz0116", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32969, "title": "[fbgemm] test fbgemm changes for windows", "body": "Test Only", "labels": [], "number_of_comments": 2, "created_at": "2020-02-04 09:43:49", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559564010": {"author_username": "BelBES", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32968, "title": "Can't convert model with torch.triu to ONNX", "body": "Models that contain torch.triu can not be converted to ONNX.\r\n\r\nError message:\r\n\r\n> UserWarning: ONNX export failed on ATen operator triu because torch.onnx.symbolic_opset9.triu does not exist\r\n\r\nSimple reproducer is here.\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass SimpleModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = nn.Conv2d(10, 10, 3, padding=1, bias=False)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        mask = torch.triu(torch.ones_like(x))\r\n        x = x * mask\r\n        return x\r\n\r\nmodel = SimpleModel()\r\nvar = torch.ones(1, 10, 120, 120)\r\n\r\ntorch.onnx.export(\r\n    model,\r\n    (var,),\r\n    \"simple_model.onnx\",\r\n    input_names=[\"x\"],\r\n    output_names=[\"out\"]\r\n)\r\n```\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 3, "created_at": "2020-02-04 08:36:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559480697": {"author_username": "mrshenli", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32966, "title": "Only fork RRef when it is used in RPC", "body": "Currently, we fork (add fork to owner or child to parent) RRef iin `PyRRef::pickle()`. However, this might be wrong if the RRef is pickled for, say, checkpointing. RRef should only be forked when it is used in RPC.\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-04 04:28:25", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559442749": {"author_username": "osalpekar", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32963, "title": "Consolidate RpcAgent and ProcessGroupAgent running atomic variables.", "body": "## \ud83d\ude80 Feature\r\n`RpcAgent` contains an atomic variable `std::atomic<bool> rpcAgentRunning_`, and `ProcessGroupAgent` contains a similar `std::atomic<bool> rpcRunning_`. They both serve the same functionality and should be consolidated. \r\n\r\nThis atomic can be set to true in `RpcAgent::start()`, and the `start()` functions in `ProcessGroupAgent` and `ThriftRpcAgent` can call the parent class function.\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["better-engineering", "module: rpc", "triaged"], "number_of_comments": 0, "created_at": "2020-02-04 02:05:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559430881": {"author_username": "shinh", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32960, "title": "[ONNX] scatter_add may result in wrong ONNX", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\ntorch.onnx translates scatter_add to Scatter, but I think this is a wrong translation when elements in `index` are not unique. See the script in the repro.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following script\r\n1. ONNX runtime refuses to run the output ONNX\r\n\r\n```\r\nimport onnx\r\nimport onnx.numpy_helper\r\nimport torch\r\nimport onnxruntime as rt\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def forward(self, data, indices, updates):\r\n        return torch.scatter_add(data, 0, indices, updates)\r\n\r\nmodel = Model()\r\ndata = torch.tensor([0.0])\r\nindices = torch.tensor([0, 0, 0])\r\nupdates = torch.tensor([1.2, 2.3, 3.4])\r\n# This works.\r\n# indices = torch.tensor([0])\r\n# updates = torch.tensor([1.2])\r\ntorch.onnx.export(model, (data, indices, updates), 'scatter_add.onnx')\r\n\r\nm = onnx.load('scatter_add.onnx')\r\nsess = rt.InferenceSession('scatter_add.onnx')\r\ninputs = {m.graph.input[0].name: data.numpy(),\r\n          m.graph.input[1].name: indices.numpy(),\r\n          m.graph.input[2].name: updates.numpy()}\r\nassert sess.run([m.graph.output[0].name], inputs)[0] == model(data, indices, updates).numpy()\r\n```\r\n\r\n## Expected behavior\r\n\r\nI think ONNX currently does not have ScatterAdd. I doubt it cannot be implemented in the current ONNX and we should add a new op. That said, torch.onnx should probably show an error message when indices are not unique?\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce MX150\r\nNvidia driver version: 440.33.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.2.1\r\n[pip3] warpctc-pytorch==0.1\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 4, "created_at": "2020-02-04 01:23:09", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559426212": {"author_username": "zhaojuanmao", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32959, "title": "jit pickling rref", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32959 jit pickling rref**\n* #32990 allow remote torchscript call to itself\n\nin rpc torch script call path, we need to pickle/unpickle rref, this diff is added to make jit pickler/unpickler be able to pickle/unpickle rref. It is similar to what is implemented for PyRef::pickle() and PyRef::unpickle()\n\nDifferential Revision: [D19713293](https://our.internmc.facebook.com/intern/diff/D19713293/)", "labels": ["jit"], "number_of_comments": 7, "created_at": "2020-02-04 01:06:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559426141": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32958, "title": "Make zip serialization the default", "body": "Stacked PRs\r\n * **#32958 - Make zip serialization the default**\r\n * #32244 - Fix some bugs with zipfile serialization\r\n\r\nThis flips the flag to change our eager serialization format to a zipfile as described in #26567. I'm not sure if we should flip the flag just yet since there's still some perf issues, the zip file format is about [10% slower for big files](https://gist.github.com/a5bee3ae642b144ad089e8b30eff6d3f)", "labels": [], "number_of_comments": 0, "created_at": "2020-02-04 01:06:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559420426": {"author_username": "elliotwaite", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32956, "title": "Slices don't work when converting to ONNX and using ONNX.js", "body": "## \ud83d\udc1b Bug\r\n\r\nIf I convert a model that uses a slice (such as `x = x[:, 0]`) into an ONNX model, and then use that ONNX model with ONNX.js, I get an error. I am not sure if this is an issue with the PyTorch-to-ONNX conversion or with the ONNX.js library.\r\n\r\n## To Reproduce\r\n\r\nGenerate an ONNX model with the script below:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n  def forward(self, x):\r\n    x = x[:, 0]\r\n    return x\r\n\r\n\r\ndef main():\r\n  model = Model()\r\n  dummy_input = torch.zeros(1, 2)\r\n  torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nThis will generate this graph:\r\n\r\n```\r\ngraph(%0 : Float(1, 2)):\r\n  %1 : Tensor = onnx::Constant[value={0}]()\r\n  %2 : Float(1) = onnx::Gather[axis=1](%0, %1)\r\n  return (%2)\r\n```\r\n\r\nThis will also generate the `model.onnx` file. We can then run the below HTML code in the same directory as the `model.onnx` file to try to use the model with ONNX.js:\r\n\r\n```\r\n<html>\r\n  <body>\r\n    <script src=\"https://cdn.jsdelivr.net/npm/onnxjs/dist/onnx.min.js\"></script>\r\n    <script>\r\n      async function test() {\r\n        const sess = new onnx.InferenceSession()\r\n        await sess.loadModel('./model.onnx')\r\n        const input = new onnx.Tensor(new Float32Array(2), 'float32', [1, 2])\r\n        const outputMap = await sess.run([input])\r\n        const outputTensor = outputMap.values().next().value\r\n        console.log(`Output tensor: ${outputTensor.data}`)\r\n      }\r\n      test();\r\n    </script>\r\n  </body>\r\n</html>\r\n```\r\n\r\nThis will produce the following console log:\r\n\r\n```\r\nUncaught (in promise) Error: Failed to compile shader: ERROR: 0:87: '' : array size must be greater than zero\r\n\r\n    at t.compileShader (webgl-context.ts:174)\r\n    at t.compile (program-manager.ts:97)\r\n    at program-manager.ts:67\r\n    at t.event (instrument.ts:294)\r\n    at t.build (program-manager.ts:64)\r\n    at t.run (inference-handler.ts:26)\r\n    at e.run (gather.ts:10)\r\n    at t.<anonymous> (execution-plan.ts:104)\r\n    at backend.ts:102\r\n    at Object.next (backend.ts:102)\r\n```\r\n\r\nTo get the expected behavior, we can swap out using a slice (`x[:, 0]`) for using `narrow` (`torch.narrow(x, dim=1, start=1, length=1)`) as in the script below:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass Model(nn.Module):\r\n  def forward(self, x):\r\n    x = torch.narrow(x, dim=1, start=1, length=1)\r\n    return x\r\n\r\n\r\ndef main():\r\n  model = Model()\r\n  dummy_input = torch.zeros(1, 2)\r\n  torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nThis will generate this graph:\r\n```\r\ngraph(%0 : Float(1, 2)):\r\n  %1 : Float(1, 1) = onnx::Slice[axes=[1], ends=[2], starts=[1]](%0)\r\n  return (%1)\r\n```\r\n\r\nAnd using the generated ONNX model with the same HTML code as above will work, with a console log output of:\r\n```\r\nOutput tensor: 0\r\n```\r\n\r\n## Environment\r\nPyTorch version: 1.5.0.dev20200203\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.5.0.dev20200203\r\n[pip] torchvision==0.6.0a0+a5525e9\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.4 233\r\n[conda] mkl-service 2.3.0 py37hfbe908c_0\r\n[conda] mkl_fft 1.0.15 py37h5e564d8_0\r\n[conda] mkl_random 1.1.0 py37ha771720_0\r\n[conda] pytorch 1.5.0.dev20200203 py3.7_0 pytorch-nightly\r\n[conda] torchvision 0.6.0.dev20200203 py37_cpu pytorch-nightly\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 2, "created_at": "2020-02-04 00:45:37", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559414666": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32955, "title": "[jit] Fix flipped PackedSequence outputs in script", "body": "Stacked PRs\n * **#32955 - [jit] Fix flipped PackedSequence outputs in script**\n * #32953 - [jit] Support properties on `Device`\n\nFixes #32605", "labels": [], "number_of_comments": 0, "created_at": "2020-02-04 00:25:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559414551": {"author_username": "elliotwaite", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32954, "title": "Can't use avg_pool2d with the default stride when converting to ONNX", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen calling `torch.onnx.export()` with a model that uses `F.avg_pool2d()` without specifying a stride, an error occurs. According to the docs, if the stride is left as `None`, it should by default be set to the kernel size.\r\n\r\n## To Reproduce\r\n\r\nRun this script:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Model(nn.Module):\r\n  def forward(self, x):\r\n    x = F.avg_pool2d(x, 2)\r\n    return x\r\n\r\n\r\ndef main():\r\n  model = Model()\r\n  dummy_input = torch.zeros(1, 1, 4, 4)\r\n  torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nHere is the output with the error message:\r\n```\r\ngraph(%0 : Float(1, 1, 4, 4)):\r\n  %1 : Tensor = onnx::Pad[mode=\"constant\", pads=[0, 0, 0, 0, 0, 0, 0, 0], value=0.](%0)\r\n  %2 : Float(1, 1, 2, 2) = onnx::AveragePool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=annotate(List[int], [])](%1)\r\n  return (%2)\r\n\r\n...\r\n\r\nRuntimeError: Attribute 'strides' is expected to have field 'ints'\r\n\r\n==> Context: Bad node spec: input: \"1\" output: \"2\" name: \"AveragePool_1\" op_type: \"AveragePool\" attribute { name: \"kernel_shape\" ints: 2 ints: 2 type: INTS } attribute { name: \"pads\" ints: 0 ints: 0 ints: 0 ints: 0 type: INTS } attribute { name: \"strides\" type: INTS }\r\n```\r\n\r\nIn contrast to `avg_pool2d`, `max_pool2d` handles the default stride correctly, as demonstrated by running the code below.\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Model(nn.Module):\r\n  def forward(self, x):\r\n    x = F.max_pool2d(x, 2)\r\n    return x\r\n\r\n\r\ndef main():\r\n  model = Model()\r\n  dummy_input = torch.zeros(1, 1, 4, 4)\r\n  torch.onnx.export(model, dummy_input, 'model.onnx', verbose=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n  main()\r\n```\r\n\r\nThis will output a graph with the correct `strides=[2, 2]`:\r\n```\r\ngraph(%0 : Float(1, 1, 4, 4)):\r\n  %1 : Float(1, 1, 2, 2) = onnx::MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%0)\r\n  return (%1)\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.5.0.dev20200203\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.5.0.dev20200203\r\n[pip] torchvision==0.6.0a0+a5525e9\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      233  \r\n[conda] mkl-service               2.3.0            py37hfbe908c_0  \r\n[conda] mkl_fft                   1.0.15           py37h5e564d8_0  \r\n[conda] mkl_random                1.1.0            py37ha771720_0  \r\n[conda] pytorch                   1.5.0.dev20200203         py3.7_0    pytorch-nightly\r\n[conda] torchvision               0.6.0.dev20200203        py37_cpu    pytorch-nightly\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 6, "created_at": "2020-02-04 00:24:47", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559400361": {"author_username": "lara-hdr", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32950, "title": "ONNX Update training ops and training amenable export API", "body": "- Update Dropout and Batchnorm in opset 12 : https://github.com/onnx/onnx/pull/2568\r\n- Update api logic for exporting to ONNX training amenable models", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-02-03 23:39:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559277331": {"author_username": "mtkliema", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32937, "title": "Operation Registration Error", "body": "## \ud83d\udc1b Bug\r\n<!-- A clear and concise description of what the bug is. -->\r\nIssue with operator registration in aten.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nI am attempting to follow the tutorial here: https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html\r\n\r\nThe following line is causing a compilation error on 1.4 that did not occur on 1.1.\r\nstatic auto registry =\r\n  torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective);\r\n\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n```\r\n[1/2] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=correlate -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.6/dist-packages/torch/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.6/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -std=c++11 -c /workspace/second.pytorch/second/pytorch/models/correlate.cu -o correlate.cuda.o\r\nFAILED: correlate.cuda.o \r\n/usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=correlate -DTORCH_API_INCLUDE_EXTENSION_H -isystem /usr/local/lib/python3.6/dist-packages/torch/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.6/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.6/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.6m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -std=c++11 -c /workspace/second.pytorch/second/pytorch/models/correlate.cu -o correlate.cuda.o\r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/ivalue_inl.h(837): error: static assertion failed with \"You are calling from with a type that it doesn't support, and isn't a potential custom class (ie: is an intrusive_ptr)\"\r\n          detected during:\r\n            instantiation of \"c10::IValue c10::ivalue::detail::from_(T, std::false_type) [with T=at::Tensor]\" \r\n(844): here\r\n            instantiation of \"c10::IValue c10::ivalue::from(T) [with T=at::Tensor]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/kernel_functor.h(184): here\r\n            instantiation of \"c10::IValue c10::detail::return_to_ivalue<T,AllowDeprecatedTypes>(T &&) [with T=at::Tensor, AllowDeprecatedTypes=true]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/kernel_functor.h(208): here\r\n            instantiation of \"void c10::detail::push_outputs<OutputType, AllowDeprecatedTypes>::call(OutputType &&, c10::Stack *) [with OutputType=at::Tensor, AllowDeprecatedTypes=true]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/kernel_functor.h(236): here\r\n            instantiation of \"void c10::detail::wrap_kernel_functor_boxed<KernelFunctor, AllowDeprecatedTypes, c10::guts::enable_if_t<<expression>, void>>::call(c10::OperatorKernel *, c10::Stack *) [with KernelFunctor=c10::detail::WrapRuntimeKernelFunctor<c10::guts::decay_t<at::Tensor (const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t)>>, AllowDeprecatedTypes=true]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/KernelFunction.h(172): here\r\n            instantiation of \"c10::KernelFunction c10::KernelFunction::makeFromUnboxedFunctor<AllowLegacyTypes,KernelFunctor>(std::unique_ptr<c10::OperatorKernel, std::default_delete<c10::OperatorKernel>>) [with AllowLegacyTypes=true, KernelFunctor=c10::detail::WrapRuntimeKernelFunctor<c10::guts::decay_t<at::Tensor (const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t)>>]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/KernelFunction.h(315): here\r\n            instantiation of \"c10::KernelFunction c10::KernelFunction::makeFromUnboxedRuntimeFunction(FuncType *) [with AllowLegacyTypes=true, FuncType=at::Tensor (const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t)]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_registration.h(517): here\r\n            instantiation of \"c10::guts::enable_if_t<<expression>, c10::RegisterOperators &&> c10::RegisterOperators::op(const std::string &, FuncType *, c10::RegisterOperators::Options &&) && [with FuncType=at::Tensor (const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t)]\" \r\n/usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_registration.h(474): here\r\n            instantiation of \"c10::RegisterOperators::RegisterOperators(const std::string &, FuncType &&, c10::RegisterOperators::Options &&) [with FuncType=at::Tensor (*)(const at::Tensor &, const at::Tensor &, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t)]\" \r\n/workspace/second.pytorch/second/pytorch/models/correlate.cu(135): here\r\n```\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4\r\n - OS (e.g., Linux): Linux Ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): Pip from homepage\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 10.1\r\n - GPU models and configuration:\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n", "labels": ["module: dispatch", "triaged"], "number_of_comments": 4, "created_at": "2020-02-03 19:12:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559244025": {"author_username": "bpstark", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32933, "title": "Created a jit post pass which removes branches that contain exceptions.", "body": "Since ONNX does not support the concept of an exception, there is no way\r\nto replicate such behavior in ONNX. We must instead remove branches of\r\ncode that throw exceptions, and have undefined behavior for bad input.\r\n\r\n", "labels": ["jit", "open source", "triaged"], "number_of_comments": 3, "created_at": "2020-02-03 18:05:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "559225388": {"author_username": "alondj", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32930, "title": "TupleUnpack outputs not used in the correct places in the trace graph", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\nclass ABC(nn.Module):\r\n    def __init__(self):\r\n        super(ABC, self).__init__()\r\n        self.l = ReturnTuple()\r\n\r\n    def forward(self, x):\r\n        a, b, c = self.l(x)\r\n        # use different ops to distinguish tuple elements\r\n        # the trace should show that a goes to relu and c to softmax\r\n        return a.relu(), b.sigmoid(), c.softmax(1)\r\n\r\n\r\nclass CBA(nn.Module):\r\n    def __init__(self):\r\n        super(CBA, self).__init__()\r\n        self.l = ReturnTuple()\r\n\r\n    def forward(self, x):\r\n        a, b, c = self.l(x)\r\n        # use different ops to distinguish tuple elements\r\n        # the trace should show that c goes to relu and a to softmax\r\n        # but looks exactly the same as above\r\n        return c.relu(), b.sigmoid(), a.softmax(1)\r\n\r\n\r\nclass ReturnTuple(nn.Module):\r\n    def forward(self, x):\r\n        # return a tuple\r\n        return x * 3, x * 4, x * 5\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sample = torch.randn(10, 10)\r\n    traced_ABC = torch.jit.trace(ABC(), sample)\r\n    traced_CBA = torch.jit.trace(CBA(), sample)\r\n    scripted_CAB = torch.jit.script(CAB())\r\n\r\n    print(\"ABC trace\")\r\n    print(str(traced_ABC.graph))\r\n    print(\"CBA trace\")\r\n    print(str(traced_CBA.graph))\r\n    print(\"CBA scripted\")\r\n    print(str(scripted_CAB.graph))\r\n```\r\n\r\nABC trace\r\n```python\r\ngraph(%self.1 : __torch__.torch.nn.modules.module.___torch_mangle_0.Module,\r\n      %x : Float(10, 10)):\r\n  %18 : __torch__.torch.nn.modules.module.Module = prim::GetAttr[name=\"l\"](%self.1)\r\n  %27 : (Tensor, Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%18, %x)\r\n  %24 : Tensor, %25 : Tensor, %26 : Tensor = prim::TupleUnpack(%27)\r\n  %11 : Float(10, 10) = aten::relu(%24) # tuple_unpack.py:12:0\r\n  %12 : Float(10, 10) = aten::sigmoid(%25) # tuple_unpack.py:12:0\r\n  %13 : int = prim::Constant[value=1]() # tuple_unpack.py:12:0\r\n  %14 : None = prim::Constant()\r\n  %15 : Float(10, 10) = aten::softmax(%26, %13, %14) # tuple_unpack.py:12:0\r\n  %16 : (Float(10, 10), Float(10, 10), Float(10, 10)) = prim::TupleConstruct(%11, %12, %15)\r\n  return (%16)\r\n```\r\n\r\nCBA trace unexpected behavior\r\n```python\r\ngraph(%self.1 : __torch__.torch.nn.modules.module.___torch_mangle_4.Module,\r\n      %x : Float(10, 10)):\r\n  %18 : __torch__.torch.nn.modules.module.___torch_mangle_3.Module = prim::GetAttr[name=\"l\"](%self.1)\r\n  %27 : (Tensor, Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%18, %x)\r\n  %24 : Tensor, %25 : Tensor, %26 : Tensor = prim::TupleUnpack(%27)\r\n  %11 : Float(10, 10) = aten::relu(%24) # tuple_unpack.py:22:0\r\n  %12 : Float(10, 10) = aten::sigmoid(%25) # tuple_unpack.py:22:0\r\n  %13 : int = prim::Constant[value=1]() # tuple_unpack.py:22:0\r\n  %14 : None = prim::Constant()\r\n  %15 : Float(10, 10) = aten::softmax(%26, %13, %14) # tuple_unpack.py:22:0\r\n  %16 : (Float(10, 10), Float(10, 10), Float(10, 10)) = prim::TupleConstruct(%11, %12, %15)\r\n  return (%16)\r\n```\r\nthe relu should use the third output of the tuple unpack and not the first\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\nin the CBA trace it should show aten::relu(%26) and aten::softmax(%24, %13, %14)\r\nto indicate the tuple element used.\r\n as in the scripted version\r\n```python\r\ngraph(%self : __torch__.CBA,\r\n      %x.1 : Tensor):\r\n  %14 : None = prim::Constant()\r\n  %13 : int = prim::Constant[value=1]() # tuple_unpack.py:22:48\r\n  %2 : __torch__.ReturnTuple = prim::GetAttr[name=\"l\"](%self)\r\n  %4 : (Tensor, Tensor, Tensor) = prim::CallMethod[name=\"forward\"](%2, %x.1) # tuple_unpack.py:21:18\r\n  %a.1 : Tensor, %b.1 : Tensor, %c.1 : Tensor = prim::TupleUnpack(%4)\r\n  %9 : Tensor = aten::relu(%c.1) # tuple_unpack.py:22:15\r\n  %11 : Tensor = aten::sigmoid(%b.1) # tuple_unpack.py:22:25\r\n  %15 : Tensor = aten::softmax(%a.1, %13, %14) # tuple_unpack.py:22:38\r\n  %16 : (Tensor, Tensor, Tensor) = prim::TupleConstruct(%9, %11, %15)\r\n  return (%16)\r\n```\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.14.0\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 435.21\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-include               2019.4                      243  \r\n[conda] mkl-service               2.3.0            py37he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py37ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0  \r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @suo @ezyang @gchanan @zou3519", "labels": ["high priority", "jit", "triage review", "triaged"], "number_of_comments": 0, "created_at": "2020-02-03 17:26:32", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558880466": {"author_username": "shinh", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32926, "title": "[ONNX] torch.onnx emits -1 in shape for Expand", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\ntorch.onnx may emit -1 in the second input of Expand op for `torch.tensor.expand`. It seems ONNX does not allow -1 as dims in the expanded shape. I think torch.onnx should translate -1 to 1.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Run the following script.\r\n1. Notice the Constant node fed to Expand op has a negative value.\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```\r\nimport onnx\r\nimport onnx.numpy_helper\r\nimport torch\r\n\r\nclass Model(torch.nn.Module):\r\n    def forward(self, x):\r\n        return x.expand((-1, 3, 3, 1))\r\n\r\nmodel = Model()\r\ntorch.onnx.export(model, torch.rand(3, 3, 1, 1), 'expand.onnx')\r\n\r\nm = onnx.load('expand.onnx')\r\nn = m.graph.node[0]\r\nassert n.op_type == 'Constant'\r\n# [-1  3  3  1]\r\nprint(onnx.numpy_helper.to_array(n.attribute[0].t))\r\n```\r\n\r\n## Expected behavior\r\n\r\nI think -1 should be translated to 1.\r\n\r\n## Environment\r\n\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.1.243\r\nGPU models and configuration: GPU 0: GeForce MX150\r\nNvidia driver version: 440.33.01\r\ncuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.3.1\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.17.0\r\n[pip3] torch==1.4.0\r\n[pip3] torchvision==0.2.1\r\n[pip3] warpctc-pytorch==0.1\r\n[conda] Could not collect\r\n\r\n\r\n## Additional context\r\n\r\nBoth ONNX's shape inference and ONNX runtime are confused by this.\n\ncc @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof", "labels": ["module: onnx", "triaged"], "number_of_comments": 1, "created_at": "2020-02-03 06:51:52", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558855675": {"author_username": "csummersea", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32925, "title": "Int8 packed weight serializer and deserializer", "body": "Summary: Add serializer/deserializer for int8 packed weights without architecture dependent packing\n\nTest Plan:\n```\nbuck test mode/opt caffe2/caffe2/quantization/server:fully_connected_dnnlowp_op_test\n```\n\nDifferential Revision: D19689332\n\n", "labels": ["fb-exported"], "number_of_comments": 5, "created_at": "2020-02-03 05:40:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558806911": {"author_username": "boren-ms", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32924, "title": "Connect timeout feature do not work in DDP with TCPStore", "body": "## Issue description\r\nThe timeout setting in the function init_process_group in DDP don not work.\r\nI found that after release v1.3.0, the TCPStore initilizaiton function will can connect the server with timeout, but the timeout value is actually set after the TCPStore initialization function.\r\ninit_process_group -> TCPStore()-> set timeout\r\nThis mean the first connect will always use the default timeout (300 seconds).\r\n\r\nin v1.2.0, such call stack will be fine since the first connect in initialization will call with no timeout.\r\n// Connect to the daemon\r\n  storeSocket_ = tcputil::connect(tcpStoreAddr_, tcpStorePort_);\r\n\r\nafter v.1.3.0, the first connect will be\r\n storeSocket_ = tcputil::connect(tcpStoreAddr_, tcpStorePort_, /* wait= */ true, **timeout_**); \r\nwhich will result in the bug.\r\n\r\nmy suggestion is that we can connect to server without timeout like v1.2.0 for the first call.\r\n\r\n## Code example\r\n\r\n\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Build command you used (if compiling from source):\r\n- OS:\r\n- PyTorch version:\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n\n\ncc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar", "labels": ["module: distributed", "triaged"], "number_of_comments": 2, "created_at": "2020-02-03 02:34:23", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558790095": {"author_username": "dylanbespalko", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32920, "title": "WIP: Add FPGATensorId for Xilinx Vitis Devices", "body": "In-tree-changes to allow FPGA device registration intended for Xilinx Vitis compatible devices.\r\n\r\n- [x]  Add FPGATensorId to c10\r\n- [x] Add FPGA dispatch to ATen ops\r\n- [ ] Fix Unary Op Dispatch", "labels": ["open source"], "number_of_comments": 1, "created_at": "2020-02-03 01:05:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558786660": {"author_username": "forresti", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32919, "title": "Torchscript used to work, but now it fails with VariableTensorId error", "body": "## \ud83d\udc1b Bug\r\nWhen I export torchvision's resnet18 to torchscript and run it on Android using the `speed_benchmark_torch` binary, it fails with the following error:\r\n\r\n```\r\nterminating with uncaught exception of type c10::Error: Could not run 'aten::empty.memory_format' with arguments from the 'VariableTensorId' backend. 'aten::empty.memory_format' is only available for these backends: [SparseCPUTensorId, CPUTensorId, MkldnnCPUTensorId]. (dispatch_ at ../aten/src/ATen/core/dispatch/Dispatcher.h:257)\r\n(no backtrace available)\r\nAborted \r\n```\r\n\r\n## History\r\nFor a while, I was using on hash b05d0fa67144d5af0545a333d5792c2e3946b8c5, which was committed to the master branch of pytorch in Oct 2019. I was working with Torchscript models on Android, and things were working fairly well.\r\n\r\nThen, I updated to the latest code tag `v1.4.0`, specifically hash 7f73f1d591afba823daa4a99a939217fb54d7688. After that, the following issue emerged.\r\n\r\n## To Reproduce\r\n\r\n### Model\r\n```\r\n#python\r\nimport torch\r\nimport torchvision\r\n\r\nmodel = torchvision.models.resnet18(pretrained=True)\r\nmodel.eval()\r\nexample = torch.rand(1, 3, 224, 224)\r\ntraced_script_module = torch.jit.trace(model, example)\r\ntraced_script_module.save(\"resnet18.pt\")\r\n```\r\n\r\nAnd, put the model onto the android phone (Pixel 3):\r\n```\r\n#!/bin/bash\r\nadb push resnet18.pt /data/local/tmp/pt/ \r\n\r\n```\r\n\r\n### Binary\r\n```\r\ncd pytorch\r\n\r\nexport ANDROID_DEBUG_SYMBOLS=1 ANDROID_ABI=arm64-v8a\r\nexport ANDROID_NDK=/path/to/Android/Sdk/ndk/21.0.6113669/\r\n./scripts/build_android.sh \\\r\n-DBUILD_BINARY=ON \\\r\n-DBUILD_CAFFE2_MOBILE=OFF \\\r\n-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \\\r\n-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)')\r\n\r\n# compiles build_android/bin/speed_benchmark_torch\r\n\r\nadb push build_android/bin/speed_benchmark_torch /data/local/tmp/pt/ \r\n```\r\n\r\n### Running the model\r\n```\r\nadb shell /data/local/tmp/pt/speed_benchmark_torch --model =/data/local/tmp/pt/resnet18.pt --input_dims=\"1,3,224,224\" --input_type=float --warmup=5 --iter=20\r\n\r\n```\r\n\r\n## Expected behavior\r\n\r\nOn the older hash (b05d0fa67144d5af0545a333d5792c2e3946b8c5), the above would print after running the `adb shell ... speed_benchmark_torch ...` command:\r\n```\r\nStarting benchmark.\r\nRunning warmup runs.\r\n\r\nMain runs.\r\nMain run finished. Milliseconds per iter: 188.382. Iters per second: 5.30836\r\n```\r\n\r\nHowever, on the current `v1.4.0` tag's hash (7f73f1d591afba823daa4a99a939217fb54d7688), it fails with the following error:\r\n```\r\nterminating with uncaught exception of type c10::Error: Could not run 'aten::empty.memory_format' with arguments from the 'VariableTensorId' backend. 'aten::empty.memory_format' is only available for these backends: [SparseCPUTensorId, CPUTensorId, MkldnnCPUTensorId]. (dispatch_ at ../aten/src/ATen/core/dispatch/Dispatcher.h:257)\r\n(no backtrace available)\r\nAborted \r\n```\r\nI'm not too clear on what a `VariableTensorId` is, so I'm not quite sure what's going on. \r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 18.04.3 LTS\r\nGCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\nCMake version: version 3.10.2\r\n\r\nPython version: 3.8\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN V\r\nGPU 1: TITAN V\r\nGPU 2: TITAN V\r\n\r\nNvidia driver version: 430.50\r\ncuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.18.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                         mkl  \r\n[conda] mkl                       2019.4                      243  \r\n[conda] mkl-service               2.3.0            py38he904b0f_0  \r\n[conda] mkl_fft                   1.0.15           py38ha843d7b_0  \r\n[conda] mkl_random                1.1.0            py38h962f231_0  \r\n[conda] pytorch                   1.4.0           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision               0.5.0                py38_cu101    pytorch\r\n```\r\n\r\n\n\ncc @suo", "labels": ["jit", "mobile", "module: android"], "number_of_comments": 3, "created_at": "2020-02-03 00:46:13", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558728163": {"author_username": "ggaziv", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32918, "title": "Interpolate in the \u201cbicubic\u201d mode with the same shape outputs zeros from second sample onwards", "body": "## \ud83d\udc1b Bug\r\nWhen using torch.nn.functional.interpolate with \"bicubic\" mode and target resolution similar to the input 4D tensor, the output tensor has zeros everywhere starting from the second sample.\r\n\r\n## To Reproduce\r\n```\r\n>>> import torch\r\n>>> tmp = torch.arange(2*3*3*4).reshape(2,3,3,4).float()\r\n>>> tmp\r\ntensor([[[[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.],\r\n          [ 8.,  9., 10., 11.]],\r\n\r\n         [[12., 13., 14., 15.],\r\n          [16., 17., 18., 19.],\r\n          [20., 21., 22., 23.]],\r\n\r\n         [[24., 25., 26., 27.],\r\n          [28., 29., 30., 31.],\r\n          [32., 33., 34., 35.]]],\r\n\r\n        [[[36., 37., 38., 39.],\r\n          [40., 41., 42., 43.],\r\n          [44., 45., 46., 47.]],\r\n\r\n         [[48., 49., 50., 51.],\r\n          [52., 53., 54., 55.],\r\n          [56., 57., 58., 59.]],\r\n\r\n         [[60., 61., 62., 63.],\r\n          [64., 65., 66., 67.],\r\n          [68., 69., 70., 71.]]]])\r\n\r\n>>> torch.nn.functional.interpolate(tmp, (3, 4), mode=\"bicubic\")\r\ntensor([[[[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.],\r\n          [ 8.,  9., 10., 11.]],\r\n\r\n         [[12., 13., 14., 15.],\r\n          [16., 17., 18., 19.],\r\n          [20., 21., 22., 23.]],\r\n\r\n         [[24., 25., 26., 27.],\r\n          [28., 29., 30., 31.],\r\n          [32., 33., 34., 35.]]],\r\n\r\n        [[[ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.]],\r\n\r\n         [[ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.],\r\n          [ 0.,  0.,  0.,  0.]]]])\r\n```\r\n\r\n## Expected behavior\r\n```\r\n>>> torch.nn.functional.interpolate(tmp, (3, 4), mode=\"bicubic\")`\r\ntensor([[[[ 0.,  1.,  2.,  3.],\r\n          [ 4.,  5.,  6.,  7.],\r\n          [ 8.,  9., 10., 11.]],\r\n\r\n         [[12., 13., 14., 15.],\r\n          [16., 17., 18., 19.],\r\n          [20., 21., 22., 23.]],\r\n\r\n         [[24., 25., 26., 27.],\r\n          [28., 29., 30., 31.],\r\n          [32., 33., 34., 35.]]],\r\n\r\n        [[[36., 37., 38., 39.],\r\n          [40., 41., 42., 43.],\r\n          [44., 45., 46., 47.]],\r\n\r\n         [[48., 49., 50., 51.],\r\n          [52., 53., 54., 55.],\r\n          [56., 57., 58., 59.]],\r\n\r\n         [[60., 61., 62., 63.],\r\n          [64., 65., 66., 67.],\r\n          [68., 69., 70., 71.]]]])\r\n```\r\n## Environment\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: CentOS Linux 7 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-36)\r\nCMake version: version 2.8.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration:\r\nGPU 0: Tesla V100-PCIE-16GB\r\nGPU 1: Tesla V100-PCIE-16GB\r\nGPU 2: Tesla V100-PCIE-16GB\r\n\r\nNvidia driver version: 410.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] _tflow_select             2.3.0                       mkl\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2019.4                      243\r\n[conda] mkl-service               2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                   1.0.14           py37ha843d7b_0\r\n[conda] mkl_random                1.1.0            py37hd6b4f25_0\r\n[conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] tensorflow                2.0.0           mkl_py37h66b46cc_0\r\n[conda] tensorflow-base           2.0.0           mkl_py37h9204916_0\r\n[conda] torchvision               0.5.0                py37_cu101    pytorch\n\ncc @ezyang @gchanan @zou3519", "labels": ["high priority", "triaged"], "number_of_comments": 1, "created_at": "2020-02-02 17:26:01", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558701057": {"author_username": "ludwigwinkler", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32917, "title": "Docfix for #31986", "body": "Docfix for ``torch.nn.Module.modules()``  [(https://github.com/pytorch/pytorch/issues/31986)].\r\n\r\nIf ``modules()`` is called inside a module, it also returns itself which can lead to unbounded recursion causing the system to abort the function call.", "labels": ["open source", "triaged"], "number_of_comments": 0, "created_at": "2020-02-02 14:03:43", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558699408": {"author_username": "ksiv", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32916, "title": " from torch._C import * (ImportError: DLL load failed)", "body": "## \ud83d\udc1b Bug\r\nBad import error handling\r\nBy rising this issue my target not to get it to work \"somehow\"\r\nBut to improve the error handling giving to the people an opportunity to understand what went wrong.\r\nI seen A LOT of such issues through the internet where solution was to reinstall totally different components to make it work and even downgrade the product which is totally NOT acceptable in today's world.\r\nOther suggestion is to change packaging system which is not relate to the issue but worked for some people.\r\nFor some people adding some Cuda libs into some unpredictable place works\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html\r\n2. python\r\n3. help()\r\n4. modules torch\r\n\r\nHere is a list of modules whose name or summary contains 'torch'.\r\nIf there are any, enter a module name to get more help.\r\n\r\n<generator object walk_packages at 0x000002AA581F4A48>\r\n<generator object walk_packages at 0x000002AA581F4AC8>\r\ntorch - The torch package contains data structures for multi-dimensional\r\n<generator object walk_packages at 0x000002AA581F4AC8>\r\ntorchfile - Mostly direct port of the Lua and C serialization implementation to\r\ntorchvision\r\n<generator object walk_packages at 0x000002AA581F4AC8>\r\n\r\n5. quit\r\n6. >>> import torch\r\n<generator object walk_packages at 0x000002AA54151CC8>\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\__init__.py\", line 86, in <module>\r\n    from torch._C import *\r\nImportError: DLL load failed: Module not found.\r\n7. add print(_dl_flags.environ['PATH']) into __init__\r\nshow me many paths including \r\nC:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;\r\nthis is the place where torch.dll is \r\nSo it definitely sees the torch but which DLL it wants to load? \r\nrunning python with -v key did not help. \r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nExpected: \r\n-(min ) error contains what file it was looking for. So one should be able to find this in their system and fix the path OR not to find it in the system and add it into it.\r\n -(max) user friendly and at the same time useful error description\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n\r\n```\r\n\r\n C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\\Library\\bin;C:\\Program Files\\NVIDIA Corporation\\NvToolsExt\\bin\\x64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\libnvvp;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Intel\\iCLS Client\\;C:\\Program Files\\Intel\\iCLS Client\\;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\IPT;C:\\Program Files (x86)\\Windows Live\\Shared;d:\\php\\;C:\\Program Files\\nodejs\\;C:\\ProgramData\\ComposerSetup\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\;C:\\Program Files\\dotnet\\;C:\\Program Files\\TortoiseGit\\bin;C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2019.5.0\\;c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\lib\\x64;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;C:\\Users\\Asus\\AppData\\Roaming\\npm;C:\\Users\\Asus\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Asus\\AppData\\Roaming\\Composer\\vendor\\bin;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts;c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\lib\\x64;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\torch\\lib;;C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\numpy\\.libs\r\n<generator object walk_packages at 0x000002517FB64648>\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\n\r\nOS: Microsoft Windows 10 Home Single Language\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: N/A\r\nCUDA runtime version: 10.2.89\r\nGPU models and configuration: GPU 0: GeForce GTX 850M\r\nNvidia driver version: 441.22\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.18.1\r\n[pip3] torch==1.4.0\r\n[pip3] torchfile==0.1.0\r\n[pip3] torchvision==0.5.0\r\n[conda] Could not collect\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n\r\n\r\nThanks!", "labels": ["triaged"], "number_of_comments": 6, "created_at": "2020-02-02 13:51:15", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558585074": {"author_username": "adamlerer", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32912, "title": "Optional seq_len argument to torch.nn.utils.rnn.pad_sequence", "body": "## \ud83d\ude80 Feature\r\n\r\ntorch.nn.utils.rnn.pad_sequence should take an optional argument that specifies the padded length of the sequence.\r\n\r\n## Motivation\r\n\r\nSuppoose I have a batch of sentences of different lengths 1-100, and I want to pad batches to length 100 (because say my model only takes length-100 sequences). You can't currently do that.\r\n\r\n## Alternatives\r\n\r\nYou can hack it by cat-ing an extra tensor onto the end of the padded tensor.\r\n", "labels": ["enhancement", "module: operators", "triaged"], "number_of_comments": 1, "created_at": "2020-02-01 19:24:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558567157": {"author_username": "bnehoran", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32910, "title": "isfinite() as a torch.Tensor method", "body": "## \ud83d\ude80 Feature\r\n\r\nAllow `isfinite()` to be a tesnor method, so that it can be called as `x.isfinite()` instead of `torch.isfinite(x)`. That is, allow both options.\r\n\r\n#### Example usage:\r\n```python\r\n# slightly awkward code\r\nprint(torch.isfinite(x.sqrt()).sum().item())\r\n\r\n# much neater\r\nprint(x.sqrt().isfinite().sum().item())\r\n```\r\n_(Note: This is a simplification of an actual use case. Of course, for `sqrt()` you could just check for negative inputs.)_\r\n\r\nAs an added bonus, if numpy also follows suit, it could allow writing simpler code that seamlessly works on both PyTorch tensors and numpy ndarrays (deciding between `torch.isfinite(x)` and `np.isfinite(x)` vs simply `x.isfinite()`).\r\nThere may very well be other functions for which this could be useful. This is just one that\u2019s been bothering me a lot.", "labels": ["enhancement", "module: operators", "triaged"], "number_of_comments": 0, "created_at": "2020-02-01 17:02:23", "reactions": {"total_count": 1, "+1": 1, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558524768": {"author_username": "Coderx7", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32909, "title": "Some questions Concerning Intel's DNNL(MKLDNN) support in Pytorch (adding support for Intel Processors GPUs) by transitioning to DNNL", "body": "## \u2753 Questions and Help\r\n[Deep Neural Network Library (DNNL)](https://github.com/intel/mkl-dnn) is an open-source performance library for deep learning applications. The library includes basic building blocks for neural networks optimized for Intel Architecture Processors and Intel Processor **Graphics**.  \r\n\r\nDNNL supports systems based on Intel 64 or AMD64 architecture.\r\n\r\nThe library is optimized for the following CPUs:\r\n-   Intel Atom processor with Intel SSE4.1 support\r\n-   4th, 5th, 6th, 7th, and 8th generation Intel Core(TM) processor\r\n-   Intel Xeon(R) processor E3, E5, and E7 family (formerly Sandy Bridge, Ivy Bridge, Haswell, and Broadwell)\r\n-  Intel Xeon Phi(TM) processor (formerly Knights Landing and Knights Mill)\r\n-  Intel Xeon Scalable processor (formerly Skylake and Cascade Lake)\r\n-  future Intel Xeon Scalable processor (code name Cooper Lake)\r\n\r\nand it is also optimized for the following GPUs:\r\n\r\n-  Intel HD Graphics\r\n-  Intel UHD Graphics\r\n-  Intel Iris Plus Graphics\r\n\r\nCurrently The MKLDNN version shipped with Pytorch as of yesterday (last nightly build) is version 0.21.1 which dates back to sep 2019. Since that day, Intel has introduced DNNL instead of MKLDNN.\r\nAnd since version 1.0, there are breaking changes.  \r\nMy Questions thus are : \r\n1. Do we need a lot of work to transision from MKLDNN 0.21.1 to DNNL 1.2 (which provides a lot of benfits in  terms of supporting accelerated computing using integrated GPUs) ? [here](https://intel.github.io/mkl-dnn/dev_guide_transition_to_dnnl.html)'s a transision guide by the way, which seems to be not that time consuming. (excuse my ignorance if thats not the case)   \r\n2. Knowing that DNNL supports both CPUs and Intel **GPUs** for accelerated computing, this would be a great addition to Pytorch. My question thus is, in case we transition to DNNL, would updating the DNNL library, require recomplition, or would it just work out of the box (specially on Python part) ? \r\n3. In order to incorporate this into Pytorch, what else is required other than the transition  from mkldnn 0.21.1 to the latest version? are there any complications in this regard? \r\n\r\nThank you all in advance \r\n \r\n\n\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh", "labels": ["module: mkldnn", "triaged"], "number_of_comments": 1, "created_at": "2020-02-01 11:09:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558521825": {"author_username": "sarkararpan710", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32908, "title": "Trying to compile Pytorch for contributing to the codebase.", "body": "Hi, I have wanting to understand the PyTorch codebase and am interested to  contributing to PyTorch and as a first step, I am trying to compile PyTorch from source using the command python setup.py develop.\r\n\r\nPreviously I was able to compille PyTorch without the develop flag in order to do the FastAI course.\r\nBut while compiling it for develop mode. I am getting this error. I tried looking for solutions and saw many people able to get through this error by compiling protobuf from source. I compiled protobuf from source using Visual Studio Solution file. \r\n\r\nThis is the error I am facing when trying to build PyTorch for development\r\n\r\n`(base) D:\\pytorch\\pytorch\\pytorch>python setup.py develop\r\nBuilding wheel torch-1.5.0a0+040bc1d\r\n-- Building version 1.5.0a0+040bc1d\r\ncmake --build . --target install --config Release -- -j 8\r\n[2/804] Linking CXX shared library bin\\torch_cpu.dll\r\nFAILED: bin/torch_cpu.dll lib/torch_cpu.lib\r\ncmd.exe /C \"cd . && D:\\CMake\\bin\\cmake.exe -E vs_link_dll --intdir=caffe2\\CMakeFiles\\torch_cpu.dir --manifests  -- \"D:\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\Hostx64\\x64\\link.exe\" /nologo @CMakeFiles/torch_cpu.rsp  /out:bin\\torch_cpu.dll /implib:lib\\torch_cpu.lib /pdb:bin\\torch_cpu.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /INCREMENTAL:NO   && cd .\"\r\nLINK: command \"D:\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\Hostx64\\x64\\link.exe /nologo @CMakeFiles/torch_cpu.rsp /out:bin\\torch_cpu.dll /implib:lib\\torch_cpu.lib /pdb:bin\\torch_cpu.pdb /dll /version:0.0 /machine:x64 /ignore:4049 /ignore:4217 /INCREMENTAL:NO /MANIFEST /MANIFESTFILE:bin\\torch_cpu.dll.manifest\" failed (exit code 1120) with the following output:\r\n   Creating library lib\\torch_cpu.lib and object lib\\torch_cpu.exp\r\nlibprotobuf.lib(text_format.cc.obj) : error LNK2019: unresolved external symbol __std_reverse_trivially_swappable_8 referenced in function \"void __cdecl std::_Buffered_inplace_merge_divide_and_conquer2<class google::protobuf::Message const * *,class google::protobuf::DynamicMapSorter::MapEntryMessageComparator>(class google::protobuf::Message const * *,class google::protobuf::Message const * *,class google::protobuf::Message const * *,__int64,__int64,class google::protobuf::Message const * * const,__int64,class google::protobuf::DynamicMapSorter::MapEntryMessageComparator,class google::protobuf::Message const * *,class google::protobuf::Message const * *,__int64,__int64)\" (??$_Buffered_inplace_merge_divide_and_conquer2@PEAPEBVMessage@protobuf@google@@VMapEntryMessageComparator@DynamicMapSorter@23@@std@@YAXPEAPEBVMessage@protobuf@google@@00_J1QEAPEBV123@_JVMapEntryMessageComparator@DynamicMapSorter@23@0011@Z)\r\nlibprotobuf.lib(wire_format.cc.obj) : error LNK2001: unresolved external symbol __std_reverse_trivially_swappable_8\r\nbin\\torch_cpu.dll : fatal error LNK1120: 1 unresolved externals\r\nninja: build stopped: subcommand failed.\r\nTraceback (most recent call last):\r\n  File \"setup.py\", line 737, in <module>\r\n    build_deps()\r\n  File \"setup.py\", line 316, in build_deps\r\n    cmake=cmake)\r\n  File \"D:\\pytorch\\pytorch\\pytorch\\tools\\build_pytorch_libs.py\", line 62, in build_caffe2\r\n    cmake.build(my_env)\r\n  File \"D:\\pytorch\\pytorch\\pytorch\\tools\\setup_helpers\\cmake.py\", line 339, in build\r\n    self.run(build_args, my_env)\r\n  File \"D:\\pytorch\\pytorch\\pytorch\\tools\\setup_helpers\\cmake.py\", line 141, in run\r\n    check_call(command, cwd=self.build_dir, env=env)\r\n  File \"D:\\Anaconda\\lib\\subprocess.py\", line 291, in check_call\r\n    raise CalledProcessError(retcode, cmd)\r\nsubprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '8']' returned non-zero exit status 1.`\n\ncc @peterjc123", "labels": ["module: build", "module: windows", "triaged"], "number_of_comments": 1, "created_at": "2020-02-01 10:42:12", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558506735": {"author_username": "suo", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32906, "title": "[wip] improving error messages", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32906 [wip] improving error messages**\n* #32880 [jit] de-optionalize SourceRange context\n* #32879 [jit] Delete the ErrorReport default constructor\n\n", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-01 08:25:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558500512": {"author_username": "tma15", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32903, "title": "Fix: add state initialization in step of Adagrad", "body": "After applying `add_param_group` of Adagrad, KeyError is raised because `state` for newly added parameters is not initialized. This PR fixes this issue.\r\n\r\n```python\r\nimport torch\r\n\r\ndef case1():\r\n    linear = torch.nn.Linear(3, 5)\r\n    optimizer = torch.optim.Adagrad(linear.parameters())\r\n\r\n    x = torch.randn(2, 3)\r\n    y = linear(x)\r\n\r\n    loss = y.sum()\r\n    loss.backward()\r\n\r\n    optimizer.step()\r\n\r\n\r\ndef case2():\r\n    linear = torch.nn.Linear(3, 5)\r\n    optimizer = torch.optim.Adagrad(linear.parameters())\r\n\r\n    linear2 = torch.nn.Linear(5, 4)\r\n    optimizer.add_param_group({'params': [linear2.weight, linear2.bias]})\r\n\r\n    x = torch.randn(2, 3)\r\n    y1 = linear(x)\r\n    y2 = linear2(y1)\r\n\r\n    loss = y2.sum()\r\n    loss.backward()\r\n\r\n    optimizer.step()\r\n\r\n\r\nif __name__ == '__main__':\r\n    case1()\r\n    case2()\r\n\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-adagrad.py\", line 35, in <module>\r\n    case2()\r\n  File \"test-adagrad.py\", line 30, in case2\r\n    optimizer.step()\r\n  File \"/Users/takuya/.pyenv/versions/3.7.3/lib/python3.7/site-packages/torch/optim/adagrad.py\", line 76, in step\r\n    state['step'] += 1\r\nKeyError: 'step'\r\n```", "labels": ["open source", "triaged"], "number_of_comments": 8, "created_at": "2020-02-01 07:28:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558485288": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32902, "title": "[test only][no review] test autograd profiler stuff", "body": "", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-02-01 05:22:05", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558476981": {"author_username": "z-a-f", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32901, "title": "QNNPACK linear doesn't preserve dimensions", "body": "When using linear, in the fbgemm and fp implementations the first matrix can be of any rank, while the second has to be of rank 2. The result is expected to be the same rank as the first matrix. For example:\r\nif multiplying two matrices A and B with these dimensions: \r\n\r\n```\r\nA.shape = [b, m, n]\r\nB.shape = [n, k]\r\n```\r\n\r\nThe resulting matrix C should have dimensions\r\n\r\n```\r\n(C = A @ B).shape = [b, m, k]\r\n```\r\n\r\nHowever in QNNPACK the result is `[b*m, k]`\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a", "labels": ["quantization", "triaged"], "number_of_comments": 0, "created_at": "2020-02-01 04:18:54", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558465175": {"author_username": "tinhchuquang", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32898, "title": "OneCycleLearningrate is bug in version", "body": "## \ud83d\udc1b Bug\r\n\r\n```\r\n scheduler.step()       \r\n File \"anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\", line 150, in step\r\nfor param_group, lr in zip(self.optimizer.param_groups,values):\r\nUnboundLocalError: local variable 'values' referenced before assignment\r\n```\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. `scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 1e-2, total_steps=None, epochs=n_epochs, steps_per_epoch=5021, pct_start=0.0,anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=100.0)`\r\n2. `optimizer.step()`\r\n    `optimizer.zero_grad()`\r\n3. `scheduler.step()`\r\n\r\n## Expected behavior\r\n## Environment\r\n - PyTorch Version (e.g., 1.4): 1.4.0\r\n - OS (e.g., Linux):\r\n - How you installed PyTorch (`conda`, `pip`, source): pip \r\n - Build command you used (if compiling from source):  pip install torch\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.2\r\n - GPU models and configuration: no config\r\n - Any other relevant information:\r\n\r\n\r\ncc @ezyang @gchanan @zou3519 @vincentqb @SsnL @albanD @gqchen", "labels": ["high priority", "module: optimizer", "triaged"], "number_of_comments": 11, "created_at": "2020-02-01 02:36:22", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558459469": {"author_username": "orionr", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32896, "title": "Refine caffe2/caffe2/python for eventual python3 only", "body": "Summary: Cleanup caffe2/caffe2/python for python3 support.\n\nTest Plan: Wait for tests to run.\n\nDifferential Revision: D19674482\n\n", "labels": ["fb-exported"], "number_of_comments": 2, "created_at": "2020-02-01 01:56:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558448604": {"author_username": "z-a-f", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32892, "title": "Unittests for c10 types check from jit tracing", "body": "An issue was raised that some ops registered with c10 were not correctly handled in jit tracing. #32886 resolves one of such issues. \r\n\r\nProbably need unittests for c10 op registrations + tracing/scripting\n\ncc @suo", "labels": ["jit", "module: dispatch"], "number_of_comments": 1, "created_at": "2020-02-01 00:52:18", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558435934": {"author_username": "xush6528", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32891, "title": "Add missing test launchers for RpcJitTest and DistAutogradJitTest", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32891 Add missing test launchers for RpcJitTest and DistAutogradJitTest**\n\n- Add DistAutoGradJitTest into fork test launcher\n- Add RpcJitTest into fork/spawn test launcher\n\nDifferential Revision: [D5785394](https://our.internmc.facebook.com/intern/diff/D5785394/)\n\n**NOTE FOR REVIEWERS**: This PR has internal Facebook specific changes or comments, please review them on [Phabricator](https://our.internmc.facebook.com/intern/diff/D5785394/)!", "labels": [], "number_of_comments": 1, "created_at": "2020-02-01 00:02:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558414324": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32884, "title": "[RFC] Nested scopes in autograd profiler should support RPC calls properly.", "body": "## \ud83d\ude80 Feature\r\nAs described in https://github.com/pytorch/pytorch/issues/32517, nested scopes in the autograd profiler do not work if there are RPC calls in the scope, an example is as follows:\r\n\r\n```\r\nwith torch.autograd.profiler.profile() as foo:\r\n    with torch.autograd.profiler.record_function():\r\n        rpc.rpc_async(...).wait()\r\n```\r\n\r\nAfter a discussion with @pritamdamania87, we'd like to propose the following solution for enabling this to work with nested scopes:\r\n\r\n1. Currently, setting and unsetting parent pointers to the current, thread-local active record_function exists inside `RecordFunction::processCallbacks()` (run at the beginning of the function) and `RecordFunction::end()` (run at the end of the function). \r\n\r\nWe'd like to change these APIs so that `end()` is separated from the code that sets `thread_local_func_ = parent`, into two separate APIs. Users can still call `end()` to do both at once and that will be the normal use case, but for RPC we will call them separately. The newly introduced APIs will be `RecordFunction::runEndCallbacks()` which will just run the end callbacks and not reset the `thread_local_func_`. Instead, this will be handled by `RecordFunction::resetThreadLocalFunc` (or some other name). This function will ensure that the currently active record function points to the parent RecordFunction so that we handle scopes correctly. \r\n\r\n\r\n2. When returning from `rpc.rpc_async()` we will call `RecordFunction::resetThreadLocalFunc` to correctly handle the nested scopes. Note that this isn't the end of the RPC since the future has not been satisfied yet. When the future corresponding to this RPC is satisfied we will call `RecordFunction::runEndCallbacks()` as we currently do to correctly accumulate the profiling information.\r\n\r\n3. I filed https://github.com/pytorch/pytorch/issues/32883 to track this more closely, but we'd also like to start a longer term discussion on the behavior for the profiler under multithreaded scenarios. For example, node A could be running the following code in one thread:\r\n```\r\nwith torch.autograd.profiler.profile() as prof:\r\n    rpc.rpc_async(nodeB, ...)\r\n```\r\nbut this could happen while node A is executing torch ops in a separate thread to respond to an RPC from B. In this case, there may be races where those RPCs show up in the profile output but they should not. \r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: autograd", "module: rpc", "triaged"], "number_of_comments": 5, "created_at": "2020-01-31 22:47:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558411270": {"author_username": "rohan-varma", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32883, "title": "Issues with multithreading in autograd profiler", "body": "## \ud83d\udc1b Bug\r\nWas experimenting with using the autograd profiler in multithreaded scenarios and ran into a couple of issues that I would like to bring up:\r\n\r\n1. Crashes when profiling is run on two separate threads: \r\nRepro (might take a couple tries):\r\n```\r\ndef test_two_threads_crash(self):\r\n        import threading\r\n        def torch_adds():\r\n            with torch.autograd.profiler.profile() as prof:\r\n                return torch.add(1,1)\r\n            print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\r\n        def torch_mul():\r\n            with torch.autograd.profiler.profile() as p1:\r\n                torch.mul(1,1)\r\n\r\n        t1 = threading.Thread(target=torch_adds, args=())\r\n        t2 = threading.Thread(target=torch_mul, args=())\r\n        t1.start() ; t2.start()\r\n        t1.join() ; t2.join()\r\n```\r\nStacktrace:\r\n```\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib64/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib64/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test/test_autograd.py\", line 258, in torch_mul\r\n    torch.mul(1,1)\r\n  File \"/home/rvarm1/pytorch/torch/autograd/profiler.py\", line 286, in __exit__\r\n    records = torch.autograd._disable_profiler()\r\nRuntimeError: can't disable profiler when it's not running\r\n```\r\n\r\nIt looks like there is a race to disable the profiler and the thread that loses has the exception thrown. What I would expect here is the profiler output being printed for both threads, one should have torch.add() and the other should have torch.mul(), since that is what we are explicitly profiling on the respective threads.\r\n\r\n2. Segfault when profiling is running on one thread, and we are running threads with torch ops in the background:\r\nRepro (Might take a couple tries):\r\n```\r\ndef test_multithread_profiler(self):\r\n        import threading\r\n        def torch_mul():\r\n            # yield\r\n            import time ; time.sleep(0)\r\n            return torch.mul(1,1)\r\n\r\n        t2 = threading.Thread(target=torch_mul, args=())\r\n        t2.start()\r\n        with torch.autograd.profiler.profile() as prof:\r\n            import time ; time.sleep(0) # yield\r\n            torch.add(1,1)\r\n\r\n        t2.join()\r\n        print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\r\n```\r\nStacktrace is just `Segmentation fault (core dumped)`\r\n\r\nIt looks like there is a segfault in `disableProfiler()`. I don't think we should crash here, the profile should be printed with only a `torch.add()` since we aren't explicitly profiling the other threads.\r\n\r\n## Environment\r\n\r\nThis happens when built from source on latest master.\r\n\r\n\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar", "labels": ["module: autograd", "module: rpc", "topic: crash", "topic: multithreading", "triaged"], "number_of_comments": 7, "created_at": "2020-01-31 22:39:10", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558408656": {"author_username": "zihaozhang9", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32881, "title": "CUDNN_STATUS_EXECUTION_FAILED", "body": "When I train the code, the following problem suddenly appears\r\n```\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 211, in <module>\r\n    train(args,model,train_loader,optimizer,epoch)\r\n  File \"/media/nnir712/F264A15264A119FD/zzh/detect/yolo/zzh_yolo/zzh_yolo_mobilenet/train.py\", line 22, in train\r\n    loss, outputs = model(imgs, targets)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/nnir712/F264A15264A119FD/zzh/detect/yolo/zzh_yolo/zzh_yolo_mobilenet/models.py\", line 297, in forward\r\n    x = module(x)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 100, in forward\r\n    input = module(input)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 532, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 345, in forward\r\n    return self.conv2d_forward(input, self.weight)\r\n  File \"/home/nnir712/anaconda3/envs/zzhpy36/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 342, in conv2d_forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\n```\r\nSystem information\r\n```\r\nubuntu18.04\r\n\r\ngmp                       6.1.2                h6c8ec71_1    defaults\r\ngoogle-auth               1.10.0                   pypi_0    pypi\r\ngoogle-auth-oauthlib      0.4.1                    pypi_0    pypi\r\ngrpcio                    1.26.0                   pypi_0    pypi\r\ngst-plugins-base          1.14.0               hbbd80ab_1    defaults\r\ngstreamer                 1.14.0               hb453b48_1    defaults\r\nicu                       58.2                 h9c2bf20_1    defaults\r\nidna                      2.8                      pypi_0    pypi\r\nimagecorruptions          1.0.0                    pypi_0    pypi\r\nimageio                   2.6.1                    pypi_0    pypi\r\nimgaug                    0.2.6                    pypi_0    pypi\r\nimportlib_metadata        1.4.0                    py36_0    defaults\r\nintel-openmp              2019.4                      243    defaults\r\nipykernel                 5.1.4            py36h39e3cac_0    defaults\r\nipython                   7.11.1           py36h39e3cac_0    defaults\r\nipython_genutils          0.2.0                    py36_0    defaults\r\nipywidgets                7.5.1                      py_0    defaults\r\njedi                      0.16.0                   py36_0    defaults\r\njinja2                    2.10.3                     py_0    defaults\r\njpeg                      9b                   h024ee3a_2    defaults\r\njsonpatch                 1.24                     pypi_0    pypi\r\njsonpointer               2.0                      pypi_0    pypi\r\njsonschema                3.2.0                    py36_0    defaults\r\njupyter                   1.0.0                    py36_7    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\njupyter_client            5.3.4                    py36_0    defaults\r\njupyter_console           6.1.0                      py_0    defaults\r\njupyter_core              4.6.1                    py36_0    defaults\r\nleveldb                   1.20                 hf484d3e_1    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nlibedit                   3.1.20181209         hc058e9b_0    defaults\r\nlibffi                    3.2.1                hd88cf55_4    defaults\r\nlibgcc-ng                 9.1.0                hdf63c60_0    defaults\r\nlibgfortran-ng            7.3.0                hdf63c60_0    defaults\r\nlibpng                    1.6.37               hbc83047_0    defaults\r\nlibprotobuf               3.8.0                hd408876_0    defaults\r\nlibsodium    \r\noauthlib                  3.1.0                    pypi_0    pypi\r\nolefile                   0.46                     py36_0    defaults\r\nopencv-contrib-python     3.4.1.15                 pypi_0    pypi\r\nopencv-python             4.1.2.30                 pypi_0    pypi\r\nopenssl                   1.1.1d               h7b6447c_3    defaults\r\npandoc                    2.2.3.2                       0    defaults\r\npandocfilters             1.4.2                    py36_1    defaults\r\nparso                     0.6.0                      py_0    defaults\r\npbr                       5.1.3                    pypi_0    pypi\r\npcre                      8.43                 he6710b0_0    defaults\r\npexpect                   4.8.0                    py36_0    defaults\r\npickleshare               0.7.5                    py36_0    defaults\r\npillow                    5.0.0                    pypi_0    pypi\r\npip                       20.0.2                   py36_1    defaults\r\nprometheus_client         0.7.1                      py_0    defaults\r\nprompt_toolkit            3.0.3                      py_0    defaults\r\nprotobuf                  3.8.0            py36he6710b0_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\nptyprocess                0.6.0                    py36_0    defaults\r\npyasn1                    0.4.8                    pypi_0    pypi\r\npyasn1-modules            0.2.7                    pypi_0    pypi\r\npycocotools               2.0.0                    pypi_0    pypi\r\npygments                  2.5.2                      py_0    defaults\r\npyqt                      5.9.2            py36h05f1152_2    defaults\r\npyrsistent                0.15.7           py36h7b6447c_0    defaults\r\npython                    3.6.8                h0371630_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\r\npython-dateutil           2.8.1                      py_0    defaults\r\npytorch                   1.4.0           py3.6_cuda10.1.243_cudnn7.6.3_0    pytorch\r\npyzmq                     18.1.0           py36he6710b0_0    defaults\r\nqt                        5.9.7                h5867ecd_1    defaults\r\nqtconsole                 4.6.0                      py_1    defaults\r\nreadline                  7.0                  h7b6447c_5    defaults\r\nrequests                  2.22.0                   pypi_0    pypi\r\nrequests-oauthlib         1.3.0                    pypi_0    pypi\r\nrsa                       4.0                      pypi_0    pypi\r\nscikit-image              0.16.2                   pypi_0    pypi\r\nscipy                     1.4.1                    pypi_0    pypi\r\nsend2trash                1.5.0                    py36_0    defaults\r\nsetuptools                41.2.0                   pypi_0    pypi\r\nsip                       4.19.8           py36hf484d3e_0    defaults\r\nsix                       1.14.0                   py36_0    defaults\r\nsnappy                    1.1.7                hbae5bb6_3    defaults\r\nsqlite                    3.30.1               h7b6447c_0    defaults\r\ntb-nightly                2.0.0a20190915           pypi_0    pypi\r\ntensorboard               2.1.0                    pypi_0    pypi\r\ntensorflow                1.13.1                   pypi_0    pypi\r\ntensorflow-estimator      1.13.0                   pypi_0    pypi\r\nterminado                 0.8.3                    py36_0    defaults\r\nterminaltables            3.1.0                    pypi_0    pypi\r\ntestpath                  0.4.4                      py_0    defaults\r\nthop                      0.0.31-1912272122          pypi_0    pypi\r\ntk                        8.6.8                hbc83047_0    defaults\r\ntorchfile                 0.1.0                    pypi_0    pypi\r\ntorchvision               0.5.0                py36_cu101    pytorch\r\ntornado                   6.0.3            py36h7b6447c_0    defaults\r\ntqdm                      4.35.0                   pypi_0    pypi\r\ntraitlets                 4.3.3                    py36_0    defaults\r\nurllib3                   1.25.7                   pypi_0    pypi\r\nvisdom                    0.1.8.9                  pypi_0    pypi\r\nwcwidth                   0.1.7                    py36_0    defaults\r\nwebencodings              0.5.1                    py36_1    defaults\r\nwebsocket-client          0.57.0                   pypi_0    pypi\r\nwheel                     0.34.1                   py36_0    defaults\r\nwidgetsnbextension        3.5.1                    py36_0    defaults\r\nxz                        5.2.4                h14c3975_4    defaults\r\nzeromq                    4.3.1                he6710b0_3    defaults\r\nzipp                      0.6.0                      py_0    defaults\r\nzlib                      1.2.11               h7b6447c_3    defaults\r\nzstd                      1.3.7                h0b5b093_0    defaults\r\n```\r\nAND :\r\n```\r\n` export | grep cuda`\r\ndeclare -x CUDA_HOME=\"/usr/local/cuda-10.1\"\r\ndeclare -x LD_LIBRARY_PATH=\"\u201d/usr/local/cuda-10.1/lib64:/home/nnir712/anaconda3/pkgs/libprotobuf-3.5.2-h6f1eeef_0/lib:/home/nnir712/anaconda3/pkgs/libprotobuf-3.8.0-hd408876_0/lib:BRARY_PATH:/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/extras/CUPTI/lib64\u201d:/home/nnir712/anaconda3/lib\"\r\ndeclare -x PATH=\"/home/nnir712/.local/bin:/home/nnir712/bin:/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/bin:/home/nnir712/anaconda3/pkgs/libprotobuf-3.5.2-h6f1eeef_0/bin:/home/nnir712/anaconda3/pkgs/libprotobuf-3.8.0-hd408876_0/bin:/home/nnir712/.local/bin:/home/nnir712/bin:/usr/local/cuda-10.1/bin:/usr/local/cuda-10.1/bin:/home/nnir712/anaconda3/pkgs/libprotobuf-3.5.2-h6f1eeef_0/bin:/home/nnir712/anaconda3/pkgs/libprotobuf-3.8.0-hd408876_0/bin:/home/nnir712/anaconda3/envs/zzhpy36/bin:/home/nnir712/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/nnir712/anaconda3/lib:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/nnir712/anaconda3/lib\"\r\n\r\n```\r\n\n\ncc @ngimel", "labels": ["module: cuda", "triaged"], "number_of_comments": 4, "created_at": "2020-01-31 22:32:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558363554": {"author_username": "pandeykartikey", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32876, "title": "Adds LogCumSumExp", "body": "Resolves #26411", "labels": ["open source", "triaged"], "number_of_comments": 7, "created_at": "2020-01-31 20:45:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558342768": {"author_username": "jiecaoyu", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32874, "title": "add 2-stage SparseAdagrad interface", "body": "Summary: Add an option for user to keep JIT'ed SparseAdagrad kernel\r\n\r\nDifferential Revision: D19528240\r\n\r\n", "labels": [], "number_of_comments": 1, "created_at": "2020-01-31 19:59:54", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558293924": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32872, "title": "Confusing error messages of tensor.scatter_ on both CPU and CUDA", "body": "Both messages seem to be wrong and confusing. basically the V should have the same dimensions as I. Input/output tensor have nothing to do with this dimension check (failing argument inds are different and confusing too). However it would be good if `torch.gather` and `tensor.scatter_` supported broadcasting (adding extra dimensions or at least expanding if all dimensions are present).\r\n\r\nThese error messages also have different terminology compared to the docs. They term `self` as `output tensor` and `src`/`V` as `input tensor` (another error message of this kind `RuntimeError: invalid argument 3: Index tensor must be either empty or have same dimensions as input tensor at /opt/conda/conda-bld/pytorch_1577225354628/work/aten/src/THC/generic/THCTensorScatterGather.cu:115`)\r\n\r\n```python\r\nimport torch\r\n\r\ndevice = 'cuda'\r\n\r\nres = torch.zeros(1, 87810, device = device, dtype = torch.long)\r\nI = torch.ones(1, 148661, device = device, dtype = torch.long)\r\nV = torch.arange(148661, device = device, dtype = torch.long)\r\n\r\nres.scatter_(-1, I, V.unsqueeze(0)) # works\r\n\r\nres.scatter_(-1, I, V)\r\n#CUDA:\r\n#Traceback (most recent call last):\r\n#  File \"bug.py\", line 11, in <module>\r\n#      res.scatter_(-1, I, V) # fails with:\r\n#     RuntimeError: invalid argument 3: Index tensor must be either empty or have same dimensions as input tensor at /opt/conda/conda-bld/pytorch_1577225354628/work/aten/src/THC/generic/THCTensorScatterGather.cu:115\r\n\r\n#CPU:\r\n#Traceback (most recent call last):\r\n#  File \"bug.py\", line 11, in <module>\r\n#      res.scatter_(-1, I, V)\r\n#     RuntimeError: invalid argument 4: Input tensor must have same dimensions as output tensor at /opt/conda/conda-bld/pytorch_1577225354628/work/aten/src/TH/generic/THTensorEvenMoreMath.cpp:670\r\n```", "labels": ["module: operators", "small", "topic: error checking", "triaged"], "number_of_comments": 0, "created_at": "2020-01-31 18:22:55", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558227138": {"author_username": "Krovatkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32869, "title": "Dictionary Constants", "body": "", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-01-31 16:17:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558188306": {"author_username": "hawkinsp", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32868, "title": "__cuda_array_interface__ conversion does not support readonly arrays", "body": "PyTorch does not support importing readonly GPU arrays via `__cuda_array_interface__`:\r\nhttps://numba.pydata.org/numba-doc/dev/cuda/cuda_array_interface.html\r\n\r\nhttps://github.com/pytorch/pytorch/blob/2471ddc96c56162861f41f3b6b772b4b4e773ea7/torch/csrc/utils/tensor_numpy.cpp#L292\r\n\r\n## To Reproduce\r\n\r\nIf you have a copy of jax and jaxlib with GPU support built from head, the following will reproduce:\r\n\r\n```\r\nIn [1]: import torch, jax, jax.numpy as jnp\r\n\r\nIn [2]: x = jnp.array([1,2,3])\r\n\r\nIn [3]: y = torch.as_tensor(x)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-f43755a1deef> in <module>\r\n----> 1 y = torch.tensor(x)\r\n\r\nTypeError: the read only flag is not supported, should always be False\r\n```\r\n\r\n(Aside: this is not a PyTorch bug, but curiously CuPy drops the readonly flag, so you can make the import \"work\" by laundering the array through CuPy:\r\n```\r\nIn [1]: import torch, jax, jax.numpy as jnp, cupy\r\n\r\nIn [2]: x = jnp.array([1,2,3])\r\n\r\nIn [3]: y = torch.as_tensor(cupy.asarray(x), device=\"cuda\")\r\n\r\nIn [4]: x.__cuda_array_interface__\r\nOut[4]:\r\n{'shape': (3,),\r\n 'typestr': '<i4',\r\n 'data': (140492215944704, True),\r\n 'version': 2}\r\n\r\nIn [5]: y.__cuda_array_interface__\r\nOut[5]:\r\n{'typestr': '<i4',\r\n 'shape': (3,),\r\n 'strides': (4,),\r\n 'data': (140492215944704, False),\r\n 'version': 1}\r\n)\r\n```\r\n\r\n## Expected behavior\r\n\r\nPyTorch should support the readonly flag.\r\n\r\n\n\ncc @ngimel", "labels": ["module: cuda", "module: internals", "module: numba", "module: numpy", "triaged"], "number_of_comments": 0, "created_at": "2020-01-31 15:10:04", "reactions": {"total_count": 5, "+1": 5, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558168774": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32867, "title": "[feature request] Bit packed tensors utilities", "body": "A usecase: storing a full backtracking pointer matrix can be okay for needleman/ctc alignment (4x memory saving compared to uint8 representation), if 2bit data type is used. Currently it's possible to do this with bit manipulation magic, but probably not very efficient (store and load will require masking and shifting, not fused)\r\n\r\nAnother usecase: compressed BoolTensor for binary neural networks\r\n\r\nAnother usecase: extremely low-bit quantized representations.\r\n\r\nIs something like this already implemented for quantization? Probably a simple version of this feature could be providing some explicitly utility functions like calculating size of the holder `uint8` tensor, fused store and load functions (potentially explicitly batched, e.g. actual store is delayed until some aligned number of memory lines has arrived)\r\n\r\nIn NumPy the related functionality is `np.packbits` and `np.unpackbits`, however these are designed to work only with 1-bit contained type. 2-bit/4-bit would be cool as well.\r\n\r\nOn 1-bit side, another related project is RoaringBitmap https://github.com/RoaringBitmap/RoaringBitmap (http://roaringbitmap.org/) - for compressed bitsets for set operations.\n\ncc @izdeby", "labels": ["feature", "needs research", "topic: boolean tensor", "triaged"], "number_of_comments": 7, "created_at": "2020-01-31 14:35:47", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558165145": {"author_username": "flysofast", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32866, "title": "CUDA error: an illegal memory access was encountered when using output_padding in nn.ConvTranspose3d", "body": "I'm building a simple Encoder-Decoder architecture for video with 3D Convolutions and TransposedConv. The aim is make it fully convolutional so it would work with any video size.\r\nThe Encoder looks like this:\r\n```  \r\n    padding = int((kernel_size - 1)/2) #kernel_size = 5\r\n    self.network = nn.Sequential(    \r\n       nn.Conv3d(3, 128, kernel_size=kernel_size, stride=1, padding=padding),\r\n       nn.BatchNorm3d(128, affine=False),\r\n       nn.ReLU(True),\r\n\r\n       nn.Conv3d(128, 64, kernel_size=kernel_size, stride=2, padding=padding),\r\n       nn.BatchNorm3d(64, affine=False),\r\n       nn.ReLU(True),\r\n\r\n       nn.Conv3d(64, 64, kernel_size=kernel_size, stride=2, padding=padding),\r\n       nn.BatchNorm3d(64, affine=False),\r\n       nn.ReLU(True),\r\n\r\n       nn.Conv3d(64, 24, kernel_size=kernel_size, stride=1, padding=padding),\r\n       nn.BatchNorm3d(output_channels, affine=False),\r\n       nn.ReLU(True)\r\n    )\r\n````\r\nAnd here is the Decoder:\r\n```\r\n    padding = int((kernel_size - 1)/2) #kernel_size = 5\r\n    self.network = nn.Sequential( \r\n       nn.ConvTranspose3d(self.input_channels, 64, kernel_size=kernel_size, stride=1, padding=padding),\r\n       nn.BatchNorm3d(64, affine=False),\r\n       nn.ReLU(True),\r\n    \r\n       nn.ConvTranspose3d(64, 64, kernel_size=kernel_size, stride=2, padding=padding, output_padding=1),\r\n       nn.BatchNorm3d(64, affine=False),\r\n       nn.ReLU(True),\r\n\r\n       nn.ConvTranspose3d(64, 128, kernel_size=kernel_size, stride=2, padding=padding, output_padding=1),\r\n       nn.BatchNorm3d(128, affine=False),\r\n       nn.ReLU(True),\r\n\r\n       nn.Conv3d(128, self.output_channels, kernel_size=kernel_size, stride=1, padding=padding),\r\n    )\r\n````\r\nI trained the model with input size of (NxCxHxW) = (16x3x112x112). The outputs of each layer looks like this:\r\n\r\n```\r\n----------------------------------------------------------------\r\n        Layer (type)               Output Shape         Param #\r\n================================================================\r\n            Conv3d-1    [-1, 128, 16, 112, 112]          48,128\r\n       BatchNorm3d-2    [-1, 128, 16, 112, 112]               0\r\n              ReLU-3    [-1, 128, 16, 112, 112]               0\r\n            Conv3d-4        [-1, 64, 8, 56, 56]       1,024,064\r\n       BatchNorm3d-5        [-1, 64, 8, 56, 56]               0\r\n              ReLU-6        [-1, 64, 8, 56, 56]               0\r\n            Conv3d-7        [-1, 64, 4, 28, 28]         512,064\r\n       BatchNorm3d-8        [-1, 64, 4, 28, 28]               0\r\n              ReLU-9        [-1, 64, 4, 28, 28]               0\r\n           Conv3d-10        [-1, 24, 4, 28, 28]         192,024\r\n      BatchNorm3d-11        [-1, 24, 4, 28, 28]               0\r\n             ReLU-12        [-1, 24, 4, 28, 28]               0\r\n        Encoder3D-13        [-1, 24, 4, 28, 28]               0\r\n   \r\n  ConvTranspose3d-16        [-1, 64, 4, 28, 28]         192,064\r\n      BatchNorm3d-17        [-1, 64, 4, 28, 28]               0\r\n             ReLU-18        [-1, 64, 4, 28, 28]               0\r\n  ConvTranspose3d-19        [-1, 64, 8, 56, 56]         512,064\r\n      BatchNorm3d-20        [-1, 64, 8, 56, 56]               0\r\n             ReLU-21        [-1, 64, 8, 56, 56]               0\r\n  ConvTranspose3d-22    [-1, 128, 16, 112, 112]       1,024,128\r\n      BatchNorm3d-23    [-1, 128, 16, 112, 112]               0\r\n             ReLU-24    [-1, 128, 16, 112, 112]               0\r\n           Conv3d-25      [-1, 3, 16, 112, 112]          48,003\r\n        Decoder3D-26      [-1, 3, 16, 112, 112]               0\r\n================================================================\r\n````\r\nAfter the the model got trained, I tested in on a UCF-101 video sequence of size (Frames x Width x Height) = (16 x 342 x 256) with `os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"` and got this error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"reconstruct.py\", line 80, in <module>\r\n    torchsummary.summary(model, dataloader.dataset[0].shape)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torchsummary/torchsummary.py\", line 72, in summary\r\n    model(*x)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/namle/VCM/E2E/Model/model.py\", line 57, in forward\r\n    x_hat = self.decoder(y)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/namle/VCM/E2E/AutoEncoder/model3d.py\", line 84, in forward\r\n    x = self.network(x)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/container.py\", line 92, in forward\r\n    input = module(input)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/namle/anaconda3/envs/condapy3/lib/python3.7/site-packages/torch/nn/modules/conv.py\", line 921, in forward\r\n    output_padding, self.groups, self.dilation)\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\n````\r\nHowever if I ran the code on CPU and not using CUDA it will output a frame sequence of size (344 x 256) which is 2 pixels more in its width.\r\nAnother way to make to code run is at inference time change the output_padding in both TransposedConv layers in the Decoder to 0. Then I would get this sequence of size (341 x 253)\r\n\r\nI hope this has something to do with the arithmetic as in this paper https://arxiv.org/pdf/1603.07285.pdf, rather than a bug. I'd appreciate if you can point me into the right direction, so that my model can take in any video size and reconstruct it to the same size automatically.\n\ncc @ezyang @gchanan @zou3519 @ngimel", "labels": ["high priority", "module: cuda", "module: operators", "needs reproduction", "topic: convolution", "topic: crash", "topic: error checking", "triaged"], "number_of_comments": 6, "created_at": "2020-01-31 14:29:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558084243": {"author_username": "xsacha", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32865, "title": "device().has_index() calls torch_cpu function and crashes", "body": "## \ud83d\udc1b Bug\r\n\r\nSince PyTorch 1.5, torch_cpu and torch_cuda have been split.\r\nAfter updating to this version, my code now crashes on:\r\nATen/Functions.h:1197\r\n```\r\nif (options.device().has_index()) {\r\n```\r\n\r\nI notice the call stack shows it calling torch_cpu instead of torch_cuda despite my device being CUDA.\r\n{type_=CUDA (1) index_=0 }\r\n\r\nIf I remove torch_cpu.dll, it fails delay loading the DLL.\r\nWhen listing dependencies of torch, it appears to only be linked to torch_cpu.dll\r\n\r\nSite of crash in debug mode:\r\n![image](https://user-images.githubusercontent.com/61218/73537271-cf6a4f00-4473-11ea-9199-692067b09505.png)\r\n\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Try to use cuda and link to torch\r\n\r\n## Expected behavior\r\n\r\nCUDA code uses torch_cuda\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.5.0 nightly\r\n - OS (e.g., Linux): Windows\r\n - How you installed PyTorch (`conda`, `pip`, source): prebuilt\r\n - Build command you used (if compiling from source):\r\n - Python version:\r\n - CUDA/cuDNN version: 10.1/7.6\r\n - GPU models and configuration: RTX 2060\r\n - Any other relevant information:\n\ncc @ngimel @peterjc123", "labels": ["module: cuda", "module: windows", "triaged"], "number_of_comments": 2, "created_at": "2020-01-31 11:46:00", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558055594": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32864, "title": "[docs] Missing docs for torch.__version__ and torch.version", "body": "https://pytorch.org/docs/master/search.html?q=__version__&check_keywords=yes&area=default#\r\n\r\nhttps://pytorch.org/docs/master/search.html?q=version&check_keywords=yes&area=default#\r\n\r\nBoth are also missing from the doc navigation left pane (`torch.__config__` is there)\r\n", "labels": ["module: docs", "small", "triaged"], "number_of_comments": 0, "created_at": "2020-01-31 10:46:50", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558041922": {"author_username": "vadimkantorov", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32863, "title": "Incorrect result of argmax on CUDA with large non-contig tensors and on non-contig dimensions", "body": "```python\r\nimport torch\r\n\r\na = torch.load('bug.pt', 'cuda') # giant uint8 tensor with binary 0/1 values\r\nC = 11359\r\nb = a[:, 0, C]\r\n\r\nprint('shape', b.shape)\r\nprint('argmax', b.argmax())\r\nprint('max', b.max())\r\n\r\nK = int(b.argmax())\r\n\r\nprint(b[K]) # should be 1, but is 0\r\nprint(a[K, 0, C]) # should be 1, but is 0\r\n\r\nprint(b.nonzero()) # different result compared to argmax\r\nprint('bad argmax', a.argmax(dim = 0)[0][C])\r\n```\r\n```\r\n>>> torch.__version__\r\n'1.4.0'\r\n>>> torch.version.git_version\r\n'143868c3df4eb2acdcb166b340a1063abf61339c'\r\n```\r\n\r\nI tried to minimize the size of `bug.pt`, but my first attempts led to bug disappearing, so I'm uploading it to my OneDrive as is (it's quite heavy - 3Gb): https://1drv.ms/u/s!Apx8USiTtrYmqfhMddRNKxibLatubA?e=wyihRU \r\n\r\n(it would be nice for such situations to have a safe unpickler that can't execute arbitrary code or have it support loading/saving tensors from/to hdf5 as well. doesn't pytorch have some rudimentary unpickler for TorchCpp use?)\r\n\r\ncc @ezyang @gchanan @zou3519 @ngimel ", "labels": ["high priority", "module: cuda", "module: operators", "topic: TensorIterator", "triaged"], "number_of_comments": 25, "created_at": "2020-01-31 10:22:07", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "558011748": {"author_username": "jecampagne", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32861, "title": "add_, addcmul_, addcdiv_ deprecated", "body": "## Do someone else experiences many messages due to deprecated functions?\r\n\r\nUsing `torch 1.5.0.dev20200129` to track a bug, I experience these Warning. Do the library will change its API???\r\n\r\n/opt/conda/conda-bld/pytorch_1580285301452/work/torch/csrc/autograd/generated/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\r\nadd_(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\nadd_(Tensor other, Number alpha)\r\n/opt/conda/conda-bld/pytorch_1580285301452/work/torch/csrc/autograd/generated/python_variable_methods.cpp:1550: UserWarning: This overload of addcmul_ is deprecated:\r\naddcmul_(Number value, Tensor tensor1, Tensor tensor2)\r\nConsider using one of the following signatures instead:\r\naddcmul_(Tensor tensor1, Tensor tensor2, Number value)\r\n/opt/conda/conda-bld/pytorch_1580285301452/work/torch/csrc/autograd/generated/python_variable_methods.cpp:1480: UserWarning: This overload of addcdiv_ is deprecated:\r\naddcdiv_(Number value, Tensor tensor1, Tensor tensor2)\r\nConsider using one of the following signatures instead:\r\naddcdiv_(Tensor tensor1, Tensor tensor2, Number value)", "labels": ["module: operators", "triaged"], "number_of_comments": 1, "created_at": "2020-01-31 09:25:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557955712": {"author_username": "ashishgupta2598", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32859, "title": "AssertionError: torch.nn.quantized.ReLU does not support inplace", "body": "## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWe have a set of [listed resources available on the website](https://pytorch.org/resources). Our primary means of support is our discussion forum:\r\n\r\n- [Discussion Forum](https://discuss.pytorch.org/)\r\n\n\ncc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a", "labels": ["quantization", "triaged"], "number_of_comments": 1, "created_at": "2020-01-31 07:12:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557952886": {"author_username": "vishal-wiai", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32858, "title": "PyTorch android failing after 29/01/20 nightly", "body": "I am using a SSD object detection model to run on android using jit trace, developed in PyTorch 1.3.\r\n\r\n2 days back, I was using the nightly `pytorch_android:1.5.0-SNAPSHOT` it was working fine and I was getting the expected output. But today the app is crashing and giving the following error `Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 17354`.\r\n\r\nAfter this I tried other versions also.\r\n\r\n`pytorch_android:1.4.0` and `pytorch_android:1.4.0-SNAPSHOT` : This is giving different output. For the same input Tensor, output tensors in python and android are coming out to be different.\r\n\r\n`pytorch_android:1.3.0` : `com.facebook.jni.CppException: false CHECK FAILED at ../c10/core/Backend.h (tensorTypeIdToBackend at ../c10/core/Backend.h:106)`\r\n\r\nVery confused with what\u2019s going on! Can I revert back to the nightly build which was 2 days old?\n\ncc @ezyang @gchanan @zou3519", "labels": ["high priority", "mobile"], "number_of_comments": 1, "created_at": "2020-01-31 07:03:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557908112": {"author_username": "jspark1105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32856, "title": "D19359178", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-01-31 04:38:14", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557883418": {"author_username": "Fiery", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32855, "title": "[ONNX/Caffe2] Align Caffe2 Slice op with ONNX slice op using extended Caffe2 Slice op", "body": "ONNX's Slice supports axes as optional argument, however, this argument is not supported by Caffe2's Slice. So currently during model export, Caffe2 will turn the ONNX's Slice to 6 Caffe2 operators: Slice + ScatterAssign + Constant + ... , which may significantly decrease the performance.\r\nThis PR is to resolve this issue by extending Caffe2 Slice op to support an optional axes argument then eliminate redundant operators in exported Caffe2 Slice op from ONNX backend.", "labels": [], "number_of_comments": 0, "created_at": "2020-01-31 03:02:29", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557853087": {"author_username": "simonverret", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32851, "title": "Silent failing of batch_sampler when the data points are lists of tensors.", "body": "If a dataset is created from a list of list of tensors, and a custom batch_sample is made to sample from the outer list, then the inner tensors get mingled together in list batches.\r\n\r\n## To Reproduce\r\n\r\nConsider a dataset consisting of two lists of tensors. The `Sampler` to be used as `batch_sampler` intend to produce a single batch of two lists with this dataset.\r\n```\r\nimport torch\r\nfrom torch.utils.data import Dataset, DataLoader, Sampler\r\n\r\nlist_of_list_of_tensor_data = [\r\n        [ torch.tensor([0,1,2]),\r\n          torch.tensor([3,4,5]) ],\r\n        [ torch.tensor([6,7,8]),\r\n          torch.tensor([9,0,1]) ]\r\n    ]\r\n\r\nclass ListData(Dataset):\r\n    def __init__(self, data):\r\n        super().__init__()\r\n        self.data = data\r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        return self.data[idx]\r\n\r\ndataset = ListData(list_of_list_of_tensor_data)\r\n\r\nclass MyBatchSampler(Sampler):\r\n    def __init__(self,data):\r\n        super().__init__(data)\r\n    def __iter__(self):        \r\n        yield [0,1]\r\n\r\nsampler = MyBatchSampler(list_of_list_of_tensor_data)\r\nloader = DataLoader(dataset, batch_sampler = sampler)\r\n\r\nfor batch in loader:\r\n    print(batch)\r\n```\r\nAs a result, the batch you get a list of mingled tensors. \r\n```\r\n [tensor([[0, 1, 2],\r\n         [6, 7, 8]]), tensor([[3, 4, 5],\r\n         [9, 0, 1]])]\r\n```\r\n## Expected behavior\r\n\r\nOf course, the expected behaviour is obtained if you use full tensors instead of lists. Stacking the lists in the dataset's `__getitem__` does the trick here:\r\n```\r\nclass TensorData(Dataset):\r\n    def __init__(self, data):\r\n        super().__init__()\r\n        self.data = data\r\n    def __len__(self):\r\n        return len(self.data)\r\n    def __getitem__(self, idx):\r\n        return torch.stack(self.data[idx])\r\n\r\ndataset2 = TensorData(list_of_list_of_tensor_data)\r\nloader2 = DataLoader(dataset2, batch_sampler = sampler)\r\n\r\nfor batch in loader2:\r\n    print(batch)\r\n```\r\nwhich yields:\r\n```\r\ntensor([[[0, 1, 2],\r\n          [3, 4, 5]],\r\n\r\n         [[6, 7, 8],\r\n          [9, 0, 1]]])\r\n```\r\n## Environment\r\n\r\nPyTorch version: 1.0.1.post2\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.14.6\r\nGCC version: Could not collect\r\nCMake version: version 3.14.3\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.2\r\n[pip3] torch==1.0.1.post2\r\n[pip3] torchvision==0.2.2.post3\r\n[conda] Could not collect\r\n\r\n## Additional Context\r\n\r\nProbably that `Sampler` is not intended to be used in the way described above (see #28743), but since the code works and produce unexpected results, it was hard to identify what we did wrong when we stumble on this.\n\ncc @SsnL @cpuhrsch", "labels": ["module: dataloader", "module: nestedtensor", "triaged"], "number_of_comments": 1, "created_at": "2020-01-31 01:15:02", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557821576": {"author_username": "andfoy", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32848, "title": "Fix LambdaLR scheduler side effects", "body": "Fixes #32756", "labels": ["open source", "triaged"], "number_of_comments": 1, "created_at": "2020-01-30 23:35:40", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557788139": {"author_username": "Krovatkin", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32842, "title": "Fix stack overflows in Interpreter", "body": "This is to address stack overflow crashes in Interpreter when generating instructions for deep expression trees. When we decide we can emit an expression inline we recurse into its arguments to see if those can be emitted online as well and so on. \r\nAt some point, I believe both @bwasti , @ZolotukhinM and myself have run into this.\r\n\r\n```\r\nimport torch\r\nfrom test_jit import get_execution_plan\r\nfrom common_utils import enable_profiling_mode\r\nimport sys\r\nimport time\r\n\r\n\r\n\r\ndef abs100(x):\r\n    for i in range(100000):\r\n        x = x.abs()\r\n    return x\r\n\r\ndef abs1(x):\r\n    return x.abs()\r\n\r\nwith enable_profiling_mode():\r\n    x = torch.ones(1)\r\n    ja = torch.jit.trace(abs100, (x,), None, False, False)\r\n    ja(x)\r\n    ja(x)\r\n\r\n    # ja = torch.jit.script(abs1)\r\n    # ja(x)\r\n    # ja(x)\r\n\r\n    start = time.perf_counter_ns()\r\n    for i in range(1):\r\n        ja(x)\r\n    #duration = time.process_time_ns() - start\r\n    duration = (time.perf_counter_ns() - start) / 1000000000\r\n    print(\"time = {}\".format(duration))\r\n```", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-01-30 22:13:48", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557785874": {"author_username": "yf225", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32841, "title": "C++ tensor multi-dim indexing: add index() and index_put_() overloads, simple indexing tests, merge with Python indexing path", "body": "This PR adds the following items:\r\n- **1st item**: `ArrayRef<TensorIndex>` and `std::initializer_list<TensorIndex>` overloads for `Tensor::index` and `Tensor::index_put_`, to be used specifically for multi-dim indexing purpose. \r\n\r\nDesign rationale:\r\n* C++ `Tensor::index` and `Tensor::index_put_` are both existing tensor APIs, and they currently (before this PR) only accept a list of tensors (i.e. `ArrayRef<Tensor>`) as indices. If we change their signatures to also accept non-tensors as indices (i.e. `ArrayRef<TensorIndex>`, and `TensorIndex` is convertible from `Tensor` / `Slice` / `None` / `Ellipsis`), it would slow down the original code path (since now it has to go through more steps), which is undesirable.\r\n    \r\n    To get around this problem, the proposed solution is to keep the original `ArrayRef<Tensor>` overload, and add `ArrayRef<TensorIndex>` and `std::initializer_list<TensorIndex>` overloads to `Tensor::index` and `Tensor::index_put_`. This way, the original code path won\u2019t be affected, and the tensor multi-dim indexing API is only used when the user explicitly pass an `ArrayRef<TensorIndex>` or a braced-init-list of `TensorIndex`-convertible types to `Tensor::index` and `Tensor::index_put_` .\r\n    \r\n    Note that the above proposed solution would still affect perf for the user\u2019s original `Tensor::index` or `Tensor::index_put_` call sites that use a braced-init-list of tensors as input, e.g. `tensor.index({...})` or `tensor.index_put_({...}, value)`, since now such function calls would take the multi-dim indexing path instead of the original advanced indexing path. However, there are only two instances of this in our codebase (one in ATen cpp test, one in a C++ API nn init function), and they can be easily changed to explicitly use `ArrayRef<Tensor>` as input (I changed them in this PR). For external user\u2019s code, since this is part of the C++ frontend which is still considered experimental, we will only talk about this change in the release note, and ask users to switch to using `ArrayRef<Tensor>` explicitly if they want to keep using the original advanced indexing code path.\r\n\r\n- **2nd item**: Mechanisms for parsing `ArrayRef<TensorIndex>` indices and performing indexing operations (mirroring the functions in `torch/csrc/autograd/python_variable_indexing.cpp`). \r\n- **3rd item**: Simple tests to demonstrate that the `Tensor::index()` and `Tensor::index_put_()` APIs work. I will add more tests after the first few PRs are reviewed.\r\n- **4th item**: Merge Python/C++ indexing code paths, for code simplicity. I tested locally and found that there is no perf regression resulting from the merge. I will get more concrete numbers for common use cases when we settle on the overall design.\r\n\r\nThis PR supersedes https://github.com/pytorch/pytorch/pull/30425.", "labels": [], "number_of_comments": 5, "created_at": "2020-01-30 22:08:57", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557764099": {"author_username": "arbabu123", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32840, "title": "Can we add support for Enum in scripted models?", "body": "Enums are not supported right now\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom enum import Enum\r\n\r\nclass Testing(Enum):\r\n    lala = \"lala\"\r\n\r\nclass Mo(nn.Module):\r\n    def __init__(self, la):\r\n        super().__init__()\r\n        self.la = la\r\n    \r\n    def forward(self, x):\r\n        if self.la == Testing.lala:\r\n            return x\r\n        return x+1\r\n\r\nsaved_mo = torch.jit.script(mo)\r\n\r\nRuntimeError: \r\nModule 'Mo' has no attribute 'la' (This attribute exists on the Python module, but we failed to convert Python type: 'Testing' to a TorchScript type.):\r\n  File \"<ipython-input-18-76956f0fdf61>\", line 7\r\n    def forward(self, x):\r\n        if self.la == Testing.lala:\r\n           ~~~~~~~ <--- HERE\r\n            return x\r\n        return x+1\n\ncc @suo", "labels": ["jit", "triaged"], "number_of_comments": 1, "created_at": "2020-01-30 21:28:06", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557730021": {"author_username": "josherich", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32838, "title": "Improve docs search engine indexing", "body": "## \ud83d\udcda Documentation\r\n\r\n### There's no way to access api from search engine in one click.\r\n\r\nI find it's very hard to search pytorch api from search engine. A typical example would be seaching `pytorch conv2d`, the top result points to `https://pytorch.org/docs/stable/nn.html` without the heading hash `#conv2d`. \r\n\r\n<img width=\"668\" alt=\"Screen Shot 2020-01-27 at 12 28 28 PM\" src=\"https://user-images.githubusercontent.com/1488391/73486826-e0339b80-4373-11ea-8c04-93551011b107.png\">\r\n\r\nIn most cases, it's sth like the following img, where there's a `jump to xxx` link to the hashed url `https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Body`\r\n\r\n<img width=\"794\" alt=\"Screen Shot 2020-01-30 at 2 40 14 PM\" src=\"https://user-images.githubusercontent.com/1488391/73484929-30a8fa00-4370-11ea-9ded-0355951e9e15.png\">\r\n\r\nI'm not sure what's missing here, a quick search seems to suggest search engine need an unique id in headings like `<h4 id=\"xxx\"></h4>` to tell the heading structure.\r\n\r\nThe search bar in the doc page definitely works, It's also nice to have one click search engine access.\r\n\r\n<!-- A clear and concise description of what content in https://pytorch.org/docs is an issue. If this has to do with the general https://pytorch.org website, please file an issue at https://github.com/pytorch/pytorch.github.io/issues/new/choose instead. If this has to do with https://pytorch.org/tutorials, please file an issue at https://github.com/pytorch/tutorials/issues/new -->\r\n\n\ncc @ezyang @gchanan @zou3519", "labels": ["high priority", "module: doc infra", "module: docs", "triaged"], "number_of_comments": 2, "created_at": "2020-01-30 20:21:24", "reactions": {"total_count": 3, "+1": 3, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557729844": {"author_username": "jspark1105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32837, "title": "D19258370", "body": "", "labels": [], "number_of_comments": 1, "created_at": "2020-01-30 20:21:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557713454": {"author_username": "zou3519", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32836, "title": "vmap proof of concept (python)", "body": "See #32558 for another proof of concept.\r\n\r\nI was curious if we could do this entirely in Python. This POC puts\r\ntogether a really simple version of approximately what I understand\r\njax's vmap to do. It supports nested vmaps and tracks batching at a\r\nper-tensor basis.\r\n\r\nThe high level approach is:\r\n- we have an add_batched function defined that understands how to do a\r\n\"batched addition\" of two tensors with batch dimension indices. We can\r\nextend this design to support other operators.\r\n- vmap(foo)(args) replaces all args with `Batched` objects and then\r\ncalls `foo` on those `Batched` objects. Each Batched object keeps track\r\nof the input tensor, the batch dimension to be batched over, and what\r\nnesting level of vmap it was constructed at.\r\n- Calling pytorch ops on `Batched` objects return new `Batched` objects.\r\nThis is done via the `__torch_function__` dispatch mechanism.\r\n- At the end of the day, only one add \"kernel\" gets called --\r\noperations on `Batched` objects desugar into either unsqueezes/transposes\r\nor the final add operation.\r\n\r\n", "labels": [], "number_of_comments": 4, "created_at": "2020-01-30 19:51:07", "reactions": {"total_count": 1, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 1}}, "557703049": {"author_username": "Tetratrio", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32835, "title": "Autograd fails if used before multiprocessing Pool", "body": "## \ud83d\udc1b Bug\r\n\r\nJust like [#3966](https://github.com/pytorch/pytorch/issues/3966) autograd fails when using it on main process before a child process but not for normally started processes but for process pools.\r\n\r\n## To Reproduce\r\n```python\r\nimport torch\r\nfrom torch import multiprocessing as mp\r\n\r\nFAIL = True\r\n\r\ndef f(a=1):\r\n    torch.rand(3).requires_grad_(True).mean().backward()\r\n    return a ** 2\r\n\r\nif FAIL:\r\n    f()\r\n\r\n# This always works\r\np = mp.Process(target=f)\r\np.start()\r\np.join()\r\n\r\n# This fails if autograd has been used\r\nwith mp.Pool(3) as pool:\r\n    result = pool.map(f, [1, 2, 3])\r\nprint(result)\r\n```\r\n\r\n## Expected Behavior\r\nThis can be fixed by adding\r\n```python\r\nif __name__ == '__main__':\r\n    mp.set_start_method(\"spawn\")\r\n```\r\nAs using a normal mp.Process works without this addition (after a fix for the issue described in [#3966](https://github.com/pytorch/pytorch/issues/3966)) I am unsure if this is a bug or not. Adding the snippet above to your code can be hard for more interactive uses such as jupyter notebook.\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas 1.0 mkl\r\n[conda] mkl 2019.4 243\r\n[conda] mkl-service 2.3.0 py37he904b0f_0\r\n[conda] mkl_fft 1.0.14 py37ha843d7b_0\r\n[conda] mkl_random 1.1.0 py37hd6b4f25_0\r\n[conda] pytorch 1.4.0 py3.7_cuda10.1.243_cudnn7.6.3_0 pytorch\r\n[conda] torchvision 0.5.0 py37_cu101 pytorch\n\ncc @ezyang @SsnL @albanD @zou3519 @gqchen", "labels": ["actionable", "module: autograd", "module: multiprocessing", "topic: deadlock", "triaged"], "number_of_comments": 2, "created_at": "2020-01-30 19:30:24", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557699348": {"author_username": "driazati", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32834, "title": "[jit] Restore old annotate(Tensor, None) behavior", "body": "Until #25361, we allowed `annotate(Tensor, None)` to result in an `Optional[Tensor]` type. While this case looks like some bad implicit type behavior / maybe something left over from undefined tensor days, it exists in some serialized models (see [forum post here](https://discuss.pytorch.org/t/upgrade-issue-with-trace-and-autograd-from-torch-1-0-1-to-torch-1-3-1/65606/9)), so we should preserve its behavior for backwards-compatibility.\r\n\r\n", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-01-30 19:22:41", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557630879": {"author_username": "bnehoran", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32829, "title": "Fix grid_sample gradients at image borders", "body": "Fixes #23925\r\n\r\nThis fixes the incorrect gradients returned by `F.grid_sample` at image borders under `\"border\"` and `\"reflection\"` padding modes.\r\n\r\nAt nondifferentiable points, the choice of which gradient to return among its super- or subgradients is rather arbitrary and generally does not affect training. Before this change, however, a bug in the code meant that the gradient returned at the exact borders was not selected from among the super- or subgradients.\r\n\r\nThe gradient is now set to zero at the borders, which is a defensible choice for both the `\"border\"` and `\"reflection\"` padding modes:\r\n* For `\"border\"` padding, this effectively means that the exact borders of the image are now considered out of bounds, and therefore receive zero gradient.\r\n* For `\"reflection\"` padding, this effectively treats the exact borders as extrema.\r\n\r\n", "labels": ["open source", "triaged"], "number_of_comments": 4, "created_at": "2020-01-30 17:05:03", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557487232": {"author_username": "Coderx7", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32826, "title": "MKLDNN doesnt work and is slower than normal cpu mode", "body": "## \ud83d\udc1b Bug\r\n\r\nMKLDNN seems not to have any effect on runtime performance. Its even some times, slower than normal cpu mode!\r\nThe issue exists both on 1.3.1 and 1.4.0. \r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nRun this on a supporting platform such as Linux (e.g. ubuntu 18.04)\r\n\r\n```python\r\nimport torch\r\nprint(f'Pytorch version : {torch.__version__}')\r\nprint(*torch.__config__.show().split(\"\\n\"), sep=\"\\n\")\r\n\r\nfrom torchvision import models\r\nfrom torch.utils import mkldnn as mkldnn_utils\r\nimport time \r\n\r\ndef forward(net, use_mkldnn=False, iteration=1, batch_size=10):\r\n  net.eval()\r\n  batch = torch.rand(batch_size, 3,224,224)\r\n  if use_mkldnn:\r\n    net = mkldnn_utils.to_mkldnn(net)\r\n    batch = batch.to_mkldnn()\r\n\r\n  start_time = time.time()\r\n  for i in range(iteration):\r\n      net(batch)\r\n  return time.time() - start_time\r\n\r\nnet = models.resnet18(False)\r\niter_cnt = 100\r\nbatch_size = 1\r\n\r\nno_mkldnn   = forward(net, False, iter_cnt, batch_size)\r\nwith_mkldnn = forward(net, True,  iter_cnt, batch_size)\r\n\r\nprint(f\"time-normal: {no_mkldnn:.4f}s\")\r\nprint(f\"time-mkldnn: {with_mkldnn:.4f}s\")\r\nprint(f\"mkldnn is {with_mkldnn/no_mkldnn:.2f}x slower!\")\r\n\r\n```\r\nOr simply run this on a [Google Colab : ](https://colab.research.google.com/drive/1OKBSZY5rc5w9QIKfyo5trS3JTgBEvpPp) \r\n\r\n## Expected behavior\r\n\r\nShould be 10x or more faster than the normal CPU mode. \r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.4.0+cpu\r\n - OS (e.g., Linux): 18.04.3 LTS\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): - \r\n - Python version: tested on both 3.6 / 3.7.3\r\n - CUDA/cuDNN version: tested both on gpu and cpu builds: 10.1.243 / 7.6.5\r\n - GPU models and configuration: None\r\n - Any other relevant information: -\r\n\r\n\r\ncc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh", "labels": ["module: mkldnn", "triaged"], "number_of_comments": 7, "created_at": "2020-01-30 13:17:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557470660": {"author_username": "Tetratrio", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32824, "title": "Incorrect typing errors", "body": "## \ud83d\udc1b Bug\r\n\r\nMypy will report typing errors for some operations that should be allowed. I have encountered four different type check errors although I am only sure one of them is an actual bug. But I will list the others as well.\r\n\r\n## To Reproduce\r\n\r\n#### Error 1 - in-place operations on tensor\r\n```python\r\nt_1 = torch.rand(3)\r\nt_2 = torch.ones_like(t_1)\r\nt_1.add_(t_2)\r\n```\r\nThe error message:\r\n```\r\nerror: \"Tensor\" has no attribute \"add_\"; maybe \"addr_\", \"addmv_\", or \"addmm_\"?\r\n```\r\n---\r\n#### Error 2 - creating a parameter requires argument 'requires_grad' even though it has a default value\r\n```python\r\nt_1 = torch.rand(3)\r\np_1 = torch.nn.Parameter(t_1)\r\n```\r\nThe error message:\r\n```\r\nerror: Too few arguments for \"Parameter\"\r\n```\r\n---\r\n#### Error 3 - `torch.is_tensor()` is not counted as a assertion for typing\r\n```\r\nfrom typing import Union\r\ndef scale_values(input: torch.Tensor, weight: Union[float, torch.Tensor]) -> torch.Tensor:\r\n    if not torch.is_tensor(weight):\r\n        weight = torch.tensor(weight)\r\n    # the following line is required for mypy to not report an error:\r\n    # assert isinstance(weight, torch.Tensor)\r\n    return input * weight.to(input)\r\n```\r\nThe error message:\r\n```\r\nerror: Item \"float\" of \"Union[float, Tensor]\" has no attribute \"to\"\r\n```\r\n---\r\n#### Error 4 - `torch.nn.Module.training` is not a bool\r\n```python\r\nclass Dropout(torch.nn.Module):\r\n\r\n    def forward(self, input) -> torch.Tensor: # type: ignore\r\n        return torch.nn.functional.dropout(\r\n            input,\r\n            p=0.5,\r\n            training=self.training\r\n        )\r\n```\r\nThe error message:\r\n```\r\nerror: Argument \"training\" to \"dropout\" has incompatible type \"Union[Tensor, Module[Any]]\"; expected \"bool\"\r\n```\r\n\r\n## Expected behavior\r\n\r\nI know that the first error is an actual bug but the others I am not sure of as there might be some internal design decision that create these errors and that it is intended.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.1\r\n\r\nOS: Ubuntu 16.04.6 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 418.56\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy==1.17.2\r\n[pip] numpydoc==0.9.1\r\n[pip] torch==1.4.0\r\n[pip] torchvision==0.5.0\r\n[conda] blas                      1.0                mkl\r\n[conda] mkl                       2019.4         243\r\n[conda] mkl-service          2.3.0            py37he904b0f_0\r\n[conda] mkl_fft                  1.0.14           py37ha843d7b_0\r\n[conda] mkl_random         1.1.0             py37hd6b4f25_0\r\n[conda] pytorch                1.4.0             py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch\r\n[conda] torchvision           0.5.0            py37_cu101    pytorch\r\n\r\n## Additional context\r\n\r\nUsing mypy 0.750\r\n\n\ncc @ezyang", "labels": ["module: typing", "triaged"], "number_of_comments": 3, "created_at": "2020-01-30 12:46:35", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557323639": {"author_username": "twostay", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32822, "title": "Error tracing custom autograd.Function", "body": "## \ud83d\udc1b Bug\r\n\r\nI got an error when I tried to use jit to trace my autograd Function. In previous Pytorch versions(1.1.0), this works, but recently I updated to 1.4.0, and this stopped working. I'm not exactly sure which version exactly did this stopped working. \r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. Make sure you have Pytorch1.4.0\r\n2. Make a custom autograd Function. In  my case, I'm using this one\r\n```\r\nclass raw(autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, inp):\r\n        ctx.a = (inp * inp + 1).reciprocal()\r\n        ctx.b = ctx.a.sqrt()\r\n        return inp * ctx.b\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return grad_output * ctx.a * ctx.b\r\n```\r\n3. Trace this Function.\r\n```\r\nfrom torch import jit\r\njit.trace(raw.apply, example_inputs=tc.randn(1))\r\n```\r\n4. The error should orginate from the line of tracing and look like this:\r\n```\r\n......................\r\n    jit.trace(raw.apply, example_inputs=tc.randn(1))\r\n  File \"...\\Python37\\lib\\site-packages\\torch\\jit\\__init__.py\", line 903, in trace\r\n    name = _qualified_name(func)\r\n  File \"...\\Python37\\lib\\site-packages\\torch\\_jit_internal.py\", line 696, in _qualified_name\r\n    \"__module__ can't be None.\".format(name))\r\nRuntimeError: Could not get qualified name for class 'apply': __module__ can't be None.\r\n```\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\nI expect the tracing to work like how the previous versions did. \r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\nPyTorch version: 1.4.0+cpu\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Microsoft Windows 10 Home\r\nGCC version: Could not collect\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.16.3\r\n[pip3] torch==1.4.0+cpu\r\n[pip3] torchvision==0.5.0+cpu\r\n[conda] Could not collect\r\n\r\n## Additional context\r\nI'm bad at using Github so if I missed something please tell me :)\r\n<!-- Add any other context about the problem here. -->\r\n\n\ncc @suo", "labels": ["jit"], "number_of_comments": 0, "created_at": "2020-01-30 07:54:17", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237935": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32816, "title": "[quant][graphmode] Add add_relu pattern in skip values", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* **#32816 [quant][graphmode] Add add_relu pattern in skip values**\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-01-30 02:59:51", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237898": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32815, "title": "[quant][graphmode] Add asserts for Conv and ReLU modules", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* **#32815 [quant][graphmode] Add asserts for Conv and ReLU modules**\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\natt\n\nTest Plan:\npython test/test_jit.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": [], "number_of_comments": 2, "created_at": "2020-01-30 02:59:44", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237869": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32814, "title": "[quant][graphmode] Skip quantizing input and output in matched module", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* **#32814 [quant][graphmode] Skip quantizing input and output in matched module**\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWe skip quantization for the intermediate values for patterns like `Conv - ReLU`,\nbut currently we didn't skip quantizing the input/output of the graphs of matched modules,\nsince we now changed the way we add observers, this also needs to be updated.\n\nTest Plan:\npython test/test_jit.py -- 'TestJit.test_insert_observers_skip_values'\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-01-30 02:59:36", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237832": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32813, "title": "[quant][graphmode][refactor] Separate preprocess step for insertObserver", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* **#32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver**\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWe need to separate the step to make the logic more clear\nand also to find all the values we want to skip in advance\nwithout the interference of inserted observers\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-01-30 02:59:27", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237811": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32812, "title": "[quant][graphmode][refactor] Change signature of getModuleAccessPath", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* **#32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath**\n* #32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nWe'll error out for the case we can't handle inside the function,\ninstead of checking each time in the callsite\n\nTest Plan:\n.\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 1, "created_at": "2020-01-30 02:59:20", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557237725": {"author_username": "jerryzh168", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32809, "title": "[quant][graphmode][refactor] Move the check for qconfig inside insertObserver call", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* #33533 [not4land] Temporary workaround to get quantized ops\n* #33481 [quant][graphmode] Handling ops doesn't require observation in insertObservers\n* #33532 [quant][graphmode] Add quantization pattern for quantized::add_relu\n* #33279 [quant][graphmode] Add quantized conv2d-relu fusion pattern\n* #33277 [WIP][quant][graphmode] Observing input/output values in callsite\n* #33276 [quant][graphmode][refactor] Move check for weight outside of insertObserverFor\n* #33275 [quant][graphmode][refactor] Move values_to_skip check inside valueNeedsToBeQuantized\n* #33274 [quant][graphmode][refactor] Simplify signature for insertObserverFor\n* #33273 [quant][graphmode][refactor] Checks for bias and weight\n* #33173 [quant][graphmode] Add a pass to handle ops that doesn't require observation\n* #33531 [quant][graphmode] Replicate dequantize nodes\n* #33172 [quant][graphmode][refactor] Factor out insertDequantCall\n* #33171 [quant][graphmode] refactor nodeQuantizable\n* #32816 [quant][graphmode] Add add_relu pattern in skip values\n* #32815 [quant][graphmode] Add asserts for Conv and ReLU modules\n* #32814 [quant][graphmode] Skip quantizing input and output in matched module\n* #32813 [quant][graphmode][refactor] Separate preprocess step for insertObserver\n* #32812 [quant][graphmode][refactor] Change signature of getModuleAccessPath\n* **#32809 [quant][graphmode][refactor] Move the check for qconfig inside insertObserver call**\n* #32379 [quant][graphmode] FoldConvBatchNorm2d support shared ClassTypes\n\nSummary:\nThis is a refactor to help further changes to quantization.cpp\nWe want some operations on the graph happen before we call insertObserver for invoked methods,\nespecially `addIntermediateValuesToSkipObserver` since we want to skip the input of the ReLU\nmodule in `Conv - ReLU` pattern.\n\nTest Plan:\ntest_jit.py\ntest_quantization.py\n\nReviewers:\nmvz\n\nSubscribers:\n\nTasks:\n\nTags:", "labels": ["jit"], "number_of_comments": 2, "created_at": "2020-01-30 02:58:56", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}, "557218523": {"author_username": "jjlilley", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "issue_number": 32806, "title": "[Don't review yet, experimental] Use faster crc impl for opensource torch::save()", "body": "Stack from [ghstack](https://github.com/ezyang/ghstack):\n* **#32806 [Don't review yet, experimental] Use faster crc impl for opensource torch::save()**\n\nDifferential Revision: [D19635487](https://our.internmc.facebook.com/intern/diff/D19635487/)", "labels": [], "number_of_comments": 1, "created_at": "2020-01-30 01:43:16", "reactions": {"total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "heart": 0, "hooray": 0}}}